"""YouTube ë¹„ë””ì˜¤ ìë™ ë³´ì´ìŠ¤ì˜¤ë²„ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (Windows ë²„ì „)"""

# Google API Discovery ìºì‹œ ë¹„í™œì„±í™” (RAG API ì¶©ëŒ ë°©ì§€)
import os
os.environ["GOOGLE_API_USE_CLIENT_CERTIFICATE"] = "false"

from moviepy import VideoFileClip, AudioFileClip, CompositeAudioClip, TextClip, CompositeVideoClip, ImageClip, ColorClip, concatenate_videoclips
from moviepy.video.VideoClip import VideoClip
from moviepy.audio.fx import MultiplyVolume, AudioLoop
from moviepy.video.fx import MultiplyColor, FadeOut, Resize
import numpy as np
from PIL import Image, ImageDraw, ImageFont, ImageColor
import time
import zipfile
import shutil
import unicodedata
import sys
import requests

try:
    from pydub import AudioSegment
    from pydub.effects import speedup, compress_dynamic_range
    PYDUB_IMPORT_ERROR = None
except ModuleNotFoundError as exc:  # Python 3.13 requires optional pyaudioop
    AudioSegment = None
    speedup = None
    compress_dynamic_range = None
    PYDUB_IMPORT_ERROR = exc
import re
import os
import json
import random
import textwrap
# 302.ai APIë§Œ ì‚¬ìš© (google.generativeai íŒ¨í‚¤ì§€ ì œê±°)
GEMINI_AVAILABLE = True  # í•­ìƒ True (302.ai ì‚¬ìš©)
try:
    from google.api_core import exceptions as google_api_exceptions
except ImportError:
    google_api_exceptions = None

try:
    from auto_subtitle_windows import GeminiSubtitleGenerator as SubtitleGenerator
    AUTO_SUBTITLE_AVAILABLE = True
except ImportError:
    try:
        from auto_subtitle import GeminiSubtitleGenerator as SubtitleGenerator
        AUTO_SUBTITLE_AVAILABLE = True
    except ImportError:
        SubtitleGenerator = None
        AUTO_SUBTITLE_AVAILABLE = False



# í°íŠ¸ ì„¤ì • ìºì‹œ (TTC ì¸ë±ìŠ¤)
FONT_INDEX_OVERRIDES = {}


def register_font_override(path, *, index=0, textclip_name=None):
    """íŠ¹ì • í°íŠ¸ íŒŒì¼ì— ëŒ€í•œ TTC ì¸ë±ìŠ¤ ì •ë³´ ë“±ë¡"""
    if path:
        FONT_INDEX_OVERRIDES[path] = index or 0


def get_textclip_font_name(font_path):
    """TextClipì—ëŠ” í•­ìƒ íŒŒì¼ ê²½ë¡œë¥¼ ì‚¬ìš© (Pillowê°€ TTC indexë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ)."""
    return font_path


def load_pil_font(font_path, font_size):
    """ë“±ë¡ëœ TTC ì¸ë±ìŠ¤ë¥¼ ê³ ë ¤í•˜ì—¬ PIL í°íŠ¸ë¥¼ ë¡œë“œ."""
    if not font_path:
        return ImageFont.load_default()
    index = FONT_INDEX_OVERRIDES.get(font_path, 0)
    try:
        return ImageFont.truetype(font_path, font_size, index=index)
    except OSError:
        # ì¸ë±ìŠ¤ ë¬¸ì œ ë“±ìœ¼ë¡œ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ ì¸ë±ìŠ¤ë¡œ ë‹¤ì‹œ ì‹œë„
        if index:
            try:
                return ImageFont.truetype(font_path, font_size)
            except OSError:
                pass
        return ImageFont.load_default()


# ì§€ì›í•˜ëŠ” ë¹„ë””ì˜¤ í™•ì¥ì ëª©ë¡
SUPPORTED_VIDEO_EXTENSIONS = (
    ".mp4",
    ".mov",
    ".mkv",
    ".avi",
    ".m4v",
    ".webm",
)


def _safe_extract_zip_member(zip_file, member, target_dir):
    """ZIP ë©¤ë²„ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ (ë””ë ‰í„°ë¦¬ íƒˆì¶œ ë°©ì§€)."""
    destination = os.path.abspath(os.path.join(target_dir, member))
    target_root = os.path.abspath(target_dir)

    if not destination.startswith(target_root + os.sep) and destination != target_root:
        raise RuntimeError(f"ZIP íŒŒì¼ì— í—ˆìš©ë˜ì§€ ì•Šì€ ê²½ë¡œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {member}")

    os.makedirs(os.path.dirname(destination), exist_ok=True)
    with zip_file.open(member) as source, open(destination, "wb") as out_file:
        shutil.copyfileobj(source, out_file)
    return destination


def extract_tag_from_filename(filename):
    """
    ë¹„ë””ì˜¤ íŒŒì¼ëª…ì—ì„œ íƒœê·¸ë¥¼ ì¶”ì¶œ.

    ì˜ˆì‹œ:
        masstiktok_@probablyeatable_1950_975.mp4 â†’ @probablyeatable
        masstiktok_dusisalim_41-212-248.mp4 â†’ dusisalim

    íŒ¨í„´: masstiktok_ ì œê±°, ë§ˆì§€ë§‰ _ìˆ«ì_ìˆ«ì ë˜ëŠ” _ìˆ«ì-ìˆ«ì-ìˆ«ì íŒ¨í„´ ì œê±°
    """
    # í™•ì¥ì ì œê±°
    name_without_ext = os.path.splitext(filename)[0]

    # masstiktok_ ì œê±°
    if name_without_ext.startswith("masstiktok_"):
        name_without_prefix = name_without_ext[len("masstiktok_"):]
    else:
        return ""

    # ë§ˆì§€ë§‰ ë¶€ë¶„ì˜ _ìˆ«ì_ìˆ«ì ë˜ëŠ” _ìˆ«ì-ìˆ«ì-ìˆ«ì íŒ¨í„´ ì œê±°
    # íŒ¨í„´: _ë¡œ ì‹œì‘í•˜ê³  ê·¸ ë’¤ë¡œ ìˆ«ì, í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´ë§Œ ìˆëŠ” ê²½ìš°
    pattern = r'_[\d\-_]+$'
    tag = re.sub(pattern, '', name_without_prefix)

    return tag


def _extract_videos_from_archives(input_dir):
    """
    ì…ë ¥ í´ë” ë‚´ ZIP íŒŒì¼ì—ì„œ ë¹„ë””ì˜¤ë¥¼ ì¶”ì¶œí•´ Temp/extracted_videos ì— ì €ì¥.

    Returns:
        (list[str], str, dict, dict): ì¶”ì¶œëœ ë¹„ë””ì˜¤ ê²½ë¡œ ëª©ë¡, ì¶”ì¶œ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬, source_map, folder_map
    """
    extracted_videos = []
    source_map = {}
    folder_map = {}  # ë¹„ë””ì˜¤ ê²½ë¡œ -> ZIP ë‚´ë¶€ í´ë” ì´ë¦„
    temp_root = get_config_value(["paths", "temp_dir"], "Temp")
    extract_root = os.path.join(temp_root, "extracted_videos")
    os.makedirs(extract_root, exist_ok=True)

    used_root = os.path.abspath(os.path.join(input_dir, "Used"))

    for root, dirs, files in os.walk(input_dir):
        # Used í´ë”ëŠ” ìŠ¤í‚µ
        dirs[:] = [
            d for d in dirs
            if not os.path.abspath(os.path.join(root, d)).startswith(used_root)
        ]
        for filename in files:
            if os.path.splitext(filename)[1].lower() != ".zip":
                continue

            zip_path = os.path.join(root, filename)
            archive_name = os.path.splitext(os.path.basename(zip_path))[0]
            target_dir = os.path.join(extract_root, archive_name)

            if os.path.exists(target_dir):
                try:
                    shutil.rmtree(target_dir)
                except (OSError, PermissionError) as e:
                    # Windowsì—ì„œ íŒŒì¼ì´ ì‚¬ìš© ì¤‘ì¼ ìˆ˜ ìˆìŒ - ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                    print(f"[WARNING] ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            os.makedirs(target_dir, exist_ok=True)

            try:
                with zipfile.ZipFile(zip_path, "r") as archive:
                    extracted_count = 0
                    for member in archive.namelist():
                        if member.endswith("/"):
                            continue

                        ext = os.path.splitext(member)[1].lower()
                        if ext not in SUPPORTED_VIDEO_EXTENSIONS:
                            continue

                        try:
                            extracted_path = _safe_extract_zip_member(archive, member, target_dir)
                            extracted_videos.append(extracted_path)
                            source_map[extracted_path] = zip_path

                            # ZIP ë‚´ë¶€ í´ë” ì´ë¦„ ì¶”ì¶œ (ì²« ë²ˆì§¸ í´ë”)
                            member_parts = member.split('/')
                            if len(member_parts) > 1:
                                folder_name = member_parts[0]
                            else:
                                folder_name = archive_name
                            folder_map[extracted_path] = folder_name

                            extracted_count += 1
                        except RuntimeError as exc:
                            print(f"[WARNING]  ZIP ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤ ({exc})")

                if extracted_count:
                    print(f"[ZIP] ZIPì—ì„œ ë¹„ë””ì˜¤ {extracted_count}ê°œ ì¶”ì¶œ: {zip_path}")
                else:
                    print(f"[WARNING]  ZIPì—ì„œ ë¹„ë””ì˜¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {zip_path}")
            except zipfile.BadZipFile as exc:
                print(f"[ERROR] ZIP íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {zip_path} ({exc})")

    return extracted_videos, extract_root, source_map, folder_map


def find_first_video_file(input_dir):
    """ì£¼ì–´ì§„ ë””ë ‰í„°ë¦¬ì™€ í•˜ìœ„ í´ë”ì—ì„œ ì´ë¦„ìˆœìœ¼ë¡œ ê°€ì¥ ë¹ ë¥¸ ë¹„ë””ì˜¤ íŒŒì¼ì„ ë°˜í™˜"""
    # ëª¨ë“  ë¹„ë””ì˜¤ íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ ê²€ìƒ‰
    all_videos = []
    seen = set()

    extracted_videos, extract_root, source_map, folder_map = _extract_videos_from_archives(input_dir)

    search_roots = [input_dir]
    if extract_root and os.path.exists(extract_root):
        search_roots.append(extract_root)

    for video_path in extracted_videos:
        abs_path = os.path.abspath(video_path)
        if abs_path not in seen:
            seen.add(abs_path)
            origin = source_map.get(video_path, input_dir)
            folder_name = folder_map.get(video_path, "")
            all_videos.append((abs_path, origin, folder_name))

    used_root = os.path.abspath(os.path.join(input_dir, "Used"))

    for root_dir in search_roots:
        if not root_dir or not os.path.exists(root_dir):
            continue

        for root, dirs, files in os.walk(root_dir):
            # Used í´ë”ëŠ” ìŠ¤í‚µ
            dirs[:] = [
                d for d in dirs
                if not os.path.abspath(os.path.join(root, d)).startswith(used_root)
            ]

            for filename in files:
                if os.path.splitext(filename)[1].lower() in SUPPORTED_VIDEO_EXTENSIONS:
                    full_path = os.path.abspath(os.path.join(root, filename))

                    # Used í´ë” ë‚´ë¶€ì˜ íŒŒì¼ì€ ìŠ¤í‚µ
                    if full_path.startswith(used_root):
                        continue

                    if full_path not in seen:
                        seen.add(full_path)
                        origin = source_map.get(full_path, input_dir)
                        folder_name = folder_map.get(full_path, "")
                        if root_dir == input_dir:
                            origin = full_path
                            # Input í´ë”ì— ì§ì ‘ ìˆëŠ” ë¹„ë””ì˜¤ì˜ ê²½ìš° íŒŒì¼ëª…ì—ì„œ íƒœê·¸ ì¶”ì¶œ
                            extracted_tag = extract_tag_from_filename(filename)
                            if extracted_tag:
                                folder_name = extracted_tag
                                print(f"[TAG] íŒŒì¼ëª…ì—ì„œ íƒœê·¸ ì¶”ì¶œ: {filename} â†’ {folder_name}")
                        all_videos.append((full_path, origin, folder_name))

    if not all_videos:
        raise FileNotFoundError(f"'{input_dir}'ì™€ í•˜ìœ„ í´ë”ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì•ŒíŒŒë²³ìˆœ)
    all_videos.sort(key=lambda item: item[0])

    print(f"[FOLDER] ë°œê²¬ëœ ë¹„ë””ì˜¤ íŒŒì¼: {len(all_videos)}ê°œ")
    print(f"   ì„ íƒëœ íŒŒì¼: {all_videos[0][0]}")

    return all_videos[0]


def move_input_file_to_used(original_path):
    """Input í´ë”ì˜ ì›ë³¸ íŒŒì¼ì„ Used í´ë”ë¡œ ì´ë™."""
    if not original_path or not os.path.exists(original_path):
        return False

    input_dir = get_config_value(["paths", "input_dir"], "Input")
    input_dir_abs = os.path.abspath(input_dir)
    origin_abs = os.path.abspath(original_path)

    # Input ë£¨íŠ¸ ìì²´ë¥¼ ì´ë™í•˜ë ¤ëŠ” ê²½ìš° ë³´í˜¸
    try:
        if os.path.isdir(origin_abs) and os.path.samefile(origin_abs, input_dir_abs):
            print(f"[WARNING] Input ë£¨íŠ¸ í´ë”ëŠ” ì´ë™ ëŒ€ìƒì—ì„œ ì œì™¸í•©ë‹ˆë‹¤: {origin_abs}")
            return False
    except FileNotFoundError:
        return False

    used_root = os.path.join(input_dir, "Used")
    used_root_abs = os.path.abspath(used_root)
    os.makedirs(used_root_abs, exist_ok=True)

    if origin_abs.startswith(used_root_abs):
        return False  # ì´ë¯¸ ì´ë™ëœ íŒŒì¼

    if origin_abs.startswith(input_dir_abs):
        rel_path = os.path.relpath(origin_abs, input_dir_abs)
        dest_path = os.path.join(used_root_abs, rel_path)
    else:
        dest_path = os.path.join(used_root_abs, os.path.basename(origin_abs))

    dest_dir = os.path.dirname(dest_path)
    os.makedirs(dest_dir, exist_ok=True)

    base_name, ext = os.path.splitext(os.path.basename(dest_path))
    counter = 1
    final_dest = dest_path
    while os.path.exists(final_dest):
        final_dest = os.path.join(dest_dir, f"{base_name}_{counter}{ext}")
        counter += 1

    shutil.move(origin_abs, final_dest)
    print(f"[ZIP] ì…ë ¥ íŒŒì¼ ì´ë™: {original_path} -> {final_dest}")
    return True


def cleanup_extracted_video(path_to_video):
    """ì¶”ì¶œëœ ì„ì‹œ ë¹„ë””ì˜¤ íŒŒì¼ê³¼ ë¹ˆ ë””ë ‰í„°ë¦¬ë¥¼ ì •ë¦¬."""
    if not path_to_video:
        return False

    temp_root = get_config_value(["paths", "temp_dir"], "Temp")
    extract_root = os.path.abspath(os.path.join(temp_root, "extracted_videos"))
    abs_path = os.path.abspath(path_to_video)

    if not abs_path.startswith(extract_root):
        return False

    # ë¹„ë””ì˜¤ íŒŒì¼ ì‚­ì œ
    if os.path.exists(abs_path):
        try:
            os.remove(abs_path)
        except (OSError, PermissionError) as e:
            print(f"[WARNING] ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            return False

    # ë¹ˆ ë””ë ‰í„°ë¦¬ ì‚­ì œ (Windows ê¶Œí•œ ì˜¤ë¥˜ ì²˜ë¦¬)
    parent = os.path.dirname(abs_path)
    while parent.startswith(extract_root) and parent != extract_root:
        try:
            if not os.listdir(parent):
                os.rmdir(parent)
                parent = os.path.dirname(parent)
            else:
                break
        except (OSError, PermissionError) as e:
            # Windowsì—ì„œ ë””ë ‰í„°ë¦¬ê°€ ì‚¬ìš© ì¤‘ì´ê±°ë‚˜ ê¶Œí•œ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ
            # ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
            break
    return True


CONFIG_PATH = os.environ.get("CONFIG_FILE", os.path.join("Config", "config.json"))


def load_config():
    """ì„¤ì • íŒŒì¼(JSON)ì„ ë¡œë“œí•˜ê³  ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜"""
    if not os.path.exists(CONFIG_PATH):
        return {}

    with open(CONFIG_PATH, "r", encoding="utf-8") as cfg:
        try:
            return json.load(cfg)
        except json.JSONDecodeError as exc:
            raise ValueError(f"ì„¤ì • íŒŒì¼ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {CONFIG_PATH}") from exc


CONFIG = load_config()


def get_config_value(path, default=None):
    """ì¤‘ì²©ëœ ì„¤ì • ê°’ì„ ì•ˆì „í•˜ê²Œ ì¡°íšŒ"""
    current = CONFIG
    for key in path:
        if not isinstance(current, dict):
            return default
        current = current.get(key)
    return current if current is not None else default


def get_layout_value(category, key, fallback_path=None, default=None):
    """layout_settings ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ ê²½ë¡œì—ì„œ ê°’ì„ ë°˜í™˜."""
    layout = get_config_value(["layout_settings", category, key], None)
    if layout is not None:
        return layout
    if fallback_path:
        return get_config_value(fallback_path, default)
    return default


def get_layout_value(category, key, fallback_path=None, default=None):
    """layout_settings ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ ê²½ë¡œì—ì„œ ê°’ì„ ë°˜í™˜."""
    layout = get_config_value(["layout_settings", category, key], None)
    if layout is not None:
        return layout
    if fallback_path:
        return get_config_value(fallback_path, default)
    return default


def _get_302ai_config():
    """302.ai API ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    api_key = os.getenv("AI_302_API_KEY")
    if not api_key:
        api_key = get_config_value(["ai_settings", "api_key"])

    if not api_key:
        raise RuntimeError(
            "302.ai API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. "
            "AI_302_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ "
            "`Config/config.json`ì˜ `ai_settings.api_key`ì— ê°’ì„ ì§€ì •í•˜ì„¸ìš”."
        )

    # API í‚¤ ê³µë°± ì œê±° ë° ì •ë¦¬
    api_key = api_key.strip()

    base_url = get_config_value(["ai_settings", "base_url"], "https://api.302.ai/v1")
    model = get_config_value(["ai_settings", "model"], "gemini-2.5-flash")

    # ë””ë²„ê·¸: API í‚¤ ì• 10ìì™€ ê¸¸ì´ í™•ì¸
    print(f"[DEBUG] API í‚¤ ì‹œì‘: {api_key[:10] if len(api_key) > 10 else api_key}...")
    print(f"[DEBUG] API í‚¤ ê¸¸ì´: {len(api_key)} ë¬¸ì")

    return {
        "api_key": api_key,
        "base_url": base_url,
        "model": model
    }


def get_available_chromakey_videos():
    """reaction chromakey í´ë”ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡œë§ˆí‚¤ ë¹„ë””ì˜¤ ëª©ë¡ ë°˜í™˜"""
    chromakey_dir = "reaction chromakey"
    if not os.path.exists(chromakey_dir):
        return []

    video_files = []
    for filename in os.listdir(chromakey_dir):
        if filename.lower().endswith(SUPPORTED_VIDEO_EXTENSIONS):
            video_files.append(filename)

    return sorted(video_files)


def _build_gemini_prompt(duration_seconds: float) -> str:
    """Gemini í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ ìƒì„±"""
    duration_text = f"{duration_seconds:.2f} seconds"
    num_highlights = 1  # í•­ìƒ 1ë¬¸ì¥ë§Œ ì‚¬ìš©

    # ì–¸ì–´ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    language = get_config_value(["voice_settings", "language"], "en")

    # ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡œë§ˆí‚¤ ë¹„ë””ì˜¤ ëª©ë¡
    chromakey_videos = get_available_chromakey_videos()
    chromakey_list = "\n".join([f"  - {video}" for video in chromakey_videos]) if chromakey_videos else "  (None available)"

    if language == "ko":
        # í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸
        instructions = f"""
ë‹¹ì‹ ì€ ìŠ¤í¬ì¸  ìˆœê°„ì„ ê°ë™ì ì´ê³  ë“œë¼ë§ˆí‹±í•œ ì´ì•¼ê¸°ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í† ë¦¬í…”ëŸ¬ì…ë‹ˆë‹¤.

â±ï¸ **ì¤‘ìš” ì œì•½ì‚¬í•­**: ì´ ë¹„ë””ì˜¤ëŠ” ì •í™•íˆ {duration_text} ê¸¸ì´ì…ë‹ˆë‹¤. ëª¨ë“  ë‚˜ë ˆì´ì…˜ íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” ì´ ê¸¸ì´ ë‚´ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤!

ğŸ¯ í•„ìˆ˜ ë¶„ì„:
1. ì „ì²´ í´ë¦½ì„ ì‹œì²­í•˜ê³  ìŠ¤í† ë¦¬ë¥¼ êµ¬ì„±í•˜ì„¸ìš”:
   - ë“œë¼ë§ˆí‹±í•œ ì„¤ì •ì€ ë¬´ì—‡ì¸ê°€? (ìƒí™©, ë¬´ì—‡ì´ ê±¸ë ¤ìˆëŠ”ê°€)
   - ì–´ë–¤ ê°ˆë“±ì´ë‚˜ ë„ì „ì´ ìˆëŠ”ê°€?
   - ì˜ˆìƒì¹˜ ëª»í•œ ë°˜ì „ì´ë‚˜ í´ë¼ì´ë§¥ìŠ¤ ìˆœê°„ì€?
   - ì–´ë–»ê²Œ í•´ê²°ë˜ëŠ”ê°€? (ì„±ê³µ, ì‹¤íŒ¨, ë†€ë¼ì›€)
   - ê°€ì¥ ì¤‘ìš”í•œ ìˆœê°„ì˜ ì •í™•í•œ íƒ€ì„ìŠ¤íƒ¬í”„
   - íŠ¹íˆ ê´€ì¤‘ ë¦¬ì•¡ì…˜ì´ë‚˜ ì¹´ë©”ë¼ ì´ë™ì´ ì•„ë‹Œ **ê°€ì¥ í™”ë ¤í•˜ê³  ì•¡ì…˜ì´ í­ë°œí•˜ëŠ” ì‹œì **ì„ ì •í™•íˆ ì°¾ìœ¼ì„¸ìš”

2. **ì¤‘ìš” - ë¶ˆí•„ìš”í•œ ì¸íŠ¸ë¡œ ì œê±° (ë§¤ìš° ì ê·¹ì ìœ¼ë¡œ)**:
   - ë¹„ë””ì˜¤ë¥¼ ë³´ê³  í¥ë¯¸ë¡­ê±°ë‚˜ ì¤‘ìš”í•œ ì•¡ì…˜ì´ ì‹œì‘ë˜ëŠ” ì •í™•í•œ ì‹œì ì„ ì°¾ìœ¼ì„¸ìš”
   - ê·¸ ìˆœê°„ ì´ì „ì˜ ëª¨ë“  ê²ƒì„ ì˜ë¼ë‚´ì„¸ìš” - ì§€ë£¨í•œ ì¸íŠ¸ë¡œ, ì¤€ë¹„, ëŒ€ê¸°, ì¹´ë©”ë¼ ì¡°ì •, ëŠë¦° ì‹œì‘
   - **ë³´ìˆ˜ì ì´ì§€ ë§ˆì„¸ìš”** - ì²˜ìŒ 3-5ì´ˆê°€ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ë©´ ì˜ë¼ë‚´ì„¸ìš”
   - ë¹„ë””ì˜¤ê°€ ì²« 0.5ì´ˆ ì´ë‚´ì— ì¦‰ì‹œ ì¤‘ìš”í•œ ì•¡ì…˜ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ì—ë§Œ trimì„ 0ì´ˆë¡œ ì„¤ì •í•˜ì„¸ìš”
   - ì¼ë°˜ì ì¸ ì‹œë‚˜ë¦¬ì˜¤:
     * ì•¡ì…˜ ì „ì— ì‚¬ëŒì´ ê±·ê±°ë‚˜ ì ‘ê·¼ â†’ ê±·ëŠ” ë¶€ë¶„ ì œê±°
     * ë©”ì¸ í”¼ì‚¬ì²´ ì „ì— ì¹´ë©”ë¼ íŒ¨ë‹/ì¡°ì • â†’ íŒ¨ë‹ ì œê±°
     * í´ë¼ì´ë§¥ìŠ¤ ì „ì˜ ëŠë¦° ë¹Œë“œì—… â†’ í´ë¼ì´ë§¥ìŠ¤ ì§ì „ìœ¼ë¡œ ì´ë™
     * ì§€ë£¨í•œ ì„¤ì • ìƒ· â†’ ì™„ì „íˆ ì œê±°
   - ëª©í‘œ: ì‹œì²­ìë¥¼ ì¦‰ì‹œ ì¢‹ì€ ë¶€ë¶„ìœ¼ë¡œ ë°ë ¤ê°€ê¸°
   - ì¼ë°˜ì ì¸ trim: 2-5ì´ˆ, ë§¤ìš° ëŠë¦° ì¸íŠ¸ë¡œì˜ ê²½ìš° 8-10ì´ˆ
   - í™•ì‹¤í•˜ì§€ ì•Šì„ ë•ŒëŠ” ë” ë§ì´ ìë¥´ì„¸ìš”

3. ë‚˜ë ˆì´ì…˜ ìŠ¤íƒ€ì¼ (ì§§ê³  ê°•ë ¬í•˜ê²Œ):
   - ë§¤ìš° ì§§ê²Œ - ìµœëŒ€ 5-6ë‹¨ì–´ë§Œ
   - ì„íŒ©íŠ¸ ìˆê³  ê¸°ì–µì— ë‚¨ëŠ” ë‹¨ì–´ ì‚¬ìš©
   - ë¶ˆí•„ìš”í•œ í˜•ìš©ì‚¬ë‚˜ í•„ëŸ¬ ë‹¨ì–´ ì—†ì´
   - ì˜ˆì‹œ: "ì™„ë²½í•œ ì°©ì§€", "ì¹´ë¥´ë§ˆì˜ ì—­ìŠµ", "ì–¼ìŒì´ ì´ê²¼ë‹¤"
   - **ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ ì™„ì „í•œ ì¢…ê²° ì–´ë¯¸(ì˜ˆ: \"ì…ë‹ˆë‹¤\", \"ì´ì—ìš”\", \"ì´ë„¤ìš”\", \"ì´ì£ \", \"ì¸ë°ìš”\")ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.** ì¤‘ê°„ì— ëŠê¸´ êµ¬ë‚˜ ëª…ì‚¬í˜•ìœ¼ë¡œ ëë‚˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.

4. íƒ€ì´ë° ê·œì¹™ (ì›ë³¸ ë¹„ë””ì˜¤ ê¸¸ì´: {duration_text}):
   - **ì¤‘ìš”**: ì •í™•íˆ 1ê°œì˜ ë‚˜ë ˆì´ì…˜ ë¼ì¸ë§Œ ìƒì„±í•˜ì„¸ìš”. ì—¬ëŸ¬ ë¼ì¸ ìƒì„± ê¸ˆì§€.
   - **ì¤‘ìš”**: ëª¨ë“  íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” ì›ë³¸ ë¹„ë””ì˜¤ ê¸°ì¤€ì´ì–´ì•¼ í•©ë‹ˆë‹¤ (trim ì „)
   - Trimì€ ìë™ìœ¼ë¡œ ì ìš©ë˜ë©° íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì¡°ì •ë©ë‹ˆë‹¤
   - ë‹¨ í•˜ë‚˜ì˜ ë‚˜ë ˆì´ì…˜ ì„¸ê·¸ë¨¼íŠ¸ë§Œ ìƒì„±
   - **ì§§ê²Œ ìœ ì§€**: ìµœëŒ€ 5-6ë‹¨ì–´ë§Œ (ì•½ 1-1.5ì´ˆ ë¶„ëŸ‰)
   - ê°•ë ¬í•˜ê³  ì„íŒ©íŠ¸ ìˆëŠ” ë‹¨ì–´ ì‚¬ìš© - í•„ëŸ¬ ì—†ì´
   - ì˜ˆì‹œ: "ì™„ë²½í•œ ì°©ì§€", "ì–¼ìŒì˜ ìŠ¹ë¦¬", "ì¦‰ê°ì ì¸ ì¹´ë¥´ë§ˆ"
   - ë¼ì¸ì€ {duration_text} ì´ì „ì— ëë‚˜ì•¼ í•©ë‹ˆë‹¤
   - ì—¬ëŸ¬ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë‚˜ëˆ„ì§€ ë§ˆì„¸ìš” - ì´ 1ê°œ ë¼ì¸ë§Œ

5. ì œëª©, ì„¤ëª…:
   - YouTube ì œëª©: ìµœëŒ€ 3ë‹¨ì–´ë§Œ. ì§§ê³  ê°•ë ¬í•˜ê²Œ.
   - ì„¤ëª…: 2-4ë¬¸ì¥ìœ¼ë¡œ ìŠ¤í† ë¦¬ë¥¼ ì„¤ì •í•˜ê³  ì™œ ë³¼ ê°€ì¹˜ê°€ ìˆëŠ”ì§€ ì•”ì‹œ.

6. ë¦¬ì•¡ì…˜ ë¹„ë””ì˜¤ ì„ íƒ:
ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡œë§ˆí‚¤ í´ë¦½ (ì •í™•íˆ í•˜ë‚˜ ì„ íƒ):
{chromakey_list}

í´ë¦½ì˜ ê°ì •ì  ë¶„ìœ„ê¸°ì™€ ë§¤ì¹­:
- "Green Screen Laughing Dog Meme.mp4" â†’ ì¬ë¯¸ìˆê±°ë‚˜ ì¦ê±°ìš´ ìˆœê°„
- "laughing.mp4" â†’ ê°€ë²¼ìš´ ë˜ëŠ” ì¦ê±°ìš´ ìƒí™©
- "surprised.mp4" â†’ ì˜ˆìƒì¹˜ ëª»í•œ ë˜ëŠ” ì¶©ê²©ì ì¸ í”Œë ˆì´
- "wow.mp4" â†’ ì¸ìƒì ì´ê±°ë‚˜ ìˆ™ë ¨ëœ ìˆœê°„
- "wtf.mp4" â†’ íŠ¹ì´í•˜ê±°ë‚˜ í˜¼ë€ìŠ¤ëŸ¬ìš´ ìƒí™©

ì‹œì²­ìì—ê²Œ ìì—°ìŠ¤ëŸ¬ì›Œ ë³´ì¼ ê²ƒì„ ì„ íƒí•˜ì„¸ìš”.

7. ì¶œë ¥ í˜•ì‹ (ëª¨ë‘ í•œêµ­ì–´ë¡œ):
=== ë¶„ì„ ===
ë¹„ë””ì˜¤ ìš”ì•½: [ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€, ì™œ ê°ì •ì ìœ¼ë¡œ ì¤‘ìš”í•œì§€ 2-3ë¬¸ì¥ìœ¼ë¡œ]
í•µì‹¬ ìš”ì†Œ: [ë“œë¼ë§ˆí‹±í•œ ë¹„íŠ¸, ê¸´ì¥ í¬ì¸íŠ¸, í´ë¼ì´ë§¥ìŠ¤ ìˆœê°„ì— ëŒ€í•œ ë¶ˆë¦¿ ìŠ¤íƒ€ì¼ ì„¤ëª…]
ë‚˜ë ˆì´ì…˜ ì „ëµ: [ì‹œì²­ìë¥¼ ì–´ë–»ê²Œ ëŒì–´ë“¤ì´ê³ , ê¸´ì¥ì„ êµ¬ì¶•í•˜ê³ , í´ë¼ì´ë§¥ìŠ¤ë¥¼ ì „ë‹¬í• ì§€ ì„¤ëª…]

=== ìŠ¤í¬ë¦½íŠ¸ ===
**ì¤‘ìš”**: ë‹¨ í•˜ë‚˜ì˜ ë‚˜ë ˆì´ì…˜ ë¼ì¸ë§Œ ìƒì„±í•˜ì„¸ìš”. 2ê°œë„, 3ê°œë„ ì•„ë‹Œ, ë”± 1ê°œ!
**ì¤‘ìš”**: ìµœëŒ€ 5-6ë‹¨ì–´ë§Œ. ì§§ê³  ê°•ë ¬í•˜ê²Œ.
**ì¤‘ìš”**: ì›ë³¸ ë¹„ë””ì˜¤ ê¸¸ì´ëŠ” {duration_text}ì…ë‹ˆë‹¤. ì›ë³¸ ë¹„ë””ì˜¤ ê¸°ì¤€ íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš©.
(ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ¬ìš´ ì¢…ê²°ì–´ë¯¸ë¡œ ëë‚˜ ì™„ì „í•œ ë¬¸ì¥ì´ ë˜ë„ë¡ í•˜ì„¸ìš”.)
(MM:SS - MM:SS) [ì§§ê³  ê°•ë ¬í•œ êµ¬ë¬¸ - ìµœëŒ€ 5-6ë‹¨ì–´ - {duration_text} ì´ì „ì— ëë‚˜ì•¼ í•¨]

ì˜ˆì‹œ (ì–¼ë§ˆë‚˜ ì§§ì€ì§€ ì£¼ëª©):
(00:02 - 00:04) ì™„ë²½í•œ ì°©ì§€ì˜€ì–´ìš”.
(00:01 - 00:03) ì–¼ìŒì´ í•­ìƒ ì´ê¸°ë„¤ìš”.
(00:03 - 00:05) ì¦‰ê°ì ì¸ ì¹´ë¥´ë§ˆì…ë‹ˆë‹¤.

=== ì‹œì‘ ë¶€ë¶„ ìë¥´ê¸° ===
**ì¤‘ìš”**: ì‹œì‘ ë¶€ë¶„ì˜ ëª¨ë“  ì§€ë£¨í•œ ì½˜í…ì¸ ë¥¼ ì˜ë¼ë‚´ì„¸ìš”. ì ê·¹ì ìœ¼ë¡œ!
Trim Start: X.X ì´ˆ [ì™œ ì´ ë¶€ë¶„ì„ ìë¥´ëŠ”ì§€ êµ¬ì²´ì ì¸ ì´ìœ ]

ê·œì¹™:
- í¥ë¯¸ë¡­ê±°ë‚˜ ì¤‘ìš”í•œ ì¼ì´ ë°œìƒí•˜ëŠ” ì •í™•í•œ ì´ˆë¥¼ ì°¾ìœ¼ì„¸ìš”
- ê·¸ ìˆœê°„ ì´ì „ì˜ ëª¨ë“  ê²ƒì„ ì˜ë¼ë‚´ì„¸ìš”
- ì†Œìˆ˜ì  ì •ë°€ë„ ì‚¬ìš© (ì˜ˆ: 3.5, 4.2, 7.8)
- ì•¡ì…˜ì´ ì¦‰ì‹œ ì‹œì‘í•˜ëŠ” ê²½ìš°ì—ë§Œ 0.0ìœ¼ë¡œ ì„¤ì • (0.5ì´ˆ ì´ë‚´)
- í™•ì‹¤í•˜ì§€ ì•Šì„ ë•ŒëŠ” ë” ë§ì´ ìë¥´ì„¸ìš” (ëœ ìë¥´ì§€ ë§ˆì„¸ìš”)

ì˜ˆì‹œ:
- Trim Start: 3.5 ì´ˆ [ë©”ì¸ ì•¡ì…˜ ì „ì— ì²œì²œíˆ ê±·ëŠ” ì‚¬ëŒ]
- Trim Start: 5.0 ì´ˆ [ì¹´ë©”ë¼ íŒ¨ë‹ ë° ì¡°ì •]
- Trim Start: 2.2 ì´ˆ [ì§€ë£¨í•œ ì„¤ì • ìƒ·]
- Trim Start: 0.0 ì´ˆ [ì•¡ì…˜ì´ ì¦‰ì‹œ ì‹œì‘]

Key moment: MM:SS [ê°€ì¥ í™”ë ¤í•œ ì•¡ì…˜/í´ë¼ì´ë§¥ìŠ¤ê°€ í„°ì§€ëŠ” ì •í™•í•œ ì´ˆ (trim ì ìš© í›„)]
Background Music: [Yes/No]
Reaction Video: [ë¦¬ìŠ¤íŠ¸ì—ì„œ ì •í™•í•œ íŒŒì¼ëª…]
YouTube Title: [ìµœëŒ€ 3ë‹¨ì–´ - ì§§ê³  ê°•ë ¬í•˜ê²Œ]
YouTube Description: [ë‚´ëŸ¬í‹°ë¸Œë¥¼ ì„¤ì •í•˜ê³  í´ë¼ì´ë§¥ìŠ¤ë¥¼ ì•”ì‹œí•˜ëŠ” 2-4ê°œì˜ í•œêµ­ì–´ ë¬¸ì¥, ê´€ë ¨ í•´ì‹œíƒœê·¸ í¬í•¨]

ë¹„ë””ì˜¤ ì •ë³´:
- **ë¹„ë””ì˜¤ ê¸¸ì´: {duration_text} - ì–´ë–¤ íƒ€ì„ìŠ¤íƒ¬í”„ë„ ì´ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ì§€ ë§ˆì„¸ìš”!**
- ì´ˆì : ì„¤ì •, ê¸´ì¥, í´ë¼ì´ë§¥ìŠ¤, í•´ê²°ì„ í¬í•¨í•œ ì´ì•¼ê¸° êµ¬ì¡°
- ìŠ¤íƒ€ì¼: ë“œë¼ë§ˆ, ê°ì •, ì‹œì²­ìë¥¼ ê³„ì† ë³´ê²Œ ë§Œë“œëŠ” í›…ì„ ì‚¬ìš©í•œ ë§¤ë ¥ì ì¸ ìŠ¤í† ë¦¬í…”ë§
- ëª©í‘œ: ê°ì •ì  ì—°ê²° ìƒì„±, ì‹œì²­ìê°€ í¥ë¶„, ë†€ë¼ì›€, ìœ ë¨¸ë¥¼ ëŠë¼ê²Œ ë§Œë“¤ê¸°
- **íƒ€ì´ë° ì œì•½**: ê° ë‚˜ë ˆì´ì…˜ ë¼ì¸ì€ ì•½ 2-3ì´ˆ. {duration_text} ë‚´ì— ëª¨ë“  ë‚˜ë ˆì´ì…˜ì´ ë“¤ì–´ê°€ë„ë¡ íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°.
- **ì¤‘ìš”: ëª¨ë“  ë‚˜ë ˆì´ì…˜ ë¼ì¸, ìë§‰ í…ìŠ¤íŠ¸, ì œëª©, ì„¤ëª…ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.**
- ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ë§ˆí¬ë‹¤ìš´ ì—†ì´ ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""
    else:
        # ì˜ì–´ í”„ë¡¬í”„íŠ¸
        instructions = f"""
You are a compelling storyteller who transforms sports moments into engaging narratives with drama and emotion.

â±ï¸ **CRITICAL CONSTRAINT**: This video is exactly {duration_text} long. ALL narration timestamps MUST fit within this duration!

ğŸ¯ Required analysis:
1. Watch the entire clip and craft a story:
   - What's the dramatic setup? (the situation, the stakes)
   - What's the conflict or challenge being faced?
   - What's the unexpected twist or climactic moment?
   - How does it resolve? (success, failure, surprise)
   - The exact timestamp of the most critical moment.
   - This MUST be the flashiest, most intense action beat (not crowd shots or slow reactions) â€” the instant viewers would replay.

2. Build the narrative context:
   - Set the scene: who, where, what sport/activity
   - Create tension: what makes this moment high-stakes or interesting?
   - Find the human element: effort, skill, surprise, humor, or irony
   - Highlight the payoff: why viewers should care about the outcome

3. **CRITICAL - Trim unnecessary intro (BE VERY AGGRESSIVE)**:
   - Watch the video and identify EXACTLY when the interesting/important action starts
   - Cut EVERYTHING before that moment - boring intros, setup, waiting, camera adjustments, slow lead-ins
   - **DO NOT be conservative** - if the first 3-5 seconds are not critical, CUT THEM
   - Only set trim to 0 seconds if the video IMMEDIATELY starts with critical action (within first 0.5 seconds)
   - Common scenarios:
     * Person walking/approaching before action â†’ CUT the walking part
     * Camera panning/adjusting before main subject â†’ CUT the panning
     * Slow buildup before payoff â†’ CUT to right before the payoff
     * Boring establishing shots â†’ CUT them completely
   - Your goal: Get viewers to the good part INSTANTLY
   - Typical trim: 2-5 seconds, but can be 8-10 seconds for very slow intros
   - When in doubt, trim MORE rather than less

4. Narration style (SHORT and PUNCHY):
   - Keep it EXTREMELY brief - just 5-6 words maximum
   - Use impactful, memorable words
   - No unnecessary adjectives or filler words
   - Think: "Ice wins again" not "The ice completely defeats her once more"
   - Examples: "She crushed it", "Karma strikes back", "Perfect landing nailed"

5. Timing rules (ORIGINAL VIDEO LENGTH: {duration_text}):
   - **CRITICAL**: Deliver EXACTLY 1 narration line ONLY. DO NOT create multiple lines.
   - **IMPORTANT**: All timestamps should be based on the ORIGINAL video (before trim)
   - The trim will be applied automatically, and timestamps will be adjusted
   - Create only ONE single narration segment
   - **KEEP IT SHORT**: Maximum 5-6 words ONLY (about 1-1.5 seconds of speech)
   - Use punchy, impactful words - no filler
   - Examples: "She nailed the move", "Ice wins again", "Karma strikes hard"
   - The line MUST finish before {duration_text}
   - DO NOT split into multiple segments - just ONE line total

6. Title, description:
   - YouTube title: MAXIMUM 3 words ONLY. Keep it SHORT and punchy.
   # - Thumbnail title: 3â€“4 punchy English words that tease the story or surprise. (ë¹„í™œì„±í™”ë¨)
   - Description: 2â€“4 sentences that set up the story and hint at why it's worth watching.

7. Reaction video picker:
Available chroma-key clips (choose exactly one):
{chromakey_list}

Match the clip's emotional beat:
- "Green Screen Laughing Dog Meme.mp4" â†’ funny or entertaining moments
- "laughing.mp4" â†’ lighthearted or fun situations
- "surprised.mp4" â†’ unexpected or shocking plays
- "wow.mp4" â†’ impressive or skillful moments
- "wtf.mp4" â†’ unusual or confusing situations

Choose what would look natural for the viewer.

8. Output format (everything in English):
=== ANALYSIS ===
Video Summary: [2â€“3 sentences about the story: what happens, why it matters emotionally]
Key Elements: [bullet-style description of dramatic beats, tension points, and payoff moments]
Narration Strategy: [explain the narrative arc - how you'll hook viewers, build tension, and deliver the payoff]

=== SCRIPT ===
**CRITICAL**: Create ONLY ONE narration line. NOT 2, NOT 3, JUST 1!
**CRITICAL**: Maximum 5-6 words ONLY. Keep it SHORT and PUNCHY.
**IMPORTANT**: Original video duration is {duration_text}. Use timestamps based on ORIGINAL video.
(MM:SS - MM:SS) [Short punchy phrase - 5-6 words MAX - MUST finish before {duration_text}]

Examples (notice how SHORT they are):
(00:02 - 00:04) She nailed the move.
(00:01 - 00:03) Ice wins every time.
(00:03 - 00:05) Karma strikes instantly.

=== TRIM START ===
**CRITICAL**: Trim ALL boring content from the start. Be AGGRESSIVE!
Trim Start: X.X seconds [specific reason why you're cutting this]

Rules:
- Find the EXACT second when something interesting/important happens
- Cut everything before that moment
- Use decimal precision (e.g., 3.5, 4.2, 7.8)
- Only set to 0.0 if action starts IMMEDIATELY (within 0.5 seconds)
- When unsure, trim MORE (not less)

Examples:
- Trim Start: 3.5 seconds [person walking slowly before main action]
- Trim Start: 5.0 seconds [camera panning and adjusting]
- Trim Start: 2.2 seconds [boring establishing shot]
- Trim Start: 0.0 seconds [action starts immediately]

Key moment: MM:SS [timestamp of the flashiest, most intense action beat - AFTER trim is applied]
# Thumbnail Title: [3â€“4 word English phrase] (ë¹„í™œì„±í™”ë¨ - ì¸ë„¤ì¼ íƒ€ì´í‹€ ì‚¬ìš© ì•ˆ í•¨)
# Core Keyword: [1â€“3 English words pulled from the thumbnail] (ë¹„í™œì„±í™”ë¨ - í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸ ì‚¬ìš© ì•ˆ í•¨)
Background Music: [Yes/No]
Reaction Video: [exact filename from the list]
YouTube Title: [MAXIMUM 3 words - SHORT and punchy]
YouTube Description: [2â€“4 English sentences that set up the narrative and tease the payoff, plus relevant hashtags]

Video info:
- **VIDEO DURATION: {duration_text} - DO NOT exceed this length in any timestamp!**
- Focus: narrative arc with setup, tension, climax, and resolution
- Style: engaging storytelling with drama, emotion, and hooks that keep viewers watching
- Goal: create an emotional connection and make viewers feel the excitement, surprise, or humor
- **Timing constraint**: Each narration line is ~2-3 seconds. Calculate timestamps to fit ALL narration within {duration_text}.
- **Important: write every narration line, subtitle text, thumbnail copy, title, and description in natural English.**
- Output plain text only with zero extra commentary or Markdown.
"""
    return textwrap.dedent(instructions).strip()


def _get_gemini_file_state(file_obj) -> str:
    """Gemini íŒŒì¼ ìƒíƒœ ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    state = getattr(file_obj, "state", None)
    if hasattr(state, "name"):
        return str(state.name)
    if isinstance(state, int):
        state_map = {
            0: "STATE_UNSPECIFIED",
            1: "PROCESSING",
            2: "ACTIVE",
            3: "FAILED",
        }
        return state_map.get(state, str(state))
    if isinstance(state, str):
        return state
    return str(state)


def generate_script_with_gemini(video_path: str) -> str:
    """302.ai APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    ai_settings = get_config_value(["ai_settings"], {}) or {}
    if not ai_settings.get("enabled", True):
        raise RuntimeError("AI ìë™ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

    clip = VideoFileClip(video_path)
    try:
        duration = clip.duration or 0.0
    finally:
        clip.close()

    config = _get_302ai_config()
    prompt = _build_gemini_prompt(duration)

    print("\n[AI] 302.ai APIë¡œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘...")
    print(f"   ë¹„ë””ì˜¤: {os.path.basename(video_path)}")
    print(f"   ê¸¸ì´: {duration:.2f}ì´ˆ")

    # 302.ai chat completions API í˜¸ì¶œ
    url = f"{config['base_url']}/chat/completions"
    headers = {
        "Authorization": f"Bearer {config['api_key']}",
        "Content-Type": "application/json"
    }

    # ë¹„ë””ì˜¤ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸
    video_context = f"ì´ ë¹„ë””ì˜¤ëŠ” {duration:.1f}ì´ˆ ê¸¸ì´ì˜ ì†Œì…œ ë¯¸ë””ì–´ ì‡¼ì¸  ë¹„ë””ì˜¤ì…ë‹ˆë‹¤. íŒŒì¼ëª…: {os.path.basename(video_path)}"
    full_prompt = f"{video_context}\n\n{prompt}"

    data = {
        "model": config['model'],
        "messages": [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì†Œì…œ ë¯¸ë””ì–´ ì‡¼ì¸  ë¹„ë””ì˜¤ë¥¼ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": full_prompt}
        ],
        "temperature": ai_settings.get("generation_config", {}).get("temperature", 0.7),
        "max_tokens": 2000
    }

    # API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    max_retries = 3
    retry_delay = 1.0

    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=data, timeout=int(ai_settings.get("timeout", 60)))

            if response.status_code == 200:
                result = response.json()
                script_text = result['choices'][0]['message']['content'].strip()
                print("[OK] 302.ai ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì™„ë£Œ")
                return script_text
            else:
                error_text = response.text
                raise RuntimeError(f"API ì˜¤ë¥˜ (HTTP {response.status_code}): {error_text}")

        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                print(f"[WARNING] API íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{max_retries})")
                print(f"[INFO] {retry_delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                raise RuntimeError(f"API íƒ€ì„ì•„ì›ƒ ({max_retries}íšŒ ì¬ì‹œë„ í›„)")

        except Exception as exc:
            if attempt < max_retries - 1:
                print(f"[WARNING] API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {exc}")
                print(f"[INFO] {retry_delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                raise RuntimeError(f"ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì‹¤íŒ¨ ({max_retries}íšŒ ì¬ì‹œë„ í›„): {exc}") from exc


def parse_script(text):
    """
    íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ìŠ¤í¬ë¦½íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„°ë¡œ ë³€í™˜

    Args:
        text (str): í˜•ì‹:
            === ANALYSIS ===
            Video Summary: ...
            Key Elements: ...
            Narration Strategy: ...
            === SCRIPT ===
            (MM:SS - MM:SS) í…ìŠ¤íŠ¸ ë‚´ìš©
            Key moment: MM:SS
            Thumbnail Title: ì œëª©
            Background Music: ì˜µì…˜

    Returns:
        tuple: (segments, metadata)
            segments: [{'start': ì´ˆ, 'end': ì´ˆ, 'text': 'ë‚´ìš©'}]
            metadata: {'key_moment': ì´ˆ, 'thumbnail_title': 'ì œëª©', 'background_music': 'ì˜µì…˜', 'analysis': {...}}
    """
    segments = []
    metadata = {
        'key_moment': None,
        'thumbnail_title': None,
        'background_music': None,
        'core_keyword': None,
        'youtube_title': None,
        'youtube_description': None,
        'reaction_video': None,
        'trim_start': 0.0,
        'analysis': {
            'video_summary': None,
            'key_elements': None,
            'narration_strategy': None
        }
    }

    # ANALYSIS ì„¹ì…˜ ì¶”ì¶œ
    analysis_match = re.search(
        r'===\s*(?:ANALYSIS|ë¶„ì„)\s*===(.*?)(?:===\s*(?:SCRIPT|ìŠ¤í¬ë¦½íŠ¸)\s*===|$)',
        text,
        re.DOTALL | re.IGNORECASE
    )
    if analysis_match:
        analysis_text = analysis_match.group(1).strip()

        # Video Summary ì¶”ì¶œ
        summary_match = re.search(
            r'(?:Video Summary|ë¹„ë””ì˜¤ ìš”ì•½):\s*(.*?)(?:\n|$)',
            analysis_text,
            re.IGNORECASE
        )
        if summary_match:
            metadata['analysis']['video_summary'] = summary_match.group(1).strip()

        # Key Elements ì¶”ì¶œ
        elements_match = re.search(
            r'(?:Key Elements|í•µì‹¬ ìš”ì†Œ):\s*(.*?)(?:\n|$)',
            analysis_text,
            re.IGNORECASE
        )
        if elements_match:
            metadata['analysis']['key_elements'] = elements_match.group(1).strip()

        # Narration Strategy ì¶”ì¶œ
        strategy_match = re.search(
            r'(?:Narration Strategy|ë‚˜ë ˆì´ì…˜ ì „ëµ):\s*(.*?)(?:\n|$)',
            analysis_text,
            re.IGNORECASE
        )
        if strategy_match:
            metadata['analysis']['narration_strategy'] = strategy_match.group(1).strip()

        # ë¶„ì„ ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
        print("\n" + "="*60)
        print("[AI ë¶„ì„] ë¹„ë””ì˜¤ ë¶„ì„ ê²°ê³¼")
        print("="*60)
        if metadata['analysis']['video_summary']:
            print(f"\nğŸ“¹ ì£¼ìš” ë‚´ìš©:\n   {metadata['analysis']['video_summary']}")
        if metadata['analysis']['key_elements']:
            print(f"\nğŸ¯ í•µì‹¬ ìš”ì†Œ:\n   {metadata['analysis']['key_elements']}")
        if metadata['analysis']['narration_strategy']:
            print(f"\nğŸ’¬ ëŒ€ì‚¬ ì „ëµ:\n   {metadata['analysis']['narration_strategy']}")
        print("\n" + "="*60)

    lines = text.strip().split('\n')

    # SCRIPT ì„¹ì…˜ê³¼ CUT SEGMENTS ì„¹ì…˜ ë¶„ë¦¬
    script_section = []
    in_script_section = False

    script_header_patterns = [
        r'=+\s*SCRIPT\s*=+',
        r'=+\s*ìŠ¤í¬ë¦½íŠ¸\s*=+',
    ]
    trim_header_patterns = [
        r'=+\s*TRIM\s+START\s*=+',
        r'=+\s*ì‹œì‘\s*ë¶€ë¶„\s*ìë¥´ê¸°\s*=+',
    ]

    for line in lines:
        line_stripped = line.strip()
        if any(re.match(pattern, line_stripped, re.IGNORECASE) for pattern in script_header_patterns):
            in_script_section = True
            continue
        elif any(re.match(pattern, line_stripped, re.IGNORECASE) for pattern in trim_header_patterns):
            in_script_section = False
            continue

        if in_script_section:
            script_section.append(line_stripped)

    # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ë“¤
    dialogue_pattern = r'\((\d{2}:\d{2}(?:\.\d+)?)\s*-\s*(\d{2}:\d{2}(?:\.\d+)?)\)\s*(.*)'
    key_moment_pattern = r'(?:Key moment|Most important timeline):\s*(?:(\d{2}:\d{2})|(\d+)\s*seconds?)'
    thumbnail_pattern = r'Thumbnail Title:\s*(.*)'
    background_pattern = r'Background Music:\s*(.*)'
    core_keyword_pattern = r'Core Keyword:\s*(.*)'
    youtube_title_pattern = r'YouTube Title:\s*(.*)'
    youtube_desc_pattern = r'YouTube Description:\s*(.*)'
    reaction_video_pattern = r'Reaction Video:\s*(.*)'

    # SCRIPT ì„¹ì…˜ì—ì„œë§Œ ëŒ€ì‚¬ íŒŒì‹±
    for line in script_section:
        dialogue_match = re.match(dialogue_pattern, line, flags=re.IGNORECASE)
        if dialogue_match:
            start_time = dialogue_match.group(1)
            end_time = dialogue_match.group(2)
            text_content = dialogue_match.group(3).strip()

            # [ ]ë¡œ ì‹œì‘í•˜ëŠ” ì£¼ì„/ì„¤ëª…ì€ ì œì™¸
            if text_content and not text_content.startswith('['):
                segments.append({
                    'start': time_to_seconds(start_time),
                    'end': time_to_seconds(end_time),
                    'text': text_content
                })

    # ë©”íƒ€ë°ì´í„°ëŠ” ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ íŒŒì‹±
    for line in lines:
        line = line.strip()

        # Key moment íŒŒì‹±
        key_match = re.match(key_moment_pattern, line, flags=re.IGNORECASE)
        if key_match:
            # MM:SS í˜•ì‹ ë˜ëŠ” ìˆ«ì(ì´ˆ) í˜•ì‹ ë‘˜ ë‹¤ ì§€ì›
            if key_match.group(1):  # MM:SS í˜•ì‹
                metadata['key_moment'] = time_to_seconds(key_match.group(1))
            elif key_match.group(2):  # ìˆ«ì(ì´ˆ) í˜•ì‹
                metadata['key_moment'] = int(key_match.group(2))
            continue

        # Thumbnail title íŒŒì‹± - ë¹„í™œì„±í™”ë¨
        # thumb_match = re.match(thumbnail_pattern, line, flags=re.IGNORECASE)
        # if thumb_match:
        #     metadata['thumbnail_title'] = thumb_match.group(1).strip()
        #     continue

        # Background music íŒŒì‹±
        background_match = re.match(background_pattern, line, flags=re.IGNORECASE)
        if background_match:
            bg_text = background_match.group(1).strip().lower()
            # "no" ë˜ëŠ” "no background music is present" ê°™ì€ ë¬¸ì¥ ëª¨ë‘ ì²˜ë¦¬
            if 'no' in bg_text:
                metadata['background_music'] = 'no'
            else:
                metadata['background_music'] = background_match.group(1).strip()
            continue

        # Core keyword íŒŒì‹± - ë¹„í™œì„±í™”ë¨
        # keyword_match = re.match(core_keyword_pattern, line, flags=re.IGNORECASE)
        # if keyword_match:
        #     keyword_value = keyword_match.group(1).strip()
        #     if len(keyword_value) >= 2 and keyword_value[0] == keyword_value[-1] and keyword_value[0] in {"'", '"'}:
        #         keyword_value = keyword_value[1:-1].strip()
        #     metadata['core_keyword'] = keyword_value
        #     continue

        # YouTube title íŒŒì‹±
        yt_title_match = re.match(youtube_title_pattern, line, flags=re.IGNORECASE)
        if yt_title_match:
            metadata['youtube_title'] = yt_title_match.group(1).strip()
            continue

        # YouTube description íŒŒì‹±
        yt_desc_match = re.match(youtube_desc_pattern, line, flags=re.IGNORECASE)
        if yt_desc_match:
            metadata['youtube_description'] = yt_desc_match.group(1).strip()
            continue

        # Reaction video íŒŒì‹±
        reaction_match = re.match(reaction_video_pattern, line, flags=re.IGNORECASE)
        if reaction_match:
            reaction_filename = reaction_match.group(1).strip()
            # ë”°ì˜´í‘œ ì œê±°
            if len(reaction_filename) >= 2 and reaction_filename[0] == reaction_filename[-1] and reaction_filename[0] in {"'", '"'}:
                reaction_filename = reaction_filename[1:-1].strip()
            metadata['reaction_video'] = reaction_filename
            continue

    # TRIM START ì„¹ì…˜ ì¶”ì¶œ
    trim_start_match = re.search(
        r'===\s*(?:TRIM\s+START|ì‹œì‘\s*ë¶€ë¶„\s*ìë¥´ê¸°)\s*===(.*?)(?:Key moment|Background Music|Reaction Video|YouTube Title|YouTube Description|$)',
        text,
        re.DOTALL | re.IGNORECASE
    )
    if trim_start_match:
        trim_text = trim_start_match.group(1).strip()
        # "Trim Start: X.X seconds" í˜•ì‹ ì¶”ì¶œ
        trim_pattern = r'Trim Start:\s*([\d.]+)\s*(?:seconds?|ì´ˆ)'
        trim_match = re.search(trim_pattern, trim_text, re.IGNORECASE)

        if trim_match:
            trim_seconds = float(trim_match.group(1))
            metadata['trim_start'] = trim_seconds
            print(f"\nâœ‚ï¸  ì•ë¶€ë¶„ ì œê±°: {trim_seconds:.2f}ì´ˆ")
        else:
            print(f"[WARNING] TRIM START ê°’ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 0ì´ˆ ì‚¬ìš©.")

    return segments, metadata


def time_to_seconds(time_str):
    """
    MM:SS ë˜ëŠ” MM:SS.X í˜•ì‹ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜

    Args:
        time_str (str): "MM:SS" ë˜ëŠ” "MM:SS.X" í˜•ì‹

    Returns:
        float: ì´ ì´ˆ
    """
    parts = time_str.split(':')
    minutes = int(parts[0])
    seconds = float(parts[1])
    return minutes * 60 + seconds


def get_random_sound_effect():
    """sound effects í´ë”ì—ì„œ ëœë¤ ì˜¤ë””ì˜¤ íŒŒì¼ ì„ íƒ"""
    sound_effects_dir = "sound effects"

    if not os.path.exists(sound_effects_dir):
        print(f"[WARNING] sound effects í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sound_effects_dir}")
        return None

    audio_files = [
        f for f in os.listdir(sound_effects_dir)
        if f.lower().endswith(('.mp3', '.wav', '.m4a', '.aac'))
    ]

    if not audio_files:
        print(f"[WARNING] sound effects í´ë”ì— ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    selected = random.choice(audio_files)
    return os.path.join(sound_effects_dir, selected)


def get_random_background_music():
    """background music í´ë”ì—ì„œ ëœë¤ ì˜¤ë””ì˜¤ íŒŒì¼ ì„ íƒ"""
    music_dir = "background music"

    if not os.path.exists(music_dir):
        print(f"[WARNING] background music í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {music_dir}")
        return None

    audio_files = [
        f for f in os.listdir(music_dir)
        if f.lower().endswith(('.mp3', '.wav', '.m4a', '.aac'))
    ]

    if not audio_files:
        print("[WARNING] background music í´ë”ì— ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    selected = random.choice(audio_files)
    return os.path.join(music_dir, selected)


def get_start_sound():
    """start sound í´ë”ì—ì„œ ëœë¤ ì˜¤ë””ì˜¤ íŒŒì¼ ì„ íƒ"""
    start_sound_dir = "start sound"

    if not os.path.exists(start_sound_dir):
        print(f"[WARNING] start sound í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {start_sound_dir}")
        return None

    audio_files = [
        f for f in os.listdir(start_sound_dir)
        if f.lower().endswith(('.mp3', '.wav', '.m4a', '.aac'))
    ]

    if not audio_files:
        print("[WARNING] start sound í´ë”ì— ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    selected = random.choice(audio_files)
    return os.path.join(start_sound_dir, selected)


def generate_voice(text, output_path):
    """302.ai MiniMax TTSë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„±ì„ ìƒì„±"""
    try:
        generate_voice_minimax(text, output_path)
        apply_voice_profile(output_path)
        print(f"[OK] ìŒì„± ìƒì„± ì™„ë£Œ: {output_path}")
    except Exception as exc:
        raise RuntimeError(f"TTS ìŒì„± ìƒì„± ì‹¤íŒ¨: {exc}") from exc


def generate_voice_minimax(text, output_path):
    """302.ai MiniMax TTS APIë¡œ ìŒì„±ì„ ìƒì„±"""
    # API í‚¤ ê°€ì ¸ì˜¤ê¸° (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” config.json)
    api_key = os.getenv("AI_302_API_KEY")
    if not api_key:
        api_key = get_config_value(["minimax_settings", "api_key"])

    if not api_key:
        raise RuntimeError(
            "302.ai API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
            "AI_302_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ "
            "Config/config.jsonì˜ minimax_settings.api_keyë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
        )

    # API ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    base_url = get_config_value(["minimax_settings", "base_url"], "https://api.302.ai/v1")
    model = get_config_value(["minimax_settings", "model"], "speech-01-turbo")
    voice = get_config_value(["voice_settings", "voice"], "Korean_SweetGirl")
    speed = float(get_config_value(["voice_settings", "speed"], 1.0))

    # API ì—”ë“œí¬ì¸íŠ¸
    url = f"{base_url}/audio/speech"

    # ìš”ì²­ í—¤ë”
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # ìš”ì²­ ë°”ë””
    data = {
        "model": model,
        "input": text,
        "voice": voice,
        "response_format": "mp3",
        "speed": speed
    }

    print(f"[TTS] 302.ai MiniMax TTS í˜¸ì¶œ ì¤‘... (model: {model}, voice: {voice}, speed: {speed})")

    # ì¬ì‹œë„ ë¡œì§
    max_retries = 3
    retry_delay = 1.0

    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=data, timeout=30)

            if response.status_code == 200:
                # ì˜¤ë””ì˜¤ ë°ì´í„° ì €ì¥
                with open(output_path, "wb") as audio_file:
                    audio_file.write(response.content)
                print(f"[TTS] ìŒì„± ìƒì„± ì„±ê³µ: {len(response.content)} bytes")
                return
            else:
                error_text = response.text
                raise RuntimeError(f"API ì˜¤ë¥˜ (HTTP {response.status_code}): {error_text}")

        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                print(f"[WARNING] TTS API íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{max_retries})")
                print(f"[INFO] {retry_delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                raise RuntimeError(f"TTS API íƒ€ì„ì•„ì›ƒ ({max_retries}íšŒ ì¬ì‹œë„ í›„)")

        except Exception as exc:
            if attempt < max_retries - 1:
                print(f"[WARNING] TTS API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {exc}")
                print(f"[INFO] {retry_delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                raise RuntimeError(f"TTS ìŒì„± ìƒì„± ì‹¤íŒ¨ ({max_retries}íšŒ ì¬ì‹œë„ í›„): {exc}") from exc


def apply_voice_profile(audio_path):
    """ì„ íƒëœ ìŒì„± í”„ë¡œí•„ì— ë§ì¶° ìŒì„±ì„ í›„ì²˜ë¦¬"""
    profile_name = str(get_config_value(["voice_settings", "profile"], "default") or "default").strip()
    profile_key = profile_name.lower()
    if profile_key in {"", "default"}:
        return

    if not os.path.exists(audio_path):
        return

    if AudioSegment is None or PYDUB_IMPORT_ERROR is not None:
        missing_module = getattr(PYDUB_IMPORT_ERROR, "name", "pydub") if PYDUB_IMPORT_ERROR else "pydub"
        print(
            f"[WARNING]  '{profile_name}' ìŒì„± í”„ë¡œí•„ í›„ì²˜ë¦¬ë¥¼ ìœ„í•´ `pydub` ëª¨ë“ˆì´ í•„ìš”í•©ë‹ˆë‹¤. "
            f"í˜„ì¬ '{missing_module}'ì„(ë¥¼) ë¶ˆëŸ¬ì˜¤ì§€ ëª»í•´ í›„ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤."
        )
        return

    profile_config = get_config_value(["voice_profiles", profile_key], None)
    if not isinstance(profile_config, dict):
        print(f"[WARNING]  '{profile_name}' ìŒì„± í”„ë¡œí•„ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ í›„ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    playback_speed = float(profile_config.get("speed", 1.0))
    pitch_shift = float(profile_config.get("pitch_shift", 0.0))
    compression_threshold = float(profile_config.get("compression_threshold", -18.0))
    compression_ratio = float(profile_config.get("compression_ratio", 3.0))
    compression_attack = float(profile_config.get("compression_attack", 5.0))
    compression_release = float(profile_config.get("compression_release", 120.0))
    gain = float(profile_config.get("gain", 0.0))

    audio = AudioSegment.from_file(audio_path)
    original_frame_rate = audio.frame_rate

    if pitch_shift:
        audio = shift_pitch(audio, pitch_shift, original_frame_rate)

    if playback_speed > 1.0:
        audio = speedup(audio, playback_speed=playback_speed, chunk_size=50, crossfade=25)
    elif 0 < playback_speed < 1.0:
        slowed = audio._spawn(audio.raw_data, overrides={"frame_rate": int(audio.frame_rate * playback_speed)})
        audio = slowed.set_frame_rate(original_frame_rate)

    audio = compress_dynamic_range(
        audio,
        threshold=compression_threshold,
        ratio=compression_ratio,
        attack=compression_attack,
        release=compression_release,
    )

    if gain:
        audio = audio.apply_gain(gain)

    audio.export(audio_path, format="mp3")


def shift_pitch(audio_segment, semitones, frame_rate):
    """í”„ë ˆì„ ë ˆì´íŠ¸ë¥¼ ì¡°ì •í•˜ì—¬ ê°„ë‹¨íˆ í”¼ì¹˜ë¥¼ ì´ë™"""
    if semitones == 0:
        return audio_segment

    factor = 2 ** (semitones / 12)
    pitched = audio_segment._spawn(
        audio_segment.raw_data,
        overrides={"frame_rate": int(audio_segment.frame_rate * factor)}
    )
    return pitched.set_frame_rate(frame_rate)




def detect_scene_changes(video_clip, threshold=30.0, min_scene_duration=1.0):
    """í”„ë ˆì„ ê°„ ì°¨ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì”¬ ì „í™˜ ì‹œì ì„ ê°ì§€."""
    if threshold <= 0:
        return []

    fps = getattr(video_clip, "fps", None) or 24
    sample_fps = max(2, min(15, int(fps / 2) or 6))

    previous_frame = None
    last_change_time = -min_scene_duration
    change_times = []

    width, height = video_clip.size
    downsample_x = max(1, width // 320)
    downsample_y = max(1, height // 180)

    for index, frame in enumerate(video_clip.iter_frames(fps=sample_fps, dtype="uint8")):
        if downsample_x > 1 or downsample_y > 1:
            frame = frame[::downsample_y, ::downsample_x]

        gray = np.dot(frame[..., :3], [0.2989, 0.5870, 0.1140]).astype("float32")

        if previous_frame is not None:
            diff = np.mean(np.abs(gray - previous_frame))
            current_time = index / sample_fps
            if diff >= threshold and (current_time - last_change_time) >= min_scene_duration:
                change_times.append(current_time)
                last_change_time = current_time

        previous_frame = gray

    return change_times


def remove_chromakey(frame, color='green', threshold=100, blend=1):
    """
    Remove a chroma key background from an RGB frame.

    Args:
        frame: numpy array (H, W, 3) in RGB order.
        color: 'green' or 'blue' indicating the key color.
        threshold: Euclidean color distance treated as keyed (0-255).
        blend: Optional Gaussian blur strength for soft edges.

    Returns:
        Tuple[np.ndarray, np.ndarray]: (cleaned RGB frame, alpha mask 0.0-1.0).
    """
    threshold = float(np.clip(threshold, 0, 255))

    if color.lower() == 'green':
        key_color = np.array([0, 255, 0], dtype=np.float32)
    else:
        key_color = np.array([0, 0, 255], dtype=np.float32)

    frame_float = frame.astype(np.float32)
    distance = np.linalg.norm(frame_float - key_color, axis=2)

    alpha = np.ones(frame.shape[:2], dtype=np.float32)
    keyed_region = distance <= threshold
    alpha[keyed_region] = 0.0

    if blend > 0 and alpha.size:
        try:
            from scipy.ndimage import gaussian_filter
            alpha = gaussian_filter(alpha, sigma=blend)
        except ImportError:
            pass

    alpha = np.clip(alpha, 0.0, 1.0)

    cleaned_frame = frame.copy()
    cleaned_frame[alpha <= 0.01] = 0

    return cleaned_frame.astype(np.uint8), alpha.astype(np.float32)


def create_chromakey_overlay(reaction_video_filename, main_video_duration, scale=0.2, position=('left', 'top')):
    """
    í¬ë¡œë§ˆí‚¤ ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ê³  ì²˜ë¦¬í•˜ì—¬ ì˜¤ë²„ë ˆì´ í´ë¦½ ìƒì„±

    Args:
        reaction_video_filename: í¬ë¡œë§ˆí‚¤ ë¹„ë””ì˜¤ íŒŒì¼ ì´ë¦„
        main_video_duration: ë©”ì¸ ë¹„ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
        scale: ë¹„ë””ì˜¤ í¬ê¸° ë°°ìœ¨ (0.2 = 1/5)
        position: ìœ„ì¹˜ ('left', 'top' ë“±)

    Returns:
        VideoClip with transparency
    """
    chromakey_dir = "reaction chromakey"
    video_path = os.path.join(chromakey_dir, reaction_video_filename)

    if not os.path.exists(video_path):
        print(f"[WARNING] Chromakey video not found: {video_path}")
        return None

    print(f"\n[CHROMAKEY] Loading: {reaction_video_filename}")

    try:
        # ë¹„ë””ì˜¤ ë¡œë“œ
        reaction_clip = VideoFileClip(video_path)

        # ì˜¤ë””ì˜¤ ì œê±° (ë¬´ìŒ)
        reaction_clip = reaction_clip.with_audio(None)

        # ë¹„ë””ì˜¤ ë£¨í”„ (ë©”ì¸ ë¹„ë””ì˜¤ ê¸¸ì´ë§Œí¼ ë°˜ë³µ)
        if reaction_clip.duration < main_video_duration:
            # ë°˜ë³µ íšŸìˆ˜ ê³„ì‚°
            num_loops = int(np.ceil(main_video_duration / reaction_clip.duration))
            # ë£¨í”„ ìƒì„±
            clips_to_concat = [reaction_clip] * num_loops
            reaction_clip = concatenate_videoclips(clips_to_concat, method="compose")

        # ë©”ì¸ ë¹„ë””ì˜¤ ê¸¸ì´ì— ë§ì¶¤
        reaction_clip = reaction_clip.subclipped(0, main_video_duration)

        # í¬ë¡œë§ˆí‚¤ ì œê±° ì ìš©
        print(f"   Removing chromakey...")

        key_threshold = 100
        key_blend = 0  # Windows only uses blend=0
        base_clip = reaction_clip
        frame_cache = {}

        def get_processed_frame(t):
            cache_key = round(float(t), 4)
            if cache_key not in frame_cache:
                cleaned, alpha = remove_chromakey(
                    base_clip.get_frame(t),
                    color='green',
                    threshold=key_threshold,
                    blend=key_blend
                )
                frame_cache[cache_key] = (cleaned, alpha)
            return frame_cache[cache_key]

        processed_clip = VideoClip(
            frame_function=lambda t: get_processed_frame(t)[0],
            duration=base_clip.duration,
            has_constant_size=True
        )
        mask_clip = VideoClip(
            frame_function=lambda t: get_processed_frame(t)[1],
            duration=base_clip.duration,
            is_mask=True,
            has_constant_size=True
        )

        if isinstance(base_clip.size, tuple):
            processed_clip.size = base_clip.size
            mask_clip.size = base_clip.size

        base_fps = getattr(base_clip, "fps", None)
        if base_fps:
            processed_clip = processed_clip.with_fps(base_fps)
            mask_clip = mask_clip.with_fps(base_fps)

        reaction_clip = processed_clip.with_mask(mask_clip)

        # í¬ê¸° ì¡°ì • (1/5)
        new_width = int(reaction_clip.w * scale)
        new_height = int(reaction_clip.h * scale)
        reaction_clip = reaction_clip.resized((new_width, new_height))

        print(f"   [SUCCESS] Chromakey processed (size: {new_width}x{new_height})")

        return reaction_clip

    except Exception as e:
        print(f"[ERROR] Chromakey processing failed: {e}")
        import traceback
        traceback.print_exc()
        return None


def get_windows_font():
    """ì‹œìŠ¤í…œ í°íŠ¸ ê²½ë¡œ ë°˜í™˜ (Windows/Mac ì§€ì›, í•œê¸€ ìš°ì„ )"""
    import platform
    system = platform.system()
    preferred_language = str(get_config_value(["voice_settings", "language"], "en") or "").lower()
    prefer_cjk = any(
        preferred_language.startswith(prefix)
        for prefix in ("ko", "ja", "zh")
    )

    if system == "Windows":
        # Windows ê¸°ë³¸ í°íŠ¸ ìš°ì„ ìˆœìœ„ (ì¸ë„¤ì¼ìš© êµµì€ í°íŠ¸ ìš°ì„ )
        font_paths = [
            r"C:\Windows\Fonts\malgunbd.ttf",     # ë§‘ì€ ê³ ë”• Bold (êµµì€ ê³ ë”•)
            r"C:\Windows\Fonts\gulimb.ttc",       # êµ´ë¦¼ Bold
            r"C:\Windows\Fonts\arialbd.ttf",      # Arial Bold
            r"C:\Windows\Fonts\malgun.ttf",       # ë§‘ì€ ê³ ë”•
            r"C:\Windows\Fonts\malgunsl.ttf",     # ë§‘ì€ ê³ ë”• Semilight
            r"C:\Windows\Fonts\batang.ttc",       # ë°”íƒ•
            r"C:\Windows\Fonts\gulim.ttc",        # êµ´ë¦¼
            r"C:\Windows\Fonts\arial.ttf",
        ]
    elif system == "Darwin":  # macOS
        # macOS í•œê¸€ ìë§‰ ìš°ì„  (Impact ë“±ì€ í•œê¸€ ë¯¸ì§€ì›)
        cjk_priority = [
            "/System/Library/Fonts/Supplemental/AppleSDGothicNeo.ttc",
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",
            "/Library/Fonts/AppleSDGothicNeo.ttc",
            "/System/Library/Fonts/Supplemental/NotoSansKR-Regular.otf",
            "/System/Library/Fonts/Supplemental/Arial Unicode.ttf",
        ]
        bold_priority = [
            "/System/Library/Fonts/Supplemental/Impact.ttf",  # Impact (YouTube ìë§‰ ìŠ¤íƒ ë‹¤ë“œ)
            "/System/Library/Fonts/Supplemental/Arial Black.ttf",  # Arial Black (êµµê³  ê°•ë ¬)
            "/System/Library/Fonts/Supplemental/DIN Condensed Bold.ttf",
            "/System/Library/Fonts/Supplemental/DIN Alternate Bold.ttf",
            "/System/Library/Fonts/Supplemental/Arial Bold.ttf",
            "/System/Library/Fonts/HelveticaNeue.ttc",  # Helvetica Neue Bold (index 5)
            "/System/Library/Fonts/SFNS.ttf",  # San Francisco
            "/System/Library/Fonts/Avenir Next.ttc",  # Avenir Next Heavy (index 8)
            "/System/Library/Fonts/Helvetica.ttc",  # Helvetica Bold
            "/Library/Fonts/Arial.ttf",
        ]
        # ëª¨ë“  ì–¸ì–´ì—ì„œ AppleSDGothicNeo ExtraBold (index 14) ìš°ì„  ì‚¬ìš©
        font_paths = cjk_priority + bold_priority
    else:  # Linux
        cjk_fonts = [
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
            "/usr/share/fonts/truetype/noto/NotoSansKR-Regular.otf",
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        ]
        latin_fonts = [
            "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf",
            "/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf",
        ]
        font_paths = cjk_fonts + latin_fonts if prefer_cjk else latin_fonts + cjk_fonts

    for font_path in font_paths:
        if font_path and os.path.exists(font_path):
            normalized = os.path.normpath(font_path)
            # TTC íŒŒì¼ì— ëŒ€í•´ êµµì€ ì¸ë±ìŠ¤ ë° TextClipìš© ì´ë¦„ì„ ë“±ë¡
            if normalized.lower().endswith(".ttc"):
                if system == "Windows" and "gulim.ttc" in normalized.lower():
                    register_font_override(normalized, index=0, textclip_name="GulimChe")
                elif system == "Darwin":
                    # macOS í°íŠ¸ë³„ Bold ì¸ë±ìŠ¤ ì„¤ì •
                    if "helveticaneue" in normalized.lower():
                        register_font_override(normalized, index=5, textclip_name="HelveticaNeue-Bold")  # HelveticaNeue Bold
                    elif "avenir" in normalized.lower():
                        register_font_override(normalized, index=8, textclip_name="Avenir-Heavy")  # Avenir Heavy
                    elif "applesdgothicneo" in normalized.lower():
                        register_font_override(normalized, index=14, textclip_name="AppleSDGothicNeo-ExtraBold")  # ExtraBold
                    elif "helvetica.ttc" in normalized.lower():
                        register_font_override(normalized, index=1, textclip_name="Helvetica-Bold")  # Helvetica Bold
                    else:
                        register_font_override(normalized, index=0)
                else:
                    register_font_override(normalized, index=0)
            else:
                if system == "Windows":
                    if "malgunsl" in normalized.lower():
                        register_font_override(normalized, index=0, textclip_name="Malgun Gothic Semilight")
                    elif "malgunbd" in normalized.lower():
                        register_font_override(normalized, index=0, textclip_name="Malgun Gothic Bold")
                    elif "malgun" in normalized.lower():
                        register_font_override(normalized, index=0, textclip_name="Malgun Gothic")
                elif system == "Darwin" and "applegothic" in normalized.lower():
                    register_font_override(normalized, index=0, textclip_name="AppleGothic")
            return normalized

    # í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° None ë°˜í™˜ (ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©)
    return None


def sanitize_filename(value, replacement="_"):
    """íŒŒì¼ëª…ì— ì‚¬ìš©í•  ë¬¸ìì—´ì—ì„œ ì œì–´/ê¸ˆì§€ ë¬¸ìë¥¼ ì œê±°í•˜ë©´ì„œ í•œê¸€Â·ê³µë°±ì€ ìœ ì§€."""
    if not isinstance(value, str):
        value = str(value or "")

    normalized = unicodedata.normalize("NFC", value).strip()
    if not normalized:
        return ""

    invalid_chars = set('\\/:*?"<>|')
    sanitized_chars = []

    for char in normalized:
        if char in invalid_chars or ord(char) < 32:
            if replacement:
                sanitized_chars.append(replacement)
            continue
        sanitized_chars.append(char)

    sanitized = "".join(sanitized_chars)

    # ê³µë°±ì€ ìœ ì§€í•˜ë˜ ì—°ì† ê³µë°±ì€ í•˜ë‚˜ë¡œ ì¶•ì†Œ
    sanitized = re.sub(r"\s+", " ", sanitized)

    if replacement:
        sanitized = re.sub(rf"{re.escape(replacement)}+", replacement, sanitized)

    sanitized = sanitized.strip(" ._")
    return sanitized


def generate_output_basename(base_name, output_dir, extension=".mp4"):
    """ì¶œë ¥ íŒŒì¼ ê¸°ë³¸ ì´ë¦„ ìƒì„± (ì¤‘ë³µ ë°©ì§€, ì„¤ì • ê¸°ë°˜ íƒ€ì„ìŠ¤íƒ¬í”„)."""
    sanitized = sanitize_filename(base_name) or "video"
    use_timestamp_prefix = bool(get_config_value(["paths", "use_timestamp_prefix"], False))

    if use_timestamp_prefix:
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        candidate = f"{timestamp} {sanitized}".strip()
    else:
        candidate = sanitized

    candidate = candidate[:120].strip(" ._") or time.strftime("%Y%m%d-%H%M%S")
    final_candidate = candidate
    counter = 1

    while os.path.exists(os.path.join(output_dir, f"{final_candidate}{extension}")):
        final_candidate = f"{candidate}_{counter}"
        counter += 1

    return final_candidate


def wrap_text_preserving_words(text, font_path, font_size, max_width, stroke_width=0):
    """
    ì£¼ì–´ì§„ í”½ì…€ ë„ˆë¹„ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì¤„ë°”ê¿ˆ.
    - ë‹¨ì–´ ì¤‘ê°„ì´ ì˜ë¦¬ì§€ ì•Šë„ë¡ ë³´ì¥
    - ê¸°ì¡´ ì¤„ë°”ê¿ˆ(\n)ì„ ìœ ì§€
    """
    if max_width <= 0:
        return text

    font = load_pil_font(font_path, font_size)

    measuring_image = Image.new("RGB", (1, 1), (0, 0, 0))
    draw_ctx = ImageDraw.Draw(measuring_image)

    wrapped_lines = []
    for paragraph in text.splitlines():
        words = paragraph.split()
        if not words:
            wrapped_lines.append("")
            continue

        current_line = words[0]
        for word in words[1:]:
            candidate = f"{current_line} {word}"
            bbox = draw_ctx.textbbox((0, 0), candidate, font=font, stroke_width=stroke_width)
            line_width = bbox[2] - bbox[0]

            if line_width <= max_width:
                current_line = candidate
            else:
                wrapped_lines.append(current_line)
                current_line = word

        wrapped_lines.append(current_line)

    return "\n".join(wrapped_lines)


def parse_color(value, default=(0, 0, 0)):
    """CSS/hex/tuple ê¸°ë°˜ ìƒ‰ìƒ ë¬¸ìì—´ì„ RGB íŠœí”Œë¡œ ë³€í™˜."""
    if isinstance(value, str):
        value = value.strip()
        if not value:
            return default

        try:
            return tuple(ImageColor.getrgb(value))
        except Exception:
            pass

        if value.startswith("#"):
            value = value.lstrip("#")
            if len(value) == 6:
                try:
                    return tuple(int(value[i:i+2], 16) for i in (0, 2, 4))
                except ValueError:
                    return default
        parts = [p.strip() for p in value.split(",") if p.strip()]
        if len(parts) == 3:
            try:
                return tuple(max(0, min(255, int(component))) for component in parts)
            except ValueError:
                return default
    elif isinstance(value, (list, tuple)) and len(value) >= 3:
        try:
            return tuple(int(max(0, min(255, round(component)))) for component in value[:3])
        except (TypeError, ValueError):
            return default
    return default


def _create_imageclip_with_mask(rgb_array, alpha_array):
    """RGB + alpha ë°°ì—´ë¡œ ImageClipì„ ìƒì„± (ë²„ì „ ê°„ ismask/is_mask í˜¸í™˜)."""
    base_clip = ImageClip(rgb_array)
    alpha_norm = np.array(alpha_array, dtype="float32")
    if alpha_norm.ndim == 3:
        alpha_norm = alpha_norm[..., 0]
    if alpha_norm.max() > 1.0:
        alpha_norm = np.clip(alpha_norm / 255.0, 0.0, 1.0)
    try:
        mask_clip = ImageClip(alpha_norm, ismask=True)
    except TypeError:
        mask_clip = ImageClip(alpha_norm, is_mask=True)
    return base_clip.with_mask(mask_clip)


def convert_textclip_to_slanted_imageclip(text_clip, italic_shear=0.2):
    """TextClipìœ¼ë¡œ ë§Œë“  ìë§‰ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ ê¸°ìš¸ì—¬ì„œ ImageClipìœ¼ë¡œ ë°˜í™˜."""
    if text_clip is None:
        return None

    try:
        base_frame = text_clip.get_frame(0)
    except Exception:
        return text_clip

    if getattr(text_clip, "mask", None) is not None:
        mask_frame = text_clip.mask.get_frame(0)
        if mask_frame.ndim == 3:
            mask_frame = mask_frame[..., 0]
        if mask_frame.max() <= 1.0:
            mask_frame = (mask_frame * 255).astype("uint8")
        else:
            mask_frame = np.clip(mask_frame, 0, 255).astype("uint8")
    else:
        mask_frame = np.full(base_frame.shape[:2], 255, dtype="uint8")

    if base_frame.dtype != np.uint8:
        base_frame = np.clip(base_frame * 255 if base_frame.max() <= 1.0 else base_frame, 0, 255).astype("uint8")

    rgba = np.dstack([base_frame, mask_frame])
    image = Image.fromarray(rgba, mode="RGBA")

    shear = float(italic_shear or 0.0)
    if abs(shear) > 1e-3:
        shift = abs(shear) * image.height
        new_width = int(image.width + shift)
        if shear >= 0:
            transform = (1, shear, 0, 0, 1, 0)
            offset = 0
        else:
            transform = (1, shear, -shear * image.height, 0, 1, 0)
            new_width = int(image.width + abs(shear) * image.height)
        image = image.transform(
            (new_width, image.height),
            Image.AFFINE,
            transform,
            resample=Image.BICUBIC,
            fillcolor=(0, 0, 0, 0)
        )

    slanted = np.array(image).astype("uint8")
    rgb = slanted[..., :3]
    alpha = slanted[..., 3]
    clip = _create_imageclip_with_mask(rgb, alpha)

    if getattr(text_clip, "fps", None):
        clip = clip.with_fps(text_clip.fps)

    try:
        text_clip.close()
    except Exception:
        pass

    return clip


def _ensure_margin_tuple(margin_value):
    """ì •ìˆ˜ ë˜ëŠ” 4íŠœí”Œì„ í•­ìƒ (top, right, bottom, left) í˜•íƒœë¡œ ë³€í™˜."""
    if isinstance(margin_value, (list, tuple)) and len(margin_value) >= 4:
        return tuple(int(margin_value[i]) for i in range(4))
    margin_int = int(max(0, round(margin_value or 0)))
    return (margin_int, margin_int, margin_int, margin_int)


def create_pil_subtitle_clip(
    text,
    font_path,
    font_size,
    text_color,
    stroke_color,
    stroke_width,
    margin,
    line_spacing,
    italic_shear=0.0
):
    """PILì„ ì´ìš©í•´ ë‘êº¼ìš´ ì™¸ê³½ì„  ìë§‰ì„ ìƒì„± (ImageClip ë°˜í™˜)."""
    if not text:
        text = " "

    font = load_pil_font(font_path, font_size)
    margin_top, margin_right, margin_bottom, margin_left = _ensure_margin_tuple(margin)
    rgb_text = parse_color(text_color, (255, 255, 255))
    rgb_stroke = parse_color(stroke_color, (0, 0, 0))
    lines = text.split("\n")
    spacing = max(0, int(round(line_spacing or 0)))

    measuring_image = Image.new("RGBA", (4, 4), (0, 0, 0, 0))
    draw = ImageDraw.Draw(measuring_image)

    line_metrics = []
    max_line_width = 0
    total_text_height = 0

    for line in lines:
        bbox = draw.textbbox((0, 0), line, font=font, stroke_width=stroke_width)
        width = bbox[2] - bbox[0]
        height = bbox[3] - bbox[1]
        max_line_width = max(max_line_width, width)
        line_metrics.append((line, width, height))
        total_text_height += height
    if len(lines) > 1:
        total_text_height += spacing * (len(lines) - 1)

    canvas_width = int(max_line_width + margin_left + margin_right)
    canvas_height = int(total_text_height + margin_top + margin_bottom)
    canvas = Image.new("RGBA", (canvas_width, canvas_height), (0, 0, 0, 0))
    draw = ImageDraw.Draw(canvas)

    y = margin_top
    for line, width, height in line_metrics:
        x = margin_left + (max_line_width - width) / 2
        draw.text(
            (x, y),
            line,
            font=font,
            fill=rgb_text,
            stroke_width=stroke_width,
            stroke_fill=rgb_stroke
        )
        y += height + spacing

    shear = float(italic_shear or 0.0)
    if abs(shear) > 1e-3:
        shift = abs(shear) * canvas_height
        new_width = int(canvas_width + shift)
        if shear >= 0:
            transform = (1, shear, 0, 0, 1, 0)
        else:
            transform = (1, shear, -shear * canvas_height, 0, 1, 0)
            new_width = int(canvas_width + abs(shear) * canvas_height)
        canvas = canvas.transform(
            (new_width, canvas_height),
            Image.AFFINE,
            transform,
            resample=Image.BICUBIC,
            fillcolor=(0, 0, 0, 0)
        )

    rgba_array = np.array(canvas).astype("uint8")
    rgb = rgba_array[..., :3]
    alpha = rgba_array[..., 3]
    return _create_imageclip_with_mask(rgb, alpha)


def apply_cinematic_filter(frame):
    """
    ì‹œë„¤ë§ˆí‹± í•„í„° ì ìš© í•¨ìˆ˜
    - ë¹„ë„¤íŠ¸ íš¨ê³¼ (ê°€ì¥ìë¦¬ ì–´ë‘¡ê²Œ)
    - ì•½ê°„ì˜ ë°ê¸° ê°ì†Œ
    - ì±„ë„ ì¡°ì •
    """
    h, w = frame.shape[:2]

    # 1. ê°•ë ¥í•œ ë¹„ë„¤íŠ¸ íš¨ê³¼ ìƒì„± (ê°€ì¥ìë¦¬ ë§ì´ ì–´ë‘¡ê²Œ)
    # ì¤‘ì•™ì—ì„œ ê°€ì¥ìë¦¬ë¡œ ê°ˆìˆ˜ë¡ ì–´ë‘ì›Œì§€ëŠ” ë§ˆìŠ¤í¬
    Y, X = np.ogrid[:h, :w]
    center_y, center_x = h / 2, w / 2

    # ê±°ë¦¬ ê³„ì‚° (ì •ê·œí™”)
    dist_from_center = np.sqrt((X - center_x)**2 + (Y - center_y)**2)
    max_dist = np.sqrt(center_x**2 + center_y**2)
    normalized_dist = dist_from_center / max_dist

    # ë¹„ë„¤íŠ¸ ê°•ë„ ì¡°ì • (ì œê³±ìœ¼ë¡œ ê°€ì¥ìë¦¬ë¥¼ ë” ì–´ë‘¡ê²Œ)
    vignette = 1 - (normalized_dist ** 1.8) * 0.85  # ê±°ë¦¬ì˜ 1.8ì œê³±ìœ¼ë¡œ ë” ê¸‰ê²©í•˜ê²Œ ì–´ë‘¡ê²Œ
    vignette = np.clip(vignette, 0.15, 1.0)  # ìµœì†Œ 15% ë°ê¸° (ê°€ì¥ìë¦¬ ë§¤ìš° ì–´ë‘ì›€)

    # RGB ì±„ë„ì— ë¹„ë„¤íŠ¸ ì ìš©
    vignette_3d = np.stack([vignette, vignette, vignette], axis=-1)
    frame = (frame * vignette_3d).astype('uint8')

    # 2. ì „ì²´ ë°ê¸° ì‚´ì§ ê°ì†Œ (ì‹œë„¤ë§ˆí‹± ëŠë‚Œ) - ë¹„í™œì„±í™”ë¨
    # frame = (frame * 0.92).astype('uint8')  # 8% ì–´ë‘¡ê²Œ

    # 3. ì±„ë„ ì•½ê°„ ê°ì†Œ (desaturated ëŠë‚Œ)
    # RGBë¥¼ HSVë¡œ ë³€í™˜í•˜ì§€ ì•Šê³  ê°„ë‹¨íˆ ì²˜ë¦¬
    gray = np.mean(frame, axis=-1, keepdims=True)
    frame = (frame * 0.85 + gray * 0.15).astype('uint8')  # ì±„ë„ 15% ê°ì†Œ

    return frame


def apply_sharpen_filter(frame):
    """
    Sharpen Edges í•„í„° ì ìš© (ìº¡ì»· ìŠ¤íƒ€ì¼)
    - 1ì°¨ ìƒ¤í”ˆ: sharpen=15 (radius=2, percent=150%)
    - 2ì°¨ ìƒ¤í”ˆ: sharpen=29 (radius=2, percent=290%)
    """
    from PIL import Image, ImageFilter

    # numpy arrayë¥¼ PIL Imageë¡œ ë³€í™˜
    pil_image = Image.fromarray(frame)

    # 1ì°¨ ìƒ¤í”ˆ ì ìš© (sharpen=15 ê¸°ì¤€)
    pil_image = pil_image.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))

    # 2ì°¨ ìƒ¤í”ˆ ì ìš© (sharpen=29 ê¸°ì¤€, ë” ê°•í•˜ê²Œ)
    pil_image = pil_image.filter(ImageFilter.UnsharpMask(radius=2, percent=290, threshold=2))

    # PIL Imageë¥¼ ë‹¤ì‹œ numpy arrayë¡œ ë³€í™˜
    return np.array(pil_image)


def apply_chromatic_aberration(get_frame, t):
    """
    Chromatic Aberration íš¨ê³¼ ì ìš© (ìº¡ì»· ìŠ¤íƒ€ì¼ - ë™ì )
    - Speed: 33 (ì• ë‹ˆë©”ì´ì…˜ ì†ë„)
    - Strength: 11 (ìƒí•˜ ë¶„ë¦¬ ê°•ë„)
    - Lateral chromatic aberration: 59 (ì¢Œìš° ë¶„ë¦¬ ê°•ë„)
    ì‹œê°„ì— ë”°ë¼ offsetì´ ì‚¬ì¸íŒŒë¡œ ë³€í™”
    """
    import cv2

    frame = get_frame(t)
    h, w = frame.shape[:2]

    # Speed=33ì„ ì‚¬ì¸íŒŒ ì£¼íŒŒìˆ˜ë¡œ ë³€í™˜ (ì•½ 0.33Hz)
    speed = 33 / 100.0

    # ì‹œê°„ì— ë”°ë¼ -0.75 ~ +0.75 ì‚¬ì´ë¡œ ë³€í™” (ì‚¬ì¸íŒŒ)
    lateral_offset = 0.75 * np.sin(2 * np.pi * speed * t)

    # BGR ì±„ë„ ë¶„ë¦¬ (OpenCVëŠ” BGR ìˆœì„œ)
    b, g, r = cv2.split(frame)

    # Red ì±„ë„: ì˜¤ë¥¸ìª½ìœ¼ë¡œ lateral_offsetë§Œí¼ ì´ë™ (ì‹œê°„ì— ë”°ë¼ ë³€í™”)
    M_red = np.float32([[1, 0, lateral_offset], [0, 1, 0]])
    r_shifted = cv2.warpAffine(r, M_red, (w, h), borderMode=cv2.BORDER_REPLICATE)

    # Blue ì±„ë„: ì™¼ìª½ìœ¼ë¡œ lateral_offsetë§Œí¼ ì´ë™ (ì‹œê°„ì— ë”°ë¼ ë³€í™”)
    M_blue = np.float32([[1, 0, -lateral_offset], [0, 1, 0]])
    b_shifted = cv2.warpAffine(b, M_blue, (w, h), borderMode=cv2.BORDER_REPLICATE)

    # ì±„ë„ ë‹¤ì‹œ í•©ì¹˜ê¸° (Greenì€ ê·¸ëŒ€ë¡œ)
    result = cv2.merge([b_shifted, g, r_shifted])

    return result


def apply_wave_effect(get_frame, t):
    """
    ì›¨ì´ë¸Œ íš¨ê³¼ ì ìš© í•¨ìˆ˜ (ë¯¸ì„¸í•œ ì¢Œìš° í”ë“¤ë¦¼ + ë¬¼ê²° ì™œê³¡)
    - ì‹œê°„ì— ë”°ë¼ í™”ë©´ì´ ë¯¸ì„¸í•˜ê²Œ ì¢Œìš°ë¡œ ì›€ì§ì„
    - ìƒí•˜ë¡œ ë¯¸ì„¸í•œ ë¬¼ê²° ì™œê³¡ ì¶”ê°€
    """
    import cv2

    frame = get_frame(t)
    h, w = frame.shape[:2]

    # ì›¨ì´ë¸Œ íŒŒë¼ë¯¸í„° (configì—ì„œ ì½ê¸°)
    amplitude_x = float(get_config_value(["video_settings", "wave_effect", "amplitude_x"], 3))
    amplitude_y = float(get_config_value(["video_settings", "wave_effect", "amplitude_y"], 2))
    frequency = float(get_config_value(["video_settings", "wave_effect", "frequency"], 2.0))
    speed = float(get_config_value(["video_settings", "wave_effect", "speed"], 1.5))

    # ì¢Œìš° í”ë“¤ë¦¼ ê³„ì‚°
    offset_x = int(amplitude_x * np.sin(2 * np.pi * frequency * t))

    # ë¬¼ê²° ì™œê³¡ ë§µ ìƒì„±
    Y, X = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')

    # ìƒí•˜ ë¬¼ê²° íš¨ê³¼ (ë¯¸ì„¸í•œ sin íŒŒë™)
    wave_offset_y = amplitude_y * np.sin(2 * np.pi * (X / w * 3 + t * speed))

    # ìƒˆë¡œìš´ ì¢Œí‘œ ê³„ì‚°
    map_x = (X + offset_x).astype(np.float32)
    map_y = (Y + wave_offset_y).astype(np.float32)

    # ë²”ìœ„ ì œí•œ
    map_x = np.clip(map_x, 0, w - 1)
    map_y = np.clip(map_y, 0, h - 1)

    # ë¦¬ë§µí•‘ ì ìš© (OpenCV ì‚¬ìš©)
    warped_frame = cv2.remap(frame, map_x, map_y, cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)

    return warped_frame


def apply_zoom_pan_effect(clip, zoom_factor=1.1, pan_direction="random"):
    """
    Zoom/Pan íš¨ê³¼ ì ìš© í•¨ìˆ˜ (ê°„ë‹¨í•œ ë²„ì „ - í™•ëŒ€ë§Œ)
    - zoom_factor: í™•ëŒ€ ë¹„ìœ¨ (1.1 = 10% í™•ëŒ€)
    - pan_direction: ì´ë™ ë°©í–¥ (í˜„ì¬ëŠ” ì¤‘ì•™ cropë§Œ ì§€ì›)
    """
    import cv2
    from moviepy.video.fx import Resize, Crop

    # ë¹„ë””ì˜¤ë¥¼ zoom_factorë§Œí¼ í™•ëŒ€
    zoomed = clip.with_effects([Resize(zoom_factor)])

    # ì›ë³¸ í¬ê¸°ë¡œ ì¤‘ì•™ crop
    w, h = clip.size
    zoomed_w, zoomed_h = zoomed.size

    # ì¤‘ì•™ì—ì„œ crop
    x1 = (zoomed_w - w) // 2
    y1 = (zoomed_h - h) // 2

    return zoomed.with_effects([Crop(x1=x1, y1=y1, x2=x1 + w, y2=y1 + h)])


def create_blur_background(text_clip, video_clip, blur_amount=15, opacity=0.7, padding=20):
    """
    í…ìŠ¤íŠ¸ í´ë¦½ ë’¤ì— ë¸”ëŸ¬ ë°°ê²½ ìƒì„± (ê°œì„ ëœ ë²„ì „)

    Args:
        text_clip: í…ìŠ¤íŠ¸ í´ë¦½
        video_clip: ì›ë³¸ ë¹„ë””ì˜¤ í´ë¦½
        blur_amount: ë¸”ëŸ¬ ê°•ë„ (í”½ì…€)
        opacity: ë°°ê²½ ì–´ë‘ì›€ ì •ë„ (0.0 ~ 1.0, ë‚®ì„ìˆ˜ë¡ ë°ìŒ)
        padding: í…ìŠ¤íŠ¸ ì£¼ë³€ ì—¬ë°± (í”½ì…€)

    Returns:
        ë¸”ëŸ¬ ë°°ê²½ í´ë¦½ (ImageClip)
    """
    import cv2

    # í…ìŠ¤íŠ¸ í´ë¦½ì˜ í¬ê¸°ì™€ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°
    txt_w, txt_h = text_clip.size
    txt_pos = text_clip.pos
    video_width = video_clip.w
    video_height = video_clip.h

    # ìœ„ì¹˜ ê³„ì‚°
    if callable(txt_pos):
        pos_x, pos_y = txt_pos(0)
    else:
        pos_x, pos_y = txt_pos

    # 'center' ê°™ì€ ë¬¸ìì—´ ìœ„ì¹˜ ì²˜ë¦¬
    if pos_x == 'center':
        pos_x = (video_width - txt_w) / 2
    if pos_y == 'center':
        pos_y = (video_height - txt_h) / 2

    # ë¸”ëŸ¬ ë°•ìŠ¤ ì˜ì—­ ê³„ì‚°
    blur_x = max(0, int(pos_x - padding))
    blur_y = max(0, int(pos_y - padding))
    blur_w = min(video_width - blur_x, int(txt_w + padding * 2))
    blur_h = min(video_height - blur_y, int(txt_h + padding * 2))

    def make_blur_frame(get_frame, t):
        """ë¸”ëŸ¬ ë°°ê²½ í”„ë ˆì„ ìƒì„± (í…ìŠ¤íŠ¸ ì˜ì—­ë§Œ)"""
        # ì›ë³¸ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
        frame = get_frame(t)

        # ë¸”ëŸ¬ ì˜ì—­ ì¶”ì¶œ
        blur_region = frame[blur_y:blur_y+blur_h, blur_x:blur_x+blur_w].copy()

        # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì ìš©
        if blur_region.size > 0 and len(blur_region.shape) == 3:
            # RGB ì±„ë„ í™•ì¸
            blurred = cv2.GaussianBlur(blur_region, (blur_amount*2+1, blur_amount*2+1), 0)

            # ì–´ë‘¡ê²Œ ì²˜ë¦¬ (opacity ì ìš©) - RGB ê° ì±„ë„ì— ì ìš©
            blurred = (blurred * opacity).astype('uint8')

            # ë¸”ëŸ¬ ì˜ì—­ì˜ í¬ê¸° í™•ì¸
            actual_h, actual_w = blurred.shape[:2]

            # RGBA í”„ë ˆì„ ìƒì„± (ì•ŒíŒŒ ì±„ë„ í¬í•¨)
            result = np.zeros((video_height, video_width, 4), dtype='uint8')

            # ë¸”ëŸ¬ ì˜ì—­ì—ë§Œ ìƒ‰ìƒ + ì™„ì „ ë¶ˆíˆ¬ëª… ì•ŒíŒŒ ì±„ë„
            if len(blurred.shape) == 3 and blurred.shape[2] == 3:
                # RGB ì´ë¯¸ì§€ì¸ ê²½ìš°
                result[blur_y:blur_y+actual_h, blur_x:blur_x+actual_w, :3] = blurred
                result[blur_y:blur_y+actual_h, blur_x:blur_x+actual_w, 3] = 255  # ì™„ì „ ë¶ˆíˆ¬ëª…
            else:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì¸ ê²½ìš° RGBë¡œ ë³€í™˜
                blurred_rgb = np.stack([blurred, blurred, blurred], axis=-1) if len(blurred.shape) == 2 else blurred
                result[blur_y:blur_y+actual_h, blur_x:blur_x+actual_w, :3] = blurred_rgb
                result[blur_y:blur_y+actual_h, blur_x:blur_x+actual_w, 3] = 255

            return result
        else:
            # ë¸”ëŸ¬ ì˜ì—­ì´ ì—†ìœ¼ë©´ ì™„ì „ íˆ¬ëª… ë°˜í™˜
            return np.zeros((video_height, video_width, 4), dtype='uint8')

    return make_blur_frame


def _safe_float(value, default):
    """Safely cast a config value to float with fallback."""
    try:
        return float(value)
    except (TypeError, ValueError):
        return default


def _resolve_clip_position(pos_value):
    """Helper that normalizes clip position definitions to a (x, y) tuple."""
    if callable(pos_value):
        try:
            pos_value = pos_value(0)
        except Exception:
            pos_value = None

    if pos_value is None:
        return ("center", "center")

    if isinstance(pos_value, (tuple, list)):
        if len(pos_value) >= 2:
            return pos_value[0], pos_value[1]
        if len(pos_value) == 1:
            return ("center", pos_value[0])

    if isinstance(pos_value, (int, float)):
        return ("center", pos_value)

    # ë¬¸ìì—´('center') ë“±ì€ x ì¢Œí‘œë¡œ ê°„ì£¼
    return (pos_value, "center")


def apply_thumbnail_exit_animation(
    clip,
    fade_duration,
    base_position=None,
    vertical_offset=80,
    scale_reduction=0.08,
):
    """
    í…ìŠ¤íŠ¸ê°€ ì‚¬ë¼ì§ˆ ë•Œ ì‚´ì§ ìœ„ë¡œ ì´ë™í•˜ë©° ì¶•ì†Œ + í˜ì´ë“œì•„ì›ƒë˜ëŠ” ì• ë‹ˆë©”ì´ì…˜ ì ìš©.
    """
    if clip is None or not fade_duration or fade_duration <= 0:
        return clip

    clip_duration = getattr(clip, "duration", None)
    if not clip_duration or clip_duration <= 0:
        return clip

    exit_duration = min(fade_duration, clip_duration)
    exit_start = clip_duration - exit_duration

    anchor_position = _resolve_clip_position(base_position if base_position else getattr(clip, "pos", None))
    anchor_x, anchor_y = anchor_position

    def eased_progress(t):
        """Cubic ease-out for smoother motion."""
        if exit_duration <= 0:
            return 0.0
        raw = (t - exit_start) / exit_duration
        if raw <= 0:
            return 0.0
        if raw >= 1:
            raw = 1.0
        return 1 - pow(1 - raw, 3)

    animated_clip = clip

    if scale_reduction and scale_reduction > 0:
        def scale_func(t):
            progress = eased_progress(t)
            return max(0.3, 1 - scale_reduction * progress)

        animated_clip = animated_clip.with_effects([Resize(scale_func)])

    def animated_position(t):
        progress = eased_progress(t)
        new_y = anchor_y
        if isinstance(anchor_y, (int, float)) and vertical_offset:
            new_y = anchor_y - vertical_offset * progress
        return (anchor_x, new_y)

    animated_clip = animated_clip.with_position(animated_position)
    animated_clip = animated_clip.with_effects([FadeOut(exit_duration)])
    return animated_clip


def sanitize_filename(value, replacement="_"):
    """íŒŒì¼ëª…ì— ì‚¬ìš©í•  ë¬¸ìì—´ì—ì„œ ì œì–´/ê¸ˆì§€ ë¬¸ìë¥¼ ì œê±°í•˜ë©´ì„œ í•œê¸€Â·ê³µë°±ì€ ìœ ì§€."""
    if not isinstance(value, str):
        value = str(value or "")

    normalized = unicodedata.normalize("NFC", value).strip()
    if not normalized:
        return ""

    invalid_chars = set('\\/:*?"<>|')
    sanitized_chars = []

    for char in normalized:
        if char in invalid_chars or ord(char) < 32:
            if replacement:
                sanitized_chars.append(replacement)
            continue
        sanitized_chars.append(char)

    sanitized = "".join(sanitized_chars)

    # ê³µë°±ì€ ìœ ì§€í•˜ë˜ ì—°ì† ê³µë°±ì€ í•˜ë‚˜ë¡œ ì¶•ì†Œ
    sanitized = re.sub(r"\s+", " ", sanitized)

    if replacement:
        sanitized = re.sub(rf"{re.escape(replacement)}+", replacement, sanitized)

    sanitized = sanitized.strip(" ._")
    return sanitized


def generate_output_basename(base_name, output_dir, extension=".mp4"):
    """ì¶œë ¥ íŒŒì¼ ê¸°ë³¸ ì´ë¦„ ìƒì„± (ì¤‘ë³µ ë°©ì§€, ì„¤ì • ê¸°ë°˜ íƒ€ì„ìŠ¤íƒ¬í”„)."""
    sanitized = sanitize_filename(base_name) or "video"
    use_timestamp_prefix = bool(get_config_value(["paths", "use_timestamp_prefix"], False))

    if use_timestamp_prefix:
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        candidate = f"{timestamp} {sanitized}".strip()
    else:
        candidate = sanitized

    candidate = candidate[:120].strip(" ._") or time.strftime("%Y%m%d-%H%M%S")
    final_candidate = candidate
    counter = 1

    while os.path.exists(os.path.join(output_dir, f"{final_candidate}{extension}")):
        final_candidate = f"{candidate}_{counter}"
        counter += 1

    return final_candidate


def wrap_text_preserving_words(text, font_path, font_size, max_width, stroke_width=0):
    """
    ì£¼ì–´ì§„ í”½ì…€ ë„ˆë¹„ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì¤„ë°”ê¿ˆ.
    - ë‹¨ì–´ ì¤‘ê°„ì´ ì˜ë¦¬ì§€ ì•Šë„ë¡ ë³´ì¥
    - ê¸°ì¡´ ì¤„ë°”ê¿ˆ(\n)ì„ ìœ ì§€
    """
    if max_width <= 0:
        return text

    font = load_pil_font(font_path, font_size)

    measuring_image = Image.new("RGB", (1, 1), (0, 0, 0))
    draw_ctx = ImageDraw.Draw(measuring_image)

    wrapped_lines = []
    for paragraph in text.splitlines():
        words = paragraph.split()
        if not words:
            wrapped_lines.append("")
            continue

        current_line = words[0]
        for word in words[1:]:
            candidate = f"{current_line} {word}"
            bbox = draw_ctx.textbbox((0, 0), candidate, font=font, stroke_width=stroke_width)
            line_width = bbox[2] - bbox[0]

            if line_width <= max_width:
                current_line = candidate
            else:
                wrapped_lines.append(current_line)
                current_line = word

        wrapped_lines.append(current_line)

    return "\n".join(wrapped_lines)
def apply_cinematic_filter(frame):
    """
    ì‹œë„¤ë§ˆí‹± í•„í„° ì ìš© í•¨ìˆ˜
    - ë¹„ë„¤íŠ¸ íš¨ê³¼ (ê°€ì¥ìë¦¬ ì–´ë‘¡ê²Œ)
    - ì•½ê°„ì˜ ë°ê¸° ê°ì†Œ
    - ì±„ë„ ì¡°ì •
    """
    h, w = frame.shape[:2]

    # 1. ê°•ë ¥í•œ ë¹„ë„¤íŠ¸ íš¨ê³¼ ìƒì„± (ê°€ì¥ìë¦¬ ë§ì´ ì–´ë‘¡ê²Œ)
    # ì¤‘ì•™ì—ì„œ ê°€ì¥ìë¦¬ë¡œ ê°ˆìˆ˜ë¡ ì–´ë‘ì›Œì§€ëŠ” ë§ˆìŠ¤í¬
    Y, X = np.ogrid[:h, :w]
    center_y, center_x = h / 2, w / 2

    # ê±°ë¦¬ ê³„ì‚° (ì •ê·œí™”)
    dist_from_center = np.sqrt((X - center_x)**2 + (Y - center_y)**2)
    max_dist = np.sqrt(center_x**2 + center_y**2)
    normalized_dist = dist_from_center / max_dist

    # ë¹„ë„¤íŠ¸ ê°•ë„ ì¡°ì • (ì œê³±ìœ¼ë¡œ ê°€ì¥ìë¦¬ë¥¼ ë” ì–´ë‘¡ê²Œ)
    vignette = 1 - (normalized_dist ** 1.8) * 0.85  # ê±°ë¦¬ì˜ 1.8ì œê³±ìœ¼ë¡œ ë” ê¸‰ê²©í•˜ê²Œ ì–´ë‘¡ê²Œ
    vignette = np.clip(vignette, 0.15, 1.0)  # ìµœì†Œ 15% ë°ê¸° (ê°€ì¥ìë¦¬ ë§¤ìš° ì–´ë‘ì›€)

    # RGB ì±„ë„ì— ë¹„ë„¤íŠ¸ ì ìš©
    vignette_3d = np.stack([vignette, vignette, vignette], axis=-1)
    frame = (frame * vignette_3d).astype('uint8')

    # 2. ì „ì²´ ë°ê¸° ì‚´ì§ ê°ì†Œ (ì‹œë„¤ë§ˆí‹± ëŠë‚Œ) - ë¹„í™œì„±í™”ë¨
    # frame = (frame * 0.92).astype('uint8')  # 8% ì–´ë‘¡ê²Œ

    # 3. ì±„ë„ ì•½ê°„ ê°ì†Œ (desaturated ëŠë‚Œ)
    # RGBë¥¼ HSVë¡œ ë³€í™˜í•˜ì§€ ì•Šê³  ê°„ë‹¨íˆ ì²˜ë¦¬
    gray = np.mean(frame, axis=-1, keepdims=True)
    frame = (frame * 0.85 + gray * 0.15).astype('uint8')  # ì±„ë„ 15% ê°ì†Œ

    return frame


def apply_sharpen_filter(frame):
    """
    Sharpen Edges í•„í„° ì ìš© (ìº¡ì»· ìŠ¤íƒ€ì¼)
    - 1ì°¨ ìƒ¤í”ˆ: sharpen=15 (radius=2, percent=150%)
    - 2ì°¨ ìƒ¤í”ˆ: sharpen=29 (radius=2, percent=290%)
    """
    from PIL import Image, ImageFilter

    # numpy arrayë¥¼ PIL Imageë¡œ ë³€í™˜
    pil_image = Image.fromarray(frame)

    # 1ì°¨ ìƒ¤í”ˆ ì ìš© (sharpen=15 ê¸°ì¤€)
    pil_image = pil_image.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))

    # 2ì°¨ ìƒ¤í”ˆ ì ìš© (sharpen=29 ê¸°ì¤€, ë” ê°•í•˜ê²Œ)
    pil_image = pil_image.filter(ImageFilter.UnsharpMask(radius=2, percent=290, threshold=2))

    # PIL Imageë¥¼ ë‹¤ì‹œ numpy arrayë¡œ ë³€í™˜
    return np.array(pil_image)


def apply_chromatic_aberration(get_frame, t):
    """
    Chromatic Aberration íš¨ê³¼ ì ìš© (ìº¡ì»· ìŠ¤íƒ€ì¼ - ë™ì )
    - Speed: 33 (ì• ë‹ˆë©”ì´ì…˜ ì†ë„)
    - Strength: 11 (ìƒí•˜ ë¶„ë¦¬ ê°•ë„)
    - Lateral chromatic aberration: 59 (ì¢Œìš° ë¶„ë¦¬ ê°•ë„)
    ì‹œê°„ì— ë”°ë¼ offsetì´ ì‚¬ì¸íŒŒë¡œ ë³€í™”
    """
    import cv2

    frame = get_frame(t)
    h, w = frame.shape[:2]

    # Speed=33ì„ ì‚¬ì¸íŒŒ ì£¼íŒŒìˆ˜ë¡œ ë³€í™˜ (ì•½ 0.33Hz)
    speed = 33 / 100.0

    # ì‹œê°„ì— ë”°ë¼ -0.75 ~ +0.75 ì‚¬ì´ë¡œ ë³€í™” (ì‚¬ì¸íŒŒ)
    lateral_offset = 0.75 * np.sin(2 * np.pi * speed * t)

    # BGR ì±„ë„ ë¶„ë¦¬ (OpenCVëŠ” BGR ìˆœì„œ)
    b, g, r = cv2.split(frame)

    # Red ì±„ë„: ì˜¤ë¥¸ìª½ìœ¼ë¡œ lateral_offsetë§Œí¼ ì´ë™ (ì‹œê°„ì— ë”°ë¼ ë³€í™”)
    M_red = np.float32([[1, 0, lateral_offset], [0, 1, 0]])
    r_shifted = cv2.warpAffine(r, M_red, (w, h), borderMode=cv2.BORDER_REPLICATE)

    # Blue ì±„ë„: ì™¼ìª½ìœ¼ë¡œ lateral_offsetë§Œí¼ ì´ë™ (ì‹œê°„ì— ë”°ë¼ ë³€í™”)
    M_blue = np.float32([[1, 0, -lateral_offset], [0, 1, 0]])
    b_shifted = cv2.warpAffine(b, M_blue, (w, h), borderMode=cv2.BORDER_REPLICATE)

    # ì±„ë„ ë‹¤ì‹œ í•©ì¹˜ê¸° (Greenì€ ê·¸ëŒ€ë¡œ)
    result = cv2.merge([b_shifted, g, r_shifted])

    return result


def apply_wave_effect(get_frame, t):
    """
    ì›¨ì´ë¸Œ íš¨ê³¼ ì ìš© í•¨ìˆ˜ (ë¯¸ì„¸í•œ ì¢Œìš° í”ë“¤ë¦¼ + ë¬¼ê²° ì™œê³¡)
    - ì‹œê°„ì— ë”°ë¼ í™”ë©´ì´ ë¯¸ì„¸í•˜ê²Œ ì¢Œìš°ë¡œ ì›€ì§ì„
    - ìƒí•˜ë¡œ ë¯¸ì„¸í•œ ë¬¼ê²° ì™œê³¡ ì¶”ê°€
    """
    import cv2

    frame = get_frame(t)
    h, w = frame.shape[:2]

    # ì›¨ì´ë¸Œ íŒŒë¼ë¯¸í„° (configì—ì„œ ì½ê¸°)
    amplitude_x = float(get_config_value(["video_settings", "wave_effect", "amplitude_x"], 3))
    amplitude_y = float(get_config_value(["video_settings", "wave_effect", "amplitude_y"], 2))
    frequency = float(get_config_value(["video_settings", "wave_effect", "frequency"], 2.0))
    speed = float(get_config_value(["video_settings", "wave_effect", "speed"], 1.5))

    # ì¢Œìš° í”ë“¤ë¦¼ ê³„ì‚°
    offset_x = int(amplitude_x * np.sin(2 * np.pi * frequency * t))

    # ë¬¼ê²° ì™œê³¡ ë§µ ìƒì„±
    Y, X = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')

    # ìƒí•˜ ë¬¼ê²° íš¨ê³¼ (ë¯¸ì„¸í•œ sin íŒŒë™)
    wave_offset_y = amplitude_y * np.sin(2 * np.pi * (X / w * 3 + t * speed))

    # ìƒˆë¡œìš´ ì¢Œí‘œ ê³„ì‚°
    map_x = (X + offset_x).astype(np.float32)
    map_y = (Y + wave_offset_y).astype(np.float32)

    # ë²”ìœ„ ì œí•œ
    map_x = np.clip(map_x, 0, w - 1)
    map_y = np.clip(map_y, 0, h - 1)

    # ë¦¬ë§µí•‘ ì ìš© (OpenCV ì‚¬ìš©)
    warped_frame = cv2.remap(frame, map_x, map_y, cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)

    return warped_frame


def apply_zoom_pan_effect(clip, zoom_factor=1.1, pan_direction="random"):
    """
    Zoom/Pan íš¨ê³¼ ì ìš© í•¨ìˆ˜ (ê°„ë‹¨í•œ ë²„ì „ - í™•ëŒ€ë§Œ)
    - zoom_factor: í™•ëŒ€ ë¹„ìœ¨ (1.1 = 10% í™•ëŒ€)
    - pan_direction: ì´ë™ ë°©í–¥ (í˜„ì¬ëŠ” ì¤‘ì•™ cropë§Œ ì§€ì›)
    """
    import cv2
    from moviepy.video.fx import Resize, Crop

    # ë¹„ë””ì˜¤ë¥¼ zoom_factorë§Œí¼ í™•ëŒ€
    zoomed = clip.with_effects([Resize(zoom_factor)])

    # ì›ë³¸ í¬ê¸°ë¡œ ì¤‘ì•™ crop
    w, h = clip.size
    zoomed_w, zoomed_h = zoomed.size

    # ì¤‘ì•™ì—ì„œ crop
    x1 = (zoomed_w - w) // 2
    y1 = (zoomed_h - h) // 2

    return zoomed.with_effects([Crop(x1=x1, y1=y1, x2=x1 + w, y2=y1 + h)])


def create_blur_background(text_clip, video_clip, blur_amount=15, opacity=0.7, padding=20):
    """
    í…ìŠ¤íŠ¸ í´ë¦½ ë’¤ì— ë¸”ëŸ¬ ë°°ê²½ ìƒì„± (ê°œì„ ëœ ë²„ì „)

    Args:
        text_clip: í…ìŠ¤íŠ¸ í´ë¦½
        video_clip: ì›ë³¸ ë¹„ë””ì˜¤ í´ë¦½
        blur_amount: ë¸”ëŸ¬ ê°•ë„ (í”½ì…€)
        opacity: ë°°ê²½ ì–´ë‘ì›€ ì •ë„ (0.0 ~ 1.0, ë‚®ì„ìˆ˜ë¡ ë°ìŒ)
        padding: í…ìŠ¤íŠ¸ ì£¼ë³€ ì—¬ë°± (í”½ì…€)

    Returns:
        ë¸”ëŸ¬ ë°°ê²½ í´ë¦½ (ImageClip)
    """
    import cv2

    # í…ìŠ¤íŠ¸ í´ë¦½ì˜ í¬ê¸°ì™€ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°
    txt_w, txt_h = text_clip.size
    txt_pos = text_clip.pos
    video_width = video_clip.w
    video_height = video_clip.h

    # ìœ„ì¹˜ ê³„ì‚°
    if callable(txt_pos):
        pos_x, pos_y = txt_pos(0)
    else:
        pos_x, pos_y = txt_pos

    # 'center' ê°™ì€ ë¬¸ìì—´ ìœ„ì¹˜ ì²˜ë¦¬
    if pos_x == 'center':
        pos_x = (video_width - txt_w) / 2
    if pos_y == 'center':
        pos_y = (video_height - txt_h) / 2

    # ë¸”ëŸ¬ ë°•ìŠ¤ ì˜ì—­ ê³„ì‚°
    blur_x = max(0, int(pos_x - padding))
    blur_y = max(0, int(pos_y - padding))
    blur_w = min(video_width - blur_x, int(txt_w + padding * 2))
    blur_h = min(video_height - blur_y, int(txt_h + padding * 2))

    def make_blur_frame(get_frame, t):
        """ë¸”ëŸ¬ ë°°ê²½ í”„ë ˆì„ ìƒì„± (í…ìŠ¤íŠ¸ ì˜ì—­ë§Œ)"""
        # ì›ë³¸ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
        frame = get_frame(t)

        # ë¸”ëŸ¬ ì˜ì—­ ì¶”ì¶œ
        blur_region = frame[blur_y:blur_y+blur_h, blur_x:blur_x+blur_w].copy()

        # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì ìš©
        if blur_region.size > 0 and len(blur_region.shape) == 3:
            # RGB ì±„ë„ í™•ì¸
            blurred = cv2.GaussianBlur(blur_region, (blur_amount*2+1, blur_amount*2+1), 0)

            # ì–´ë‘¡ê²Œ ì²˜ë¦¬ (opacity ì ìš©) - RGB ê° ì±„ë„ì— ì ìš©
            blurred = (blurred * opacity).astype('uint8')

            # ë¸”ëŸ¬ ì˜ì—­ì˜ í¬ê¸° í™•ì¸
            actual_h, actual_w = blurred.shape[:2]

            # RGBA í”„ë ˆì„ ìƒì„± (ì•ŒíŒŒ ì±„ë„ í¬í•¨)
            result = np.zeros((video_height, video_width, 4), dtype='uint8')

            # ë¸”ëŸ¬ ì˜ì—­ì—ë§Œ ìƒ‰ìƒ + ì™„ì „ ë¶ˆíˆ¬ëª… ì•ŒíŒŒ ì±„ë„
            if len(blurred.shape) == 3 and blurred.shape[2] == 3:
                # RGB ì´ë¯¸ì§€ì¸ ê²½ìš°
                result[blur_y:blur_y+actual_h, blur_x:blur_x+actual_w, :3] = blurred
                result[blur_y:blur_y+actual_h, blur_x:blur_x+actual_w, 3] = 255  # ì™„ì „ ë¶ˆíˆ¬ëª…
            else:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì¸ ê²½ìš° RGBë¡œ ë³€í™˜
                blurred_rgb = np.stack([blurred, blurred, blurred], axis=-1) if len(blurred.shape) == 2 else blurred
                result[blur_y:blur_y+actual_h, blur_x:blur_x+actual_w, :3] = blurred_rgb
                result[blur_y:blur_y+actual_h, blur_x:blur_x+actual_w, 3] = 255

            return result
        else:
            # ë¸”ëŸ¬ ì˜ì—­ì´ ì—†ìœ¼ë©´ ì™„ì „ íˆ¬ëª… ë°˜í™˜
            return np.zeros((video_height, video_width, 4), dtype='uint8')

    return make_blur_frame


def build_keyword_highlight_clip(
    txt_clip,
    keyword,
    font_path,
    font_size,
    stroke_width,
    stroke_color,
    text_color,
    margins,
    horizontal_align,
    vertical_align,
    interline,
    max_text_width,
    text_align,
    keyword_color='yellow'
):
    """í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì™„ì „íˆ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ í´ë¦½ì„ ìƒì„± (í‚¤ì›Œë“œë§Œ í•˜ì´ë¼ì´íŠ¸ ìƒ‰ìƒ)"""
    if not keyword:
        print("[WARNING] í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    keyword = keyword.strip()
    # í‚¤ì›Œë“œì—ì„œ ë”°ì˜´í‘œ ì œê±° (AIê°€ ì¶”ê°€í•œ ê²½ìš°)
    if (keyword.startswith('"') and keyword.endswith('"')) or \
       (keyword.startswith("'") and keyword.endswith("'")):
        keyword = keyword[1:-1].strip()

    if not keyword:
        print("[WARNING] í‚¤ì›Œë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return None

    full_text = txt_clip.text or ""

    # ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€ê²½í•´ì„œ ê²€ìƒ‰ (TextClipì´ ìë™ìœ¼ë¡œ ì¤„ë°”ê¿ˆí•˜ê¸° ë•Œë¬¸)
    full_text_normalized = " ".join(full_text.split())
    keyword_lower = keyword.lower()
    text_lower = full_text_normalized.lower()

    print(f"[SEARCH] í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘: '{keyword}'")
    print(f"   ì¸ë„¤ì¼ ì œëª©: '{full_text_normalized}'")

    # í‚¤ì›Œë“œ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
    start_idx = text_lower.find(keyword_lower)
    if start_idx == -1:
        print(f"[ERROR] í‚¤ì›Œë“œ '{keyword}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print(f"   ì›ë³¸ í…ìŠ¤íŠ¸: '{full_text}'")
        return None

    print(f"[OK] í‚¤ì›Œë“œ ì°¾ìŒ: ìœ„ì¹˜ {start_idx}")

    end_idx = start_idx + len(keyword)

    # normalized í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ í‚¤ì›Œë“œ ì¶”ì¶œ (ëŒ€ì†Œë¬¸ì ë³´ì¡´)
    actual_keyword = full_text_normalized[start_idx:end_idx]
    before_text = full_text_normalized[:start_idx]
    after_text = full_text_normalized[end_idx:]

    # PILë¡œ ì§ì ‘ ê·¸ë¦¬ê¸°
    pil_font = load_pil_font(font_path, font_size)

    # ì¤„ë°”ê¿ˆ ì²˜ë¦¬
    img_width, img_height = txt_clip.size
    left_margin, top_margin, right_margin, bottom_margin = margins
    effective_stroke_width = stroke_width if stroke_color else 0

    # í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ (normalized ë²„ì „ ì‚¬ìš©)
    draw_helper = ImageDraw.Draw(Image.new("RGB", (1, 1)))
    wrapped_lines = []
    current_line = ""
    words = full_text_normalized.split()

    for word in words:
        test_line = f"{current_line} {word}".strip()
        bbox = draw_helper.textbbox((0, 0), test_line, font=pil_font, stroke_width=effective_stroke_width)
        if bbox[2] <= max_text_width:
            current_line = test_line
        else:
            if current_line:
                wrapped_lines.append(current_line)
            current_line = word
    if current_line:
        wrapped_lines.append(current_line)

    if not wrapped_lines:
        return None

    # ê° ì¤„ì—ì„œ í‚¤ì›Œë“œ ìœ„ì¹˜ ì°¾ê¸°
    combined_text = " ".join(wrapped_lines)
    keyword_start = combined_text.lower().find(keyword_lower)
    if keyword_start == -1:
        return None

    # ì´ë¯¸ì§€ ìƒì„±
    highlight_image = Image.new("RGBA", (img_width, img_height), (0, 0, 0, 0))
    highlight_draw = ImageDraw.Draw(highlight_image)
    # í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸ ìƒ‰ìƒ (íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ë¨)
    highlight_color = keyword_color

    # ì¤„ ë†’ì´ ê³„ì‚°
    try:
        ascent, descent = pil_font.getmetrics()
    except AttributeError:
        ascent, descent = font_size, 0

    line_height = ascent + descent + interline
    total_text_height = len(wrapped_lines) * (ascent + descent) + (len(wrapped_lines) - 1) * interline

    # ì‹œì‘ Y ìœ„ì¹˜
    y = top_margin
    if vertical_align == "center":
        y = (img_height - top_margin - bottom_margin - total_text_height) / 2 + top_margin
    elif vertical_align == "bottom":
        y = img_height - bottom_margin - total_text_height

    # ê° ì¤„ ê·¸ë¦¬ê¸°
    char_position = 0
    for line_text in wrapped_lines:
        line_bbox = draw_helper.textbbox((0, 0), line_text, font=pil_font, stroke_width=effective_stroke_width)
        line_width = line_bbox[2]

        # X ìœ„ì¹˜
        x = left_margin
        if horizontal_align == "center":
            x = (img_width - line_width) / 2
        elif horizontal_align == "right":
            x = img_width - right_margin - line_width

        # ì´ ì¤„ì—ì„œ í‚¤ì›Œë“œ ìœ„ì¹˜ ì°¾ê¸°
        line_start_in_combined = char_position
        line_end_in_combined = char_position + len(line_text)
        keyword_end = keyword_start + len(actual_keyword)

        # í‚¤ì›Œë“œê°€ ì´ ì¤„ì— ìˆëŠ”ì§€ í™•ì¸
        if keyword_start < line_end_in_combined and keyword_end > line_start_in_combined:
            # í‚¤ì›Œë“œê°€ ì´ ì¤„ì— í¬í•¨ë¨
            local_keyword_start = max(0, keyword_start - line_start_in_combined)
            local_keyword_end = min(len(line_text), keyword_end - line_start_in_combined)

            before = line_text[:local_keyword_start]
            keyword_part = line_text[local_keyword_start:local_keyword_end]
            after = line_text[local_keyword_end:]

            current_x = x

            # ì•ë¶€ë¶„ (í•˜ì–€ìƒ‰)
            if before:
                highlight_draw.text(
                    (current_x, y + ascent),
                    before,
                    font=pil_font,
                    fill=text_color,
                    stroke_width=effective_stroke_width,
                    stroke_fill=stroke_color,
                    anchor="ls"
                )
                before_bbox = draw_helper.textbbox((0, 0), before, font=pil_font, stroke_width=effective_stroke_width)
                current_x += before_bbox[2]

            # í‚¤ì›Œë“œ (ë…¸ë€ìƒ‰)
            if keyword_part:
                highlight_draw.text(
                    (current_x, y + ascent),
                    keyword_part,
                    font=pil_font,
                    fill=highlight_color,
                    stroke_width=effective_stroke_width,
                    stroke_fill=stroke_color,
                    anchor="ls"
                )
                keyword_bbox = draw_helper.textbbox((0, 0), keyword_part, font=pil_font, stroke_width=effective_stroke_width)
                current_x += keyword_bbox[2]

            # ë’·ë¶€ë¶„ (í•˜ì–€ìƒ‰)
            if after:
                highlight_draw.text(
                    (current_x, y + ascent),
                    after,
                    font=pil_font,
                    fill=text_color,
                    stroke_width=effective_stroke_width,
                    stroke_fill=stroke_color,
                    anchor="ls"
                )
        else:
            # í‚¤ì›Œë“œê°€ ì—†ëŠ” ì¤„ (ì „ì²´ í•˜ì–€ìƒ‰)
            highlight_draw.text(
                (x, y + ascent),
                line_text,
                font=pil_font,
                fill=text_color,
                stroke_width=effective_stroke_width,
                stroke_fill=stroke_color,
                anchor="ls"
            )

        y += line_height
        char_position = line_end_in_combined + 1  # +1 for space

    highlight_np = np.array(highlight_image)
    if highlight_np.shape[2] < 4:
        return None

    alpha = highlight_np[:, :, 3].astype(np.float32) / 255.0
    if not np.any(alpha > 0):
        return None

    rgb = highlight_np[:, :, :3]
    mask = ImageClip(alpha, is_mask=True).with_duration(txt_clip.duration)
    return ImageClip(rgb).with_duration(txt_clip.duration).with_mask(mask)



def overlay_voice_on_video(video_path, segments, output_path, metadata=None, folder_name=None, add_subtitles=True, subtitle_color=None, title_color=None, keyword_color=None):
    """
    ë¹„ë””ì˜¤ì— íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ AI ìŒì„± ë‚˜ë ˆì´ì…˜ ë° ìë§‰ ì˜¤ë²„ë ˆì´

    Args:
        video_path (str): ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
        segments (list): íŒŒì‹±ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        output_path (str): ì¶œë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
        metadata (dict): {'key_moment': ì´ˆ, 'thumbnail_title': 'ì œëª©', 'background_music': 'ì˜µì…˜'}
        folder_name (str): ZIP ë‚´ë¶€ í´ë” ì´ë¦„ (ì„¸ë¡œ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ)
        add_subtitles (bool): ìë§‰ ì¶”ê°€ ì—¬ë¶€
        subtitle_color (str): ìë§‰ ìƒ‰ìƒ (Noneì´ë©´ config ì‚¬ìš©)
        title_color (str): ì¸ë„¤ì¼ íƒ€ì´í‹€ ìƒ‰ìƒ (Noneì´ë©´ config ì‚¬ìš©)
        keyword_color (str): í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸ ìƒ‰ìƒ (Noneì´ë©´ config ì‚¬ìš©)
    """
    if metadata is None:
        metadata = {}
    print(f"\n[VIDEO] ë¹„ë””ì˜¤ ë¡œë”©: {video_path}")
    video = VideoFileClip(video_path)

    # _muted ì ‘ë¯¸ì‚¬ í™•ì¸ ë° ì˜¤ë””ì˜¤ ì œê±°
    if '_muted' in os.path.basename(video_path):
        print(f"\n[MUTE] ìŒì†Œê±° í‘œì‹œ ê°ì§€: ì›ë³¸ ì˜¤ë””ì˜¤ë¥¼ ì œê±°í•©ë‹ˆë‹¤")
        video = video.without_audio()
        print(f"[MUTE] ì˜¤ë””ì˜¤ ì œê±° ì™„ë£Œ")

    # ì•ë¶€ë¶„ ì œê±° (AIê°€ íŒì •í•œ ë¶ˆí•„ìš”í•œ ì¸íŠ¸ë¡œ)
    trim_start = metadata.get('trim_start', 0.0)
    if trim_start > 0:
        print(f"\n[TRIM] AIê°€ ì‹ë³„í•œ ë¶ˆí•„ìš”í•œ ì•ë¶€ë¶„ ì œê±° ì¤‘... ({trim_start:.2f}ì´ˆ)")

        original_duration = video.duration

        # ë¹„ë””ì˜¤ê°€ 12ì´ˆ ì´í•˜ë©´ trim ê±´ë„ˆëœ€
        if original_duration <= 12.0:
            print(f"[TRIM] ë¹„ë””ì˜¤ê°€ 12ì´ˆ ì´í•˜ ({original_duration:.2f}s)ì´ë¯€ë¡œ trim ê±´ë„ˆëœ€")
            trim_start = 0
        # trim_startê°€ ë¹„ë””ì˜¤ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ í™•ì¸
        elif trim_start >= video.duration:
            print(f"[WARNING] Trim start ({trim_start:.2f}s)ê°€ ë¹„ë””ì˜¤ ê¸¸ì´ ({video.duration:.2f}s)ë³´ë‹¤ í½ë‹ˆë‹¤. Trim ê±´ë„ˆëœ€.")
            trim_start = 0
        else:
            # ë¹„ë””ì˜¤ ì•ë¶€ë¶„ ìë¥´ê¸°
            video = video.subclipped(trim_start, video.duration)
            new_duration = video.duration
            print(f"[TRIM] ì™„ë£Œ: {trim_start:.2f}ì´ˆ ì œê±°ë¨ ({original_duration:.2f}s â†’ {new_duration:.2f}s)")

            # ì„¸ê·¸ë¨¼íŠ¸ì™€ ë©”íƒ€ë°ì´í„°ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ì¡°ì • (trim_startë§Œí¼ ë¹¼ê¸°)
            for segment in segments:
                segment['start'] = max(0, segment['start'] - trim_start)
                segment['end'] = max(0, segment['end'] - trim_start)

            # metadataì˜ key_moment ì¡°ì •
            if metadata.get('key_moment') is not None:
                metadata['key_moment'] = max(0, metadata['key_moment'] - trim_start)

            print(f"[TRIM] ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„ ì¡°ì • ì™„ë£Œ")

    # ì†ë„ ë³€ê²½ ì ìš© (ì €ì‘ê¶Œ íšŒí”¼ìš©)
    speed_factor = float(get_config_value(["video_settings", "speed_factor"], 1.0))
    if speed_factor != 1.0:
        print(f"\n[SPEED] ë¹„ë””ì˜¤ ì†ë„ ë³€ê²½: {speed_factor}x")
        from moviepy.video.fx import MultiplySpeed
        video = video.with_effects([MultiplySpeed(speed_factor)])

    # _no_filters ì ‘ë¯¸ì‚¬ í™•ì¸ ë° í•„í„° ê±´ë„ˆë›°ê¸°
    apply_filters = '_no_filters' not in os.path.basename(video_path)

    if apply_filters:
        # ì‹œë„¤ë§ˆí‹± í•„í„° ì ìš© (ë¹„ë„¤íŠ¸ 1.8ì œê³± Ã— 0.85, ì±„ë„ 15% ê°ì†Œ)
        print(f"\n[FILTER] Cinematic í•„í„° ì ìš© ì¤‘ (ë¹„ë„¤íŠ¸ 1.8^2 Ã— 0.85, ì±„ë„ -15%)...")
        video = video.image_transform(apply_cinematic_filter)

        # Sharpen Edges í•„í„° ì ìš© (1ì°¨: 150% threshold=3, 2ì°¨: 290% threshold=2)
        print(f"\n[SHARPEN] Sharpen í•„í„° ì ìš© ì¤‘ (1ì°¨: 150%/t3, 2ì°¨: 290%/t2)...")
        video = video.image_transform(apply_sharpen_filter)

        # Chromatic Aberration íš¨ê³¼ ì ìš© (Speed: 0.33Hz, Offset: Â±0.75px ì‚¬ì¸íŒŒ)
        print(f"\n[CHROMATIC] Chromatic Aberration ì ìš© ì¤‘ (Speed: 0.33Hz, Offset: Â±0.75px)...")
        video = video.transform(apply_chromatic_aberration)
    else:
        print(f"\n[FILTER] í•„í„° ë¹„í™œì„±í™” í‘œì‹œ ê°ì§€: ì‹œë„¤ë§ˆí‹±/ìƒ¤í”ˆ/ìƒ‰ìˆ˜ì°¨ í•„í„°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
    original_audio = video.audio
    has_original_audio = original_audio is not None
    if not has_original_audio:
        print("\n[AUDIO] ì›ë³¸ ì˜¤ë””ì˜¤ê°€ ì—†ì–´ ë¬´ìŒ ìƒíƒœë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

    flash_settings_cfg = get_config_value(["video_settings", "scene_change_effect"], {}) or {}
    flash_settings = {
        "enabled": bool(flash_settings_cfg.get("enabled", False)),
        "threshold": float(flash_settings_cfg.get("threshold", 30.0)),
        "min_scene_duration": float(flash_settings_cfg.get("min_scene_duration", 1.0)),
        "flash_duration": float(flash_settings_cfg.get("flash_duration", 0.15)),
        "flash_intensity": float(flash_settings_cfg.get("flash_intensity", 1.5)),
    }
    scene_change_times = []
    if flash_settings["enabled"]:
        print("\n[SCENE] ì”¬ ì „í™˜ ë¶„ì„ ì¤‘...")
        scene_change_times = detect_scene_changes(
            video,
            threshold=flash_settings["threshold"],
            min_scene_duration=flash_settings["min_scene_duration"]
        )
        if scene_change_times:
            print(f"[SCENE] ì”¬ ì „í™˜ {len(scene_change_times)}íšŒ ê°ì§€")
        else:
            print("[SCENE] ì”¬ ì „í™˜ ê°ì§€ë˜ì§€ ì•ŠìŒ")

    voice_clips = []
    temp_voice_files = []
    voice_volume = float(get_config_value(["audio_settings", "voice_volume"], 1.0))
    sound_effect_volume = float(get_config_value(["audio_settings", "sound_effect_volume"], 1.0))
    background_music_volume = float(get_config_value(["audio_settings", "background_music_volume"], 0.5))
    if voice_volume != 1.0:
        print(f"\n[AUDIO] ë³´ì´ìŠ¤ ë³¼ë¥¨ ì¦í­: {voice_volume:.2f}x")
    if sound_effect_volume != 1.0:
        print(f"[AUDIO] ì‚¬ìš´ë“œ ì´í™íŠ¸ ë³¼ë¥¨ ì¡°ì •: {sound_effect_volume:.2f}x")

    # ì‹œì‘ ì‚¬ìš´ë“œ ì¶”ê°€
    start_sound_path = get_start_sound()
    if start_sound_path:
        print(f"\n[START SOUND] ì‹œì‘ ì‚¬ìš´ë“œ ë¡œë”©: {start_sound_path}")
        try:
            start_sound_clip = AudioFileClip(start_sound_path).with_start(0)
            if sound_effect_volume != 1.0:
                start_sound_clip = start_sound_clip.with_effects([MultiplyVolume(sound_effect_volume)])
            voice_clips.append(start_sound_clip)
            print(f"[START SOUND] ì‹œì‘ ì‚¬ìš´ë“œ ì¶”ê°€ ì™„ë£Œ (ê¸¸ì´: {start_sound_clip.duration:.2f}ì´ˆ)")
        except Exception as e:
            print(f"[WARNING] ì‹œì‘ ì‚¬ìš´ë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")

    # ë¹„ë””ì˜¤ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ì„¸ê·¸ë¨¼íŠ¸ ì œê±° (ìŒì„±ì´ ì§¤ë¦¬ëŠ” ê²ƒ ë°©ì§€)
    print(f"\n[TIMING] ë¹„ë””ì˜¤ ê¸¸ì´: {video.duration:.2f}ì´ˆ")
    valid_segments = []

    for idx, segment in enumerate(segments):
        seg_start = segment['start']
        seg_end = segment['end']

        # ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ì´ ë¹„ë””ì˜¤ ëì„ ë„˜ìœ¼ë©´ ìŠ¤í‚µ
        if seg_start >= video.duration:
            print(f"[SKIP] ì„¸ê·¸ë¨¼íŠ¸ {idx+1} ì œì™¸: ì‹œì‘({seg_start}ì´ˆ)ì´ ë¹„ë””ì˜¤ ë({video.duration:.2f}ì´ˆ)ì„ ì´ˆê³¼")
            continue

        # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë¹„ë””ì˜¤ ëì— ë„ˆë¬´ ê°€ê¹Œìš°ë©´ ìŠ¤í‚µ (ìµœì†Œ 1ì´ˆ ì—¬ìœ  í•„ìš”)
        if seg_start + 1.0 > video.duration:
            print(f"[SKIP] ì„¸ê·¸ë¨¼íŠ¸ {idx+1} ì œì™¸: ë¹„ë””ì˜¤ ì¢…ë£Œê¹Œì§€ ì—¬ìœ  ì‹œê°„ ë¶€ì¡± (ìµœì†Œ 1ì´ˆ í•„ìš”)")
            continue

        valid_segments.append(segment)

    segments = valid_segments

    # Key momentì™€ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì´ë° ì¡°ì •
    key_moment = metadata.get('key_moment')
    if key_moment is not None:
        key_moment_start = key_moment
        # ë¹„ë””ì˜¤ ê¸¸ì´ì— ë”°ë¼ key moment ì§€ì† ì‹œê°„ ê²°ì • (20ì´ˆ ì´ˆê³¼ ì‹œ 5ì´ˆ, ì´í•˜ ì‹œ 2ì´ˆ)
        highlight_duration = 5.0 if video.duration > 20 else 2.0
        key_moment_end = min(key_moment + highlight_duration, video.duration)
        print(f"\n[TIMING] Key moment êµ¬ê°„ ({key_moment_start}ì´ˆ ~ {key_moment_end}ì´ˆ) ê°ì§€ (í•˜ì´ë¼ì´íŠ¸ ì§€ì†: {highlight_duration}ì´ˆ)")

        adjusted_segments = []
        for idx, segment in enumerate(segments):
            seg_start = segment['start']
            seg_end = segment['end']

            # ì„¸ê·¸ë¨¼íŠ¸ê°€ key momentì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
            if seg_end > key_moment_start and seg_start < key_moment_end:
                # ì˜µì…˜ 0: ì„¸ê·¸ë¨¼íŠ¸ê°€ key momentë¥¼ ê°ì‹¸ê³  ìˆìœ¼ë©´ (AI ì˜ë„) ê·¸ëŒ€ë¡œ ìœ ì§€
                if seg_start <= key_moment_start and seg_end >= key_moment_start:
                    adjusted_segments.append(segment)
                    print(f"[TIMING] ì„¸ê·¸ë¨¼íŠ¸ {idx+1} ìœ ì§€: {seg_start}ì´ˆ~{seg_end}ì´ˆ (key moment {key_moment_start}ì´ˆë¥¼ í¬í•¨í•˜ë„ë¡ ì„¤ê³„ë¨)")
                # ì˜µì…˜ 1: key moment ì „ì— ë„£ê¸°
                elif seg_start < key_moment_start and key_moment_start - seg_start >= (seg_end - seg_start):
                    # key moment ì „ì— ì¶©ë¶„í•œ ê³µê°„ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
                    adjusted_segments.append(segment)
                    print(f"[TIMING] ì„¸ê·¸ë¨¼íŠ¸ {idx+1} ìœ ì§€: {seg_start}ì´ˆ (key moment ì „ ê³µê°„ ì¶©ë¶„)")
                else:
                    # ì˜µì…˜ 2: key moment ëë‚œ í›„ë¡œ ì´ë™ (ë¹„ë””ì˜¤ ê¸¸ì´ ì²´í¬)
                    new_start = key_moment_end
                    duration = seg_end - seg_start
                    new_end = new_start + duration

                    # ì¡°ì •ëœ ì‹œì‘ ìœ„ì¹˜ê°€ ë¹„ë””ì˜¤ ëì— ë„ˆë¬´ ê°€ê¹Œìš°ë©´ ê·¸ëƒ¥ ì›ë˜ëŒ€ë¡œ ìœ ì§€
                    if new_start + 1.0 > video.duration:
                        print(f"[TIMING] ì„¸ê·¸ë¨¼íŠ¸ {idx+1} ìœ ì§€: {seg_start}ì´ˆ~{seg_end}ì´ˆ (key moment í›„ ê³µê°„ ë¶€ì¡±, ì›ë˜ ìœ„ì¹˜ ìœ ì§€)")
                        adjusted_segments.append(segment)
                        continue

                    print(f"[TIMING] ì„¸ê·¸ë¨¼íŠ¸ {idx+1} ì¡°ì •: {seg_start}ì´ˆ â†’ {new_start}ì´ˆ (key moment íšŒí”¼)")
                    adjusted_segments.append({
                        'start': new_start,
                        'end': new_end,
                        'text': segment['text']
                    })
            else:
                adjusted_segments.append(segment)

        segments = adjusted_segments

    # ê° ì„¸ê·¸ë¨¼íŠ¸ë§ˆë‹¤ ìŒì„± ìƒì„±
    print(f"\n[MIC] AI ìŒì„± ìƒì„± ì¤‘... (ì´ {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")

    narration_clips = []  # ë‚´ë ˆì´ì…˜ ë³´ì´ìŠ¤ë§Œ ì €ì¥ (ë”í‚¹ìš©)
    last_voice_end = 0  # ì´ì „ ìŒì„±ì´ ëë‚˜ëŠ” ì‹œê°„ ì¶”ì 
    min_gap = 0.3  # ìŒì„± ê°„ ìµœì†Œ ê°„ê²© (ì´ˆ)

    for idx, segment in enumerate(segments):
        print(f"\n[{idx+1}/{len(segments)}] {segment['start']}ì´ˆ ~ {segment['end']}ì´ˆ")
        print(f"í…ìŠ¤íŠ¸: {segment['text']}")

        # ì„ì‹œ ìŒì„± íŒŒì¼ ìƒì„±
        temp_voice_file = f"temp_voice_{idx}.mp3"
        generate_voice(segment['text'], temp_voice_file)

        # ë³´ì´ìŠ¤ ì†ë„ 1.2ë°°ë¡œ ì¡°ì • (pydub ì‚¬ìš©)
        if AudioSegment is not None and speedup is not None:
            try:
                audio_seg = AudioSegment.from_file(temp_voice_file)
                audio_seg = speedup(audio_seg, playback_speed=1.2)
                audio_seg.export(temp_voice_file, format="mp3")
                print(f"[SPEED] ë³´ì´ìŠ¤ ì†ë„ 1.2ë°°ë¡œ ì¡°ì • ì™„ë£Œ")
            except Exception as e:
                print(f"[WARNING] ë³´ì´ìŠ¤ ì†ë„ ì¡°ì • ì‹¤íŒ¨: {e}")

        # ì˜¤ë””ì˜¤ í´ë¦½ ë¡œë“œ
        voice_clip = AudioFileClip(temp_voice_file)

        # ì‹œì‘ ì‹œê°„ ì¡°ì •: ì´ì „ ìŒì„±ê³¼ ê²¹ì¹˜ì§€ ì•Šë„ë¡
        adjusted_start = segment['start']
        if idx > 0 and adjusted_start < last_voice_end + min_gap:
            adjusted_start = last_voice_end + min_gap
            print(f"[TIMING] ìŒì„± ê²¹ì¹¨ ë°©ì§€: ì‹œì‘ ì‹œê°„ {segment['start']:.2f}ì´ˆ â†’ {adjusted_start:.2f}ì´ˆë¡œ ì¡°ì •")

        # ì¡°ì •ëœ ì‹œì‘ ì‹œê°„ì´ ë¹„ë””ì˜¤ ëì„ ë„˜ìœ¼ë©´ ìŠ¤í‚µ
        if adjusted_start >= video.duration:
            print(f"[SKIP] ì¡°ì •ëœ ì‹œì‘ ì‹œê°„({adjusted_start:.2f}ì´ˆ)ì´ ë¹„ë””ì˜¤ ë({video.duration:.2f}ì´ˆ)ì„ ì´ˆê³¼í•˜ì—¬ ê±´ë„ˆëœ€")
            continue

        voice_clip = voice_clip.with_start(adjusted_start)

        # ë³´ì´ìŠ¤ê°€ ë¹„ë””ì˜¤ ëì„ ë„˜ì§€ ì•Šë„ë¡ ì œí•œ
        voice_end = adjusted_start + voice_clip.duration
        if voice_end > video.duration:
            trim_duration = video.duration - adjusted_start
            if trim_duration > 0.5:  # ìµœì†Œ 0.5ì´ˆëŠ” ë‚¨ì•„ì•¼ ì˜ë¯¸ ìˆìŒ
                voice_clip = voice_clip.subclipped(0, trim_duration)
                voice_end = adjusted_start + trim_duration
                print(f"[NOTE] ë³´ì´ìŠ¤ê°€ ë¹„ë””ì˜¤ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ì—¬ {trim_duration:.2f}ì´ˆë¡œ ìë¦„")
            else:
                print(f"[SKIP] ë‚¨ì€ ì‹œê°„ì´ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœ€ (ì—¬ìœ : {trim_duration:.2f}ì´ˆ)")
                continue

        if voice_volume != 1.0:
            voice_clip = voice_clip.with_effects([MultiplyVolume(voice_volume)])

        voice_clips.append(voice_clip)
        narration_clips.append(voice_clip)  # ë‚´ë ˆì´ì…˜ë§Œ ë”°ë¡œ ì €ì¥
        temp_voice_files.append(temp_voice_file)

        # ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•´ í˜„ì¬ ìŒì„±ì´ ëë‚˜ëŠ” ì‹œê°„ ì €ì¥
        last_voice_end = voice_end

    # Key momentì— ì‚¬ìš´ë“œ ì´í™íŠ¸ ì¶”ê°€
    sound_effect_clip = None
    if metadata.get('key_moment') is not None:
        sound_effect_path = get_random_sound_effect()
        if sound_effect_path:
            print(f"\n[AUDIO] Key moment ({metadata['key_moment']}ì´ˆ)ì— ì‚¬ìš´ë“œ ì´í™íŠ¸ ì¶”ê°€: {os.path.basename(sound_effect_path)}")
            sound_effect_clip = AudioFileClip(sound_effect_path).with_start(metadata['key_moment'])
            if sound_effect_volume != 1.0:
                sound_effect_clip = sound_effect_clip.with_effects([MultiplyVolume(sound_effect_volume)])
            voice_clips.append(sound_effect_clip)

    # Background music: ë©”íƒ€ë°ì´í„°ê°€ 'no'ì¼ ë•Œë§Œ ì¶”ê°€ (ì›ë³¸ ë¹„ë””ì˜¤ì— ìŒì•…ì´ ì—†ëŠ” ê²½ìš°)
    background_music_clip = None
    bg_music_metadata = str(metadata.get('background_music', '')).lower() if metadata else ''
    enable_background_music = get_config_value(["audio_settings", "enable_background_music"], True)

    if not enable_background_music:
        print(f"\n[MUSIC] ë°±ê·¸ë¼ìš´ë“œ ìŒì•… ì¶”ê°€ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (config: enable_background_music = false)")
    elif bg_music_metadata == 'no':
        music_path = get_random_background_music()
        if music_path:
            print(f"\n[MUSIC] ì›ë³¸ ë¹„ë””ì˜¤ì— ë°°ê²½ ìŒì•… ì—†ìŒ â†’ ë°°ê²½ ìŒì•… ì¶”ê°€: {os.path.basename(music_path)}")
            background_music_clip = AudioFileClip(music_path)
            if background_music_clip.duration < video.duration:
                background_music_clip = background_music_clip.with_effects([AudioLoop(duration=video.duration)])
            background_music_clip = background_music_clip.subclipped(0, video.duration).with_start(0)
            if background_music_volume != 1.0:
                print(f"[AUDIO] ë°°ê²½ ìŒì•… ë³¼ë¥¨ ì¡°ì •: {background_music_volume:.2f}x")
                background_music_clip = background_music_clip.with_effects([
                    MultiplyVolume(background_music_volume)
                ])
            voice_clips.append(background_music_clip)
        else:
            print(f"\n[WARNING] ë°°ê²½ ìŒì•… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (background music í´ë” í™•ì¸ í•„ìš”)")
    else:
        print(f"\n[MUSIC] ì›ë³¸ ë¹„ë””ì˜¤ì— ë°°ê²½ ìŒì•…ì´ ì´ë¯¸ ìˆìŒ (Background Music: {metadata.get('background_music', 'N/A')}) â†’ ì¶”ê°€í•˜ì§€ ì•ŠìŒ")

    # ì˜¤ë””ì˜¤ ë”í‚¹: ë³´ì´ìŠ¤ êµ¬ê°„ì—ë§Œ ì›ë³¸ ì˜¤ë””ì˜¤ ë³¼ë¥¨ ê°ì†Œ
    ducking_volume = get_config_value(["audio_settings", "ducking_volume"], 0.3)
    audio_clips = []
    if has_original_audio:
        print(f"\n[AUDIO] ì˜¤ë””ì˜¤ ë”í‚¹ ì ìš© (ë³´ì´ìŠ¤ êµ¬ê°„ ì›ë³¸ ì˜¤ë””ì˜¤ {ducking_volume * 100:.0f}% ë³¼ë¥¨)")

        # ë‚´ë ˆì´ì…˜ ë³´ì´ìŠ¤ í´ë¦½ì˜ êµ¬ê°„ë“¤ë§Œ ìˆ˜ì§‘ (ë°°ê²½ìŒì•…/ì‚¬ìš´ë“œì´í™íŠ¸ ì œì™¸)
        voice_segments = [(clip.start, clip.start + clip.duration)
                          for clip in narration_clips]

        # ì›ë³¸ ì˜¤ë””ì˜¤ë¥¼ êµ¬ê°„ë³„ë¡œ ë¶„í• í•˜ì—¬ ë”í‚¹ ì ìš©
        current_time = 0
        video_duration = video.duration

        for start, end in voice_segments:
            # ë¹„ë””ì˜¤ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” êµ¬ê°„ ìŠ¤í‚µ
            if start >= video_duration:
                continue

            # ë³´ì´ìŠ¤ ì „ êµ¬ê°„: ì›ë³¸ ë³¼ë¥¨
            if current_time < start:
                audio_clips.append(
                    original_audio.subclipped(current_time, min(start, video_duration)).with_start(current_time)
                )

            # ë³´ì´ìŠ¤ êµ¬ê°„: ë”í‚¹ ì ìš©
            end_clamped = min(end, video_duration)
            if start < end_clamped:
                ducked_segment = original_audio.subclipped(start, end_clamped).with_effects(
                    [MultiplyVolume(ducking_volume)]
                ).with_start(start)
                audio_clips.append(ducked_segment)

            current_time = end

        # ë§ˆì§€ë§‰ ë³´ì´ìŠ¤ ì´í›„ êµ¬ê°„: ì›ë³¸ ë³¼ë¥¨
        if current_time < video_duration:
            audio_clips.append(
                original_audio.subclipped(current_time, video_duration).with_start(current_time)
            )
    else:
        print("\n[AUDIO] ì›ë³¸ ì˜¤ë””ì˜¤ê°€ ì—†ì–´ ë”í‚¹ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")

    # ëª¨ë“  ì˜¤ë””ì˜¤ í•©ì„±
    print("[MUSIC] ì˜¤ë””ì˜¤ í•©ì„± ì¤‘...")
    final_audio = CompositeAudioClip(audio_clips + voice_clips)

    # ë¹„ë””ì˜¤ì— ìƒˆ ì˜¤ë””ì˜¤ ì„¤ì •
    final_video = video.with_audio(final_audio)

    # ë©”ì¸ ë¹„ë””ì˜¤ ìœ„ì¹˜/ìŠ¤ì¼€ì¼ ì¡°ì • (ìº”ë²„ìŠ¤ ë‚´ì—ì„œ ì—¬ë°± í™•ë³´ìš©)
    # ì¤‘ë³µ ë¸”ë¡ - ì´ë¯¸ ìœ„ì—ì„œ ìŠ¤ì¼€ì¼/ì˜¤í”„ì…‹ì„ ì ìš©í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë¬´ì‹œ
    main_scale = 1.0
    main_offset_y = 0
    print(f"[VIDEO] ë©”ì¸ ì˜ìƒ ì˜¤í”„ì…‹/ìŠ¤ì¼€ì¼ ì ìš©: offset_y={main_offset_y}, scale={main_scale}")
    base_clip = final_video
    if main_scale != 1.0:
        base_clip = base_clip.resized(main_scale)
    video_w, video_h = final_video.size
    base_w, base_h = base_clip.size
    center_y = (video_h - base_h) / 2.0
    pos_y = center_y + main_offset_y
    background = ColorClip(size=(video_w, video_h), color=(0, 0, 0)).with_duration(final_video.duration)
    positioned = base_clip.with_position(("center", pos_y))
    final_video = CompositeVideoClip([background, positioned])

    # 9:16 ë ˆí„°ë°•ìŠ¤ ì ìš© (ë©”ì¸ ì˜ìƒ ì¤‘ì•™ ì •ë ¬)
    TARGET_WIDTH = 1080
    TARGET_HEIGHT = 1920
    fit_mode = str(get_config_value(["video_settings", "fit_mode"], "letterbox")).lower()
    top_padding = max(0, int(get_config_value(["video_settings", "top_padding"], 0)))
    bottom_padding = max(0, int(get_config_value(["video_settings", "bottom_padding"], 0)))
    current_w, current_h = final_video.size
    if False:  # ì¤‘ë³µ Letterbox ë¸”ë¡ ë¹„í™œì„±í™”
        # ë¨¼ì € ë„ˆë¹„ë¥¼ ê½‰ ì±„ìš°ë„ë¡ ìŠ¤ì¼€ì¼
        scale_factor = TARGET_WIDTH / max(1, current_w)
        print(f"[FIT] Letterbox ëª¨ë“œ ì ìš© (scale={scale_factor:.3f})")
        resized_clip = final_video.resized(scale_factor)

        # ìƒí•˜ë‹¨ íŒ¨ë”© ì ìš© (í•„ìš”í•˜ë©´ ë¹„ìœ¨ì´ 9:16ì„ ì•½ê°„ ë²—ì–´ë‚˜ë„ ì¼ë¶€ ì˜ì—­ì´ ì˜ë¦´ ìˆ˜ ìˆìŒ)
        available_height = max(1, TARGET_HEIGHT - top_padding - bottom_padding)
        offset_pixels = float(main_offset_y) * scale_factor
        print(f"[FIT] Letterbox ê°€ìš©ë†’ì´={available_height:.1f}, clip_h={resized_clip.h:.1f}, scale_offset={offset_pixels:.1f}")

        letterbox_color = parse_color(get_config_value(["video_settings", "letterbox_color"], "#000000"))
        background = ColorClip(size=(TARGET_WIDTH, TARGET_HEIGHT), color=letterbox_color).with_duration(resized_clip.duration)
        pos_x = (TARGET_WIDTH - resized_clip.w) / 2
        base_pos_y = top_padding + (available_height - resized_clip.h) / 2
        min_pos_y = top_padding + min(0, available_height - resized_clip.h)
        max_pos_y = top_padding + max(0, available_height - resized_clip.h)
        pos_y = base_pos_y + offset_pixels
        pos_y = max(min_pos_y, min(max_pos_y, pos_y))
        print(f"[FIT] Letterbox ìœ„ì¹˜: pos_x={pos_x:.1f}, pos_y={pos_y:.1f} (ë²”ìœ„ {min_pos_y:.1f} ~ {max_pos_y:.1f})")
        final_video = CompositeVideoClip([background, resized_clip.with_position((pos_x, pos_y))])
    else:
        # fit_modeê°€ letterboxê°€ ì•„ë‹ˆë”ë¼ë„ ë¹„ìœ¨ ìœ ì§€ (ì‚¬ìš©ì ìš”ì²­)
        # ì›ë³¸ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ 1080x1920 ì•ˆì— ë§ì¶¤
        if current_w != TARGET_WIDTH or current_h != TARGET_HEIGHT:
            scale_w = TARGET_WIDTH / max(1, current_w)
            scale_h = TARGET_HEIGHT / max(1, current_h)
            scale_factor = min(scale_w, scale_h)  # ì‘ì€ ìª½ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ (ë¹„ìœ¨ ìœ ì§€)
            print(f"[RESIZE] ë¹„ìœ¨ ìœ ì§€ ëª¨ë“œ ì ìš© (scale={scale_factor:.3f})")
            resized_clip = final_video.resized(scale_factor)

            letterbox_color = parse_color(get_config_value(["video_settings", "letterbox_color"], "#000000"))
            background = ColorClip(size=(TARGET_WIDTH, TARGET_HEIGHT), color=letterbox_color).with_duration(resized_clip.duration)
            pos_x = (TARGET_WIDTH - resized_clip.w) / 2
            pos_y = (TARGET_HEIGHT - resized_clip.h) / 2
            print(f"[RESIZE] ë¹„ë””ì˜¤ í¬ê¸°: {resized_clip.w:.1f}x{resized_clip.h:.1f}, ìœ„ì¹˜: ({pos_x:.1f}, {pos_y:.1f})")
            final_video = CompositeVideoClip([background, resized_clip.with_position((pos_x, pos_y))])

    # ë©”ì¸ ë¹„ë””ì˜¤ ìœ„ì¹˜/ìŠ¤ì¼€ì¼ ì¡°ì • (ìº”ë²„ìŠ¤ ë‚´ì—ì„œ ì—¬ë°± í™•ë³´ìš©)
    main_scale = float(get_layout_value("video", "scale", ["video_settings", "main_video_scale"], 1.0))
    main_offset_y = int(get_layout_value("video", "offset_y", ["video_settings", "main_video_offset_y"], 40))
    print(f"[VIDEO] ë©”ì¸ ì˜ìƒ ì˜¤í”„ì…‹/ìŠ¤ì¼€ì¼ ì ìš©: offset_y={main_offset_y}, scale={main_scale}")
    base_clip = final_video
    if main_scale != 1.0:
        base_clip = base_clip.resized(main_scale)
    video_w, video_h = final_video.size
    base_w, base_h = base_clip.size
    center_y = (video_h - base_h) / 2.0
    pos_y = center_y + main_offset_y
    background = ColorClip(size=(video_w, video_h), color=(0, 0, 0)).with_duration(final_video.duration)
    positioned = base_clip.with_position(("center", pos_y))
    final_video = CompositeVideoClip([background, positioned])

    # 9:16 ë ˆí„°ë°•ìŠ¤ ì ìš© (ë©”ì¸ ì˜ìƒ ì¤‘ì•™ ì •ë ¬)
    TARGET_WIDTH = 1080
    TARGET_HEIGHT = 1920
    fit_mode = str(get_config_value(["video_settings", "fit_mode"], "letterbox")).lower()
    top_padding = max(0, int(get_config_value(["video_settings", "top_padding"], 0)))
    bottom_padding = max(0, int(get_config_value(["video_settings", "bottom_padding"], 0)))
    current_w, current_h = final_video.size
    if fit_mode == "letterbox":
        # ë¨¼ì € ë„ˆë¹„ë¥¼ ê½‰ ì±„ìš°ë„ë¡ ìŠ¤ì¼€ì¼
        scale_factor = TARGET_WIDTH / max(1, current_w)
        print(f"[FIT] Letterbox ëª¨ë“œ ì ìš© (scale={scale_factor:.3f})")
        resized_clip = final_video.resized(scale_factor)

        # ìƒí•˜ë‹¨ íŒ¨ë”© ì ìš© (í•„ìš”í•˜ë©´ ë¹„ìœ¨ì´ 9:16ì„ ì•½ê°„ ë²—ì–´ë‚˜ë„ ì¼ë¶€ ì˜ì—­ì´ ì˜ë¦´ ìˆ˜ ìˆìŒ)
        available_height = max(1, TARGET_HEIGHT - top_padding - bottom_padding)
        offset_pixels = float(main_offset_y) * scale_factor
        print(f"[FIT] Letterbox ê°€ìš©ë†’ì´={available_height:.1f}, clip_h={resized_clip.h:.1f}, scale_offset={offset_pixels:.1f}")

        letterbox_color = parse_color(get_config_value(["video_settings", "letterbox_color"], "#000000"))
        background = ColorClip(size=(TARGET_WIDTH, TARGET_HEIGHT), color=letterbox_color).with_duration(resized_clip.duration)
        pos_x = (TARGET_WIDTH - resized_clip.w) / 2
        base_pos_y = top_padding + (available_height - resized_clip.h) / 2
        min_pos_y = top_padding + min(0, available_height - resized_clip.h)
        max_pos_y = top_padding + max(0, available_height - resized_clip.h)
        pos_y = base_pos_y + offset_pixels
        pos_y = max(min_pos_y, min(max_pos_y, pos_y))
        print(f"[FIT] Letterbox ìœ„ì¹˜: pos_x={pos_x:.1f}, pos_y={pos_y:.1f} (ë²”ìœ„ {min_pos_y:.1f} ~ {max_pos_y:.1f})")
        final_video = CompositeVideoClip([background, resized_clip.with_position((pos_x, pos_y))])
    else:
        # fit_modeê°€ letterboxê°€ ì•„ë‹ˆë”ë¼ë„ ë¹„ìœ¨ ìœ ì§€ (ì‚¬ìš©ì ìš”ì²­)
        # ì›ë³¸ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ 1080x1920 ì•ˆì— ë§ì¶¤
        if current_w != TARGET_WIDTH or current_h != TARGET_HEIGHT:
            scale_w = TARGET_WIDTH / max(1, current_w)
            scale_h = TARGET_HEIGHT / max(1, current_h)
            scale_factor = min(scale_w, scale_h)  # ì‘ì€ ìª½ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ (ë¹„ìœ¨ ìœ ì§€)
            print(f"[RESIZE] ë¹„ìœ¨ ìœ ì§€ ëª¨ë“œ ì ìš© (scale={scale_factor:.3f})")
            resized_clip = final_video.resized(scale_factor)

            letterbox_color = parse_color(get_config_value(["video_settings", "letterbox_color"], "#000000"))
            background = ColorClip(size=(TARGET_WIDTH, TARGET_HEIGHT), color=letterbox_color).with_duration(resized_clip.duration)
            pos_x = (TARGET_WIDTH - resized_clip.w) / 2
            pos_y = (TARGET_HEIGHT - resized_clip.h) / 2
            print(f"[RESIZE] ë¹„ë””ì˜¤ í¬ê¸°: {resized_clip.w:.1f}x{resized_clip.h:.1f}, ìœ„ì¹˜: ({pos_x:.1f}, {pos_y:.1f})")
            final_video = CompositeVideoClip([background, resized_clip.with_position((pos_x, pos_y))])

    if flash_settings["enabled"] and scene_change_times:
        print(f"\n[FLASH] ì”¬ ì „í™˜ í”Œë˜ì‹œ ì ìš© ({len(scene_change_times)}íšŒ)")
        flash_opacity = max(0.05, min(1.0, flash_settings["flash_intensity"] / 2.0))
        flash_clips = []
        for change_time in scene_change_times:
            start_time = max(0.0, change_time - flash_settings["flash_duration"] / 2)
            if start_time >= video.duration:
                continue
            flash_clip = ColorClip(size=video.size, color=(255, 255, 255))
            flash_clip = flash_clip.with_duration(flash_settings["flash_duration"]).with_start(start_time)
            flash_clip = flash_clip.with_opacity(flash_opacity)
            flash_clips.append(flash_clip)
        if flash_clips:
            final_video = CompositeVideoClip([final_video, *flash_clips])

    # ì„ íƒì  ë¦¬ì•¡ì…˜ ë¹„ë””ì˜¤ ì¶”ê°€
    reaction_cfg = get_config_value(["reaction_video"], {}) or {}
    if reaction_cfg.get("enabled"):
        reaction_video_dir = "reaction video"
        height_ratio = float(reaction_cfg.get("height_ratio", 0.2))
        height_ratio = max(0.05, min(0.5, height_ratio))
        if os.path.exists(reaction_video_dir):
            reaction_files = [
                f for f in os.listdir(reaction_video_dir)
                if f.lower().endswith(('.mp4', '.mov', '.avi', '.mkv'))
            ]
            if reaction_files:
                reaction_path = os.path.join(reaction_video_dir, reaction_files[0])
                print(f"\n[REACTION] ìµœí•˜ë‹¨ì— ë¦¬ì•¡ì…˜ ë¹„ë””ì˜¤ ì¶”ê°€: {reaction_path}")

                try:
                    reaction_clip = VideoFileClip(reaction_path)

                    video_width = final_video.w
                    video_height = final_video.h

                    reaction_clip = reaction_clip.resized(width=video_width)
                    target_height = int(video_height * height_ratio)

                    if reaction_clip.duration < final_video.duration:
                        reaction_clip = reaction_clip.looped(duration=final_video.duration)
                    else:
                        reaction_clip = reaction_clip.subclipped(0, final_video.duration)

                    reaction_clip = reaction_clip.cropped(y1=0, y2=target_height, x1=0, x2=video_width)

                    pos_x = 0
                    pos_y = video_height - target_height

                    reaction_clip = reaction_clip.with_position((pos_x, pos_y))
                    final_video = CompositeVideoClip([final_video, reaction_clip])

                    print(f"   [SUCCESS] ë¦¬ì•¡ì…˜ ë¹„ë””ì˜¤ ì¶”ê°€ ì™„ë£Œ (ë„ˆë¹„: {video_width}px, í‘œì‹œ ë†’ì´: {target_height}px, ìœ„ì¹˜: í•˜ë‹¨)")
                except Exception as e:
                    print(f"   [WARNING] ë¦¬ì•¡ì…˜ ë¹„ë””ì˜¤ ì¶”ê°€ ì‹¤íŒ¨: {e}")

    # ì¸ë„¤ì¼ íƒ€ì´í‹€ í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´ ì¶”ê°€ - ë¹„í™œì„±í™”ë¨
    # if metadata.get('thumbnail_title'):
    #     print(f"\n[NOTE] ì¸ë„¤ì¼ íƒ€ì´í‹€ ì¶”ê°€: {metadata['thumbnail_title']}")
    #     ... (ì½”ë“œ ì œê±°ë¨)

    # í¬ë¡œë§ˆí‚¤ ë¦¬ì•¡ì…˜ ë¹„ë””ì˜¤ ì˜¤ë²„ë ˆì´ - ë¹„í™œì„±í™”ë¨ (ì„±ëŠ¥ ìµœì í™”)
    # chromakey_cfg = get_config_value(["chromakey_settings"], {}) or {}
    # chromakey_enabled = chromakey_cfg.get("enabled", True)
    # ... (ì½”ë“œ ì œê±°ë¨)

    # AI ëŒ€ì‚¬ ìë§‰ì€ ë§¨ ë§ˆì§€ë§‰ì— ì¶”ê°€ (ë¦¬ì•¡ì…˜ ë¹„ë””ì˜¤ ë’¤)
    # ìë§‰ í´ë¦½ì„ ë¨¼ì € ìƒì„±ë§Œ í•¨
    subtitle_clips = []
    if add_subtitles and segments:
        print(f"\n[NOTE] AI ëŒ€ì‚¬ ìë§‰ ì¶”ê°€ ì¤‘... (ì´ {len(segments)}ê°œ)")

        # ìë§‰ ì„¤ì •
        subtitle_font = get_windows_font()
        subtitle_font_path = get_config_value(["subtitle_settings", "text_font"], subtitle_font)
        if not subtitle_font_path or not os.path.exists(subtitle_font_path):
            subtitle_font_path = subtitle_font

        # 9:16 ë¹„ìœ¨ ê¸°ì¤€ìœ¼ë¡œ ê³ ì •ëœ ìë§‰ í¬ê¸° (1080x1920 ê¸°ì¤€)
        # ì›ë³¸ ë¹„ìœ¨ê³¼ ê´€ê³„ì—†ì´ í•­ìƒ ì¼ì •í•œ í¬ê¸°ë¡œ í‘œì‹œ
        STANDARD_WIDTH = 1080
        STANDARD_HEIGHT = 1920

        # ìë§‰ í°íŠ¸ í¬ê¸°: 1080x1920 ê¸°ì¤€ 35px
        subtitle_font_size = int(get_layout_value("subtitle", "font_size", ["subtitle_settings", "text_size"], 35))

        # ìë§‰ ìƒ‰ìƒ: íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ëœ ìƒ‰ìƒ ìš°ì„ , ì—†ìœ¼ë©´ config ì‚¬ìš©
        if subtitle_color is None:
            subtitle_color = get_config_value(["subtitle_settings", "text_color"], "pink")
        subtitle_stroke_width = int(get_config_value(["subtitle_settings", "stroke_width"], 5))
        subtitle_stroke_color = get_config_value(["subtitle_settings", "stroke_color"], "black")
        subtitle_style = str(get_config_value(["subtitle_settings", "style"], "capcut") or "capcut").lower()
        subtitle_line_spacing = int(get_config_value(
            ["subtitle_settings", "line_spacing"],
            max(6, subtitle_font_size // 6)
        ))
        italic_shear = float(get_config_value(["subtitle_settings", "italic_shear"], 0.22))
        use_slanted_style = subtitle_style in {"sports_slant", "k-wave", "hangul_slant", "kwave", "bold_slant"}
        language_code = str(get_config_value(["voice_settings", "language"], "en") or "").lower()
        prefer_cjk_language = any(language_code.startswith(prefix) for prefix in ("ko", "ja", "zh"))
        force_pil_renderer = prefer_cjk_language or bool(get_config_value(["subtitle_settings", "force_pil_renderer"], False))

        # ëœë¤ ìƒ‰ìƒ ì„¤ì •
        random_colors_enabled = get_config_value(["subtitle_settings", "random_colors", "enabled"], False)
        random_colors_list = get_config_value(["subtitle_settings", "random_colors", "colors"], ["pink"])
        current_color_index = 0

        # í•˜ë‹¨ ì—¬ë°±: 1920 ê¸°ì¤€ 600px - ë” ìœ„ìª½ì— í‘œì‹œ
        subtitle_bottom_margin = int(get_layout_value("subtitle", "bottom_margin", ["subtitle_settings", "bottom_margin"], 600))

        # ìë§‰ ìµœëŒ€ ë„ˆë¹„: í˜„ì¬ ë¹„ë””ì˜¤ ë„ˆë¹„ì˜ 85%ë¡œ ì„¤ì • (í™”ë©´ ë°–ìœ¼ë¡œ ë‚˜ê°€ì§€ ì•Šë„ë¡)
        video_width = final_video.w
        side_margin = int(get_layout_value("subtitle", "side_margin", ["subtitle_settings", "side_margin"], 90))
        video_height = final_video.h
        TARGET_WIDTH = 1080
        TARGET_HEIGHT = 1920
        display_scale = min(TARGET_WIDTH / max(1, video_width), TARGET_HEIGHT / max(1, video_height))
        font_scale = min(1.0, display_scale)
        subtitle_font_size = max(18, int(round(subtitle_font_size * font_scale)))
        subtitle_stroke_width = max(1, int(round(subtitle_stroke_width * font_scale)))
        subtitle_line_spacing = max(0, int(round(subtitle_line_spacing * font_scale)))
        subtitle_bottom_margin = max(40, int(round(subtitle_bottom_margin * font_scale)))
        side_margin = int(round(side_margin * font_scale))
        subtitle_margin = max(10, int(round(20 * font_scale)))

        # ì‹¤ì œ ë¹„ë””ì˜¤ ë„ˆë¹„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚° (ì¢Œìš° ì—¬ë°± í¬í•¨)
        subtitle_max_width = max(200, video_width - (2 * side_margin))
        print(f"[SUBTITLE] ìë§‰ ì„¤ì •: í°íŠ¸={subtitle_font_size}px, ìµœëŒ€ë„ˆë¹„={subtitle_max_width}px (ë¹„ë””ì˜¤: {video_width}px)")

        subtitle_exit_cfg = get_config_value(["subtitle_settings", "exit_animation"], {}) or {}
        subtitle_exit_vertical_offset = _safe_float(subtitle_exit_cfg.get("vertical_offset"), 60)
        subtitle_exit_scale_reduction = _safe_float(subtitle_exit_cfg.get("scale_reduction"), 0.08)
        subtitle_clips = []
        last_subtitle_end = 0  # ì´ì „ ìë§‰ì´ ëë‚˜ëŠ” ì‹œê°„ ì¶”ì 
        min_subtitle_gap = 0.2  # ìë§‰ ê°„ ìµœì†Œ ê°„ê²© (ì´ˆ)

        for idx, segment in enumerate(segments):
            text = segment['text'].strip()
            start_time = float(segment['start'])
            end_time = float(segment['end'])

            # ìë§‰ ì§€ì† ì‹œê°„ ê³„ì‚°
            duration = end_time - start_time
            min_duration = float(get_config_value(["subtitle_settings", "min_duration"], 1.2))
            extra_hold = float(get_config_value(["subtitle_settings", "extra_hold"], 0.6))
            duration = max(duration + extra_hold, min_duration)

            # ìë§‰ ê²¹ì¹¨ ë°©ì§€: ì´ì „ ìë§‰ê³¼ ê²¹ì¹˜ë©´ ì‹œì‘ ì‹œê°„ ì¡°ì •
            adjusted_start = start_time
            if idx > 0 and start_time < last_subtitle_end + min_subtitle_gap:
                adjusted_start = last_subtitle_end + min_subtitle_gap
                print(f"   [TIMING] ìë§‰ ê²¹ì¹¨ ë°©ì§€: {start_time:.2f}ì´ˆ â†’ {adjusted_start:.2f}ì´ˆ")

            # ì¡°ì •ëœ ë ì‹œê°„ ê³„ì‚°
            adjusted_end = adjusted_start + duration

            # ë¹„ë””ì˜¤ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì¡°ì •
            if adjusted_start >= video.duration:
                print(f"   [SKIP] ìë§‰ {idx+1} ì œì™¸: ì‹œì‘({adjusted_start:.2f}ì´ˆ)ì´ ë¹„ë””ì˜¤ ë({video.duration:.2f}ì´ˆ)ì„ ì´ˆê³¼")
                continue

            if adjusted_end > video.duration:
                duration = video.duration - adjusted_start
                adjusted_end = video.duration
                if duration < 0.5:
                    print(f"   [SKIP] ìë§‰ {idx+1} ì œì™¸: ë‚¨ì€ ì‹œê°„ì´ ë„ˆë¬´ ì§§ìŒ")
                    continue

            start_time = adjusted_start

            try:
                # ìë§‰ í´ë¦½ ìƒì„± (method="caption"ìœ¼ë¡œ ìë™ ì¤„ë°”ê¿ˆ)
                effective_width = max(50, subtitle_max_width - subtitle_margin * 2)  # ì¢Œìš° margin ê³ ë ¤
                wrapped_text = wrap_text_preserving_words(
                    text,
                    subtitle_font_path,
                    subtitle_font_size,
                    effective_width,
                    subtitle_stroke_width
                )

                # ëœë¤ ìƒ‰ìƒ ì„ íƒ (í™œì„±í™”ëœ ê²½ìš°)
                if random_colors_enabled and random_colors_list:
                    current_subtitle_color = random_colors_list[current_color_index % len(random_colors_list)]
                    current_color_index += 1
                else:
                    current_subtitle_color = subtitle_color

                subtitle_clip = None
                textclip_error = None

                if not force_pil_renderer:
                    try:
                        subtitle_clip = TextClip(
                            text=wrapped_text,
                            font=get_textclip_font_name(subtitle_font_path),
                            font_size=subtitle_font_size,
                            color=current_subtitle_color,
                            stroke_color=subtitle_stroke_color,
                            stroke_width=subtitle_stroke_width,
                            margin=(subtitle_margin, subtitle_margin, subtitle_margin, subtitle_margin),
                            interline=subtitle_line_spacing,
                            method="caption",  # ì¤„ë°”ê¿ˆì€ wrap_text_preserving_words ì—ì„œ ì œì–´
                            size=(effective_width, None),  # marginì„ ì œì™¸í•œ í…ìŠ¤íŠ¸ ì˜ì—­ ë„ˆë¹„ (3270ë²ˆ ì¤„ì—ì„œ ê³„ì‚°ë¨)
                            text_align="center"  # ì¤‘ì•™ ì •ë ¬
                        )
                    except Exception as exc:
                        textclip_error = exc

                if subtitle_clip is None:
                    if textclip_error:
                        print(f"   [FALLBACK] TextClip ìë§‰ ìƒì„± ì‹¤íŒ¨, PIL ë Œë”ë§ ì‚¬ìš© ({textclip_error})")
                    elif force_pil_renderer:
                        print("   [FALLBACK] í•œêµ­ì–´/ì¼ë³¸ì–´/ì¤‘êµ­ì–´ ëª¨ë“œ â†’ PIL ìë§‰ ë Œë”ëŸ¬ ì‚¬ìš©")

                    subtitle_clip = create_pil_subtitle_clip(
                        wrapped_text,
                        subtitle_font_path,
                        subtitle_font_size,
                        current_subtitle_color,
                        subtitle_stroke_color,
                        subtitle_stroke_width,
                        (subtitle_margin, subtitle_margin, subtitle_margin, subtitle_margin),
                        subtitle_line_spacing,
                        italic_shear if use_slanted_style else 0.0
                    )
                elif use_slanted_style:
                    subtitle_clip = convert_textclip_to_slanted_imageclip(subtitle_clip, italic_shear)

                # í™”ë©´ í•˜ë‹¨ì— ìœ„ì¹˜ (1920 ê¸°ì¤€ 400px ìœ„)
                # í˜„ì¬ ë¹„ë””ì˜¤ ë†’ì´ì— ë§ì¶° ë¹„ìœ¨ ì¡°ì •
                y_position = video_height - subtitle_bottom_margin

                # ìë§‰ì´ í™”ë©´ ë°–ìœ¼ë¡œ ë‚˜ê°€ì§€ ì•Šë„ë¡ ì¡°ì •
                if y_position < 0:
                    y_position = video_height // 2  # ë„ˆë¬´ ì‘ìœ¼ë©´ ì¤‘ì•™ì— í‘œì‹œ
                elif y_position + subtitle_clip.h > video_height:
                    y_position = video_height - subtitle_clip.h - 50  # ìµœì†Œ 50px ì—¬ë°±

                subtitle_clip = (
                    subtitle_clip.with_start(start_time)
                               .with_duration(duration)
                               .with_position(('center', y_position))
                )

                # ìë§‰ì´ ì‚¬ë¼ì§ˆ ë•Œ opacityë§Œ ë‚®ì¶°ì§€ë©´ì„œ ë¹ ë¥´ê²Œ í˜ì´ë“œì•„ì›ƒ
                subtitle_exit_duration = min(0.25, max(0.15, duration * 0.2))
                subtitle_clip = subtitle_clip.with_effects([FadeOut(subtitle_exit_duration)])

                subtitle_clips.append(subtitle_clip)
                print(f"   [OK] [{idx+1}/{len(segments)}] {start_time:.1f}ì´ˆ (y={y_position}): {text[:40]}...")

                # ë‹¤ìŒ ìë§‰ì„ ìœ„í•´ í˜„ì¬ ìë§‰ì˜ ëë‚˜ëŠ” ì‹œê°„ ì €ì¥
                last_subtitle_end = start_time + duration

            except Exception as e:
                print(f"   [WARNING] ìë§‰ ìƒì„± ì‹¤íŒ¨ [{idx+1}]: {e}")
                continue

        if subtitle_clips:
            print(f"[OK] {len(subtitle_clips)}ê°œ ìë§‰ í´ë¦½ ìƒì„± ì™„ë£Œ (í•˜ë‹¨ì—ì„œ {subtitle_bottom_margin}px ìœ„)")
        else:
            print("[WARNING] ìƒì„±ëœ ìë§‰ì´ ì—†ìŠµë‹ˆë‹¤")

    # í´ë” ì´ë¦„ ì„¸ë¡œ í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´ ì¶”ê°€ (ë¹„í™œì„±í™”ë¨)
    if folder_name:
        print(f"\n[FOLDER] í´ë” ì´ë¦„ ì„¸ë¡œ í…ìŠ¤íŠ¸ ì¶”ê°€: {folder_name}")
        folder_overlay_cfg = get_config_value(["folder_overlay"], {}) or {}
        # í´ë” ì˜¤ë²„ë ˆì´ ê¸°ëŠ¥ ë¹„í™œì„±í™” - ê¸°ë³¸ê°’ Falseë¡œ ì„¤ì •
        if folder_overlay_cfg.get("enabled", False):
            # í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ì˜¤ë²„ë ˆì´ ì¶”ê°€
            # í…ìŠ¤íŠ¸ ì„¤ì • (ë¹„ë””ì˜¤ ë†’ì´ì˜ 3%ì—ì„œ 40% ë” ì‘ê²Œ = 1.8%)
            video_height = final_video.h
            folder_font_size = int(video_height * 0.03 * 0.6)  # 40% ë” ì‘ê²Œ

            # í´ë” ì´ë¦„ë§Œ (ì´ëª¨ì§€ ì œê±°)
            vertical_text = folder_name

            # í…ìŠ¤íŠ¸ ì„¤ì • (Windows í°íŠ¸ ì‚¬ìš©)
            default_font = get_windows_font()
            folder_font_path = get_config_value(["text_settings", "font"], default_font)

            # configì— í°íŠ¸ê°€ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ Windows í°íŠ¸ ì‚¬ìš©
            if not folder_font_path or not os.path.exists(folder_font_path):
                folder_font_path = default_font

            # PILë¡œ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
            try:
                pil_font = load_pil_font(folder_font_path, folder_font_size)
            except OSError:
                pil_font = ImageFont.load_default()

            # ì¹´ë©”ë¼ ì•„ì´ì½˜ ë¡œë“œ
            camera_icon_path = "asset/camera-icon-design-template-d8c5370c36c44621de2fd64718718d58_screen.png"
            camera_icon = Image.open(camera_icon_path).convert("RGBA")

            # ì¹´ë©”ë¼ ì•„ì´ì½˜ í¬ê¸° ì¡°ì • (í…ìŠ¤íŠ¸ë³´ë‹¤ 10% í¬ê²Œ)
            icon_size = int(folder_font_size * 1.1)  # í…ìŠ¤íŠ¸ë³´ë‹¤ 10% í¬ê²Œ
            camera_icon = camera_icon.resize((icon_size, icon_size), Image.Resampling.LANCZOS)

            # ì¹´ë©”ë¼ ì•„ì´ì½˜ íˆ¬ëª…ë„ ì¡°ì • (60% ë¶ˆíˆ¬ëª…ë„)
            if camera_icon.mode == 'RGBA':
                alpha = camera_icon.split()[3]
                alpha = alpha.point(lambda p: int(p * 0.6))  # 60% ë¶ˆíˆ¬ëª…ë„
                camera_icon.putalpha(alpha)

            # í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¡œí¬ ì„¤ì • ë° í¬ê¸° ì¸¡ì • (ìŠ¤íŠ¸ë¡œí¬ í¬í•¨)
            text_stroke_width = 3
            measure_draw = ImageDraw.Draw(Image.new("RGBA", (1, 1)))
            text_bbox = measure_draw.textbbox(
                (0, 0),
                vertical_text,
                font=pil_font,
                stroke_width=text_stroke_width
            )
            text_width = text_bbox[2] - text_bbox[0]
            text_height = text_bbox[3] - text_bbox[1]

            # ì—¬ë°±ê³¼ ê°„ê²©
            padding = 3
            spacing = 3  # ì•„ì´ì½˜ê³¼ í…ìŠ¤íŠ¸ ì‚¬ì´ ê°„ê²©

            # ì „ì²´ ì´ë¯¸ì§€ ë†’ì´ë¥¼ ì•„ì´ì½˜ê³¼ í…ìŠ¤íŠ¸ ì¤‘ ë” í° ê²ƒìœ¼ë¡œ ë§ì¶¤
            max_height = max(icon_size, text_height)

            # ì „ì²´ ì´ë¯¸ì§€ í¬ê¸° (ê°€ë¡œ: ì•„ì´ì½˜ + ê°„ê²© + í…ìŠ¤íŠ¸)
            img_width = icon_size + spacing + text_width + padding * 2
            img_height = max_height + padding * 2

            # ì´ë¯¸ì§€ ìƒì„± (ê°€ë¡œë¡œ ë°°ì¹˜)
            combined_image = Image.new("RGBA", (img_width, img_height), (0, 0, 0, 0))

            # ì¹´ë©”ë¼ ì•„ì´ì½˜ê³¼ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§ ì¤‘ì•™ ì •ë ¬
            icon_y = (img_height - icon_size) // 2
            text_y = (img_height - text_height) // 2

            # ì¹´ë©”ë¼ ì•„ì´ì½˜ ë¶™ì´ê¸° (ì™¼ìª½, ì¤‘ì•™ ì •ë ¬)
            combined_image.paste(camera_icon, (padding, icon_y), camera_icon)

            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (ì˜¤ë¥¸ìª½, ì¤‘ì•™ ì •ë ¬) - íˆ¬ëª…ë„ ì ìš©
            text_draw = ImageDraw.Draw(combined_image)
            text_x = padding + icon_size + spacing
            text_draw.text(
                (text_x, text_y),
                vertical_text,
                font=pil_font,
                fill=(255, 255, 255, 153),  # í°ìƒ‰ 60% ë¶ˆíˆ¬ëª…ë„ (255 * 0.6 = 153)
                stroke_width=text_stroke_width,  # ìŠ¤íŠ¸ë¡œí¬ ë” êµµê²Œ
                stroke_fill=(0, 0, 0, 153)  # ê²€ì • 60% ë¶ˆíˆ¬ëª…ë„
            )

            # ì•„ì´ì½˜/í…ìŠ¤íŠ¸ ë§ˆìŠ¤í¬ ìƒì„± (ë¶„ë¦¬ ìœ„ì¹˜ ê³„ì‚°ìš©)
            icon_mask = Image.new("L", combined_image.size, 0)
            icon_alpha = camera_icon.split()[-1] if camera_icon.getbands()[-1] == "A" else None
            if icon_alpha is None:
                icon_mask.paste(255, (padding, icon_y, padding + icon_size, icon_y + icon_size))
            else:
                icon_mask.paste(icon_alpha, (padding, icon_y))

            text_mask = Image.new("L", combined_image.size, 0)
            text_mask_draw = ImageDraw.Draw(text_mask)
            text_mask_draw.text(
                (text_x, text_y),
                vertical_text,
                font=pil_font,
                fill=255,
                stroke_width=text_stroke_width,
                stroke_fill=255
            )

            # ì´ë¯¸ì§€ë¥¼ 90ë„ íšŒì „ (ë°˜ì‹œê³„ ë°©í–¥)
            rotated_image = combined_image.rotate(90, expand=True)
            rotated_icon_mask = icon_mask.rotate(90, expand=True, resample=Image.Resampling.NEAREST)
            rotated_text_mask = text_mask.rotate(90, expand=True, resample=Image.Resampling.NEAREST)

            def _mask_bbox(mask_image):
                arr = np.array(mask_image)
                ys, xs = np.where(arr > 0)
                if ys.size == 0 or xs.size == 0:
                    return None
                x_min, x_max = int(xs.min()), int(xs.max()) + 1
                y_min, y_max = int(ys.min()), int(ys.max()) + 1
                return x_min, y_min, x_max, y_max

            def _expand_bbox(bbox, padding, width, height):
                if not bbox:
                    return None
                x_min, y_min, x_max, y_max = bbox
                x_min = max(0, x_min - padding)
                y_min = max(0, y_min - padding)
                x_max = min(width, x_max + padding)
                y_max = min(height, y_max + padding)
                if x_min >= x_max or y_min >= y_max:
                    return None
                return x_min, y_min, x_max, y_max

            icon_bbox = _mask_bbox(rotated_icon_mask)
            text_rotated_bbox = _mask_bbox(rotated_text_mask)
            padding_px = max(6, int(text_stroke_width) * 2)
            if text_rotated_bbox:
                text_rotated_bbox = _expand_bbox(
                    text_rotated_bbox,
                    padding_px,
                    rotated_image.width,
                    rotated_image.height
                )
            if icon_bbox:
                icon_bbox = _expand_bbox(
                    icon_bbox,
                    max(2, int(text_stroke_width)),
                    rotated_image.width,
                    rotated_image.height
                )

            # ê¸°ë³¸ ìœ„ì¹˜ ê³„ì‚°
            base_cfg = folder_overlay_cfg.get("base_position") or {}
            base_x = float(base_cfg.get("x", 30))
            base_y_offset = float(base_cfg.get("y_offset", -50))
            base_y = (video_height - rotated_image.height) / 2 + base_y_offset

            def _parse_xy(value, default=(0.0, 0.0)):
                if isinstance(value, dict):
                    try:
                        return float(value.get("x", default[0])), float(value.get("y", default[1]))
                    except (TypeError, ValueError):
                        return default
                if isinstance(value, (list, tuple)) and len(value) == 2:
                    try:
                        return float(value[0]), float(value[1])
                    except (TypeError, ValueError):
                        return default
                return default

            def _parse_xy_optional(value):
                if value is None:
                    return None
                if isinstance(value, dict):
                    try:
                        return float(value.get("x")), float(value.get("y"))
                    except (TypeError, ValueError):
                        return None
                if isinstance(value, (list, tuple)) and len(value) == 2:
                    try:
                        return float(value[0]), float(value[1])
                    except (TypeError, ValueError):
                        return None
                return None

            text_offset = _parse_xy(folder_overlay_cfg.get("text_offset", {}), (0.0, 0.0))
            icon_offset = _parse_xy(folder_overlay_cfg.get("icon_offset", {}), (0.0, 0.0))
            text_position_override = _parse_xy_optional(folder_overlay_cfg.get("text_position"))
            icon_position_override = _parse_xy_optional(folder_overlay_cfg.get("icon_position"))

            overlays = []

            if text_rotated_bbox:
                cropped_text = rotated_image.crop(text_rotated_bbox)
                text_np = np.array(cropped_text)
                if text_np.size:
                    text_clip = ImageClip(text_np).with_duration(video.duration)
                    if text_position_override is not None:
                        text_position = text_position_override
                    else:
                        text_position = (
                            base_x + text_rotated_bbox[0] + text_offset[0],
                            base_y + text_rotated_bbox[1] + text_offset[1]
                        )
                    overlays.append(text_clip.with_position(text_position))

            if icon_bbox:
                cropped_icon = rotated_image.crop(icon_bbox)
                icon_np = np.array(cropped_icon)
                if icon_np.size:
                    icon_clip = ImageClip(icon_np).with_duration(video.duration)
                    if icon_position_override is not None:
                        icon_position = icon_position_override
                    else:
                        icon_position = (
                            base_x + icon_bbox[0] + icon_offset[0],
                            base_y + icon_bbox[1] + icon_offset[1]
                        )
                    overlays.append(icon_clip.with_position(icon_position))

            if overlays:
                final_video = CompositeVideoClip([final_video, *overlays])
            else:
                print("[WARNING] í´ë” ì˜¤ë²„ë ˆì´ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # í”„ë ˆì„ ì˜¤ë²„ë ˆì´ ì¶”ê°€
    frame_overlay_cfg = get_config_value(["frame_overlay"], {}) or {}
    if frame_overlay_cfg.get("enabled", False):
        image_path = frame_overlay_cfg.get("image_path")
        resolved_path = image_path
        if resolved_path and not os.path.isabs(resolved_path):
            resolved_path = os.path.join(os.getcwd(), resolved_path)
        if resolved_path and os.path.exists(resolved_path):
            try:
                frame_clip = ImageClip(resolved_path).with_duration(final_video.duration)

                video_w, video_h = final_video.size
                scale = float(frame_overlay_cfg.get("scale", 1.0))
                target_w = max(1, int(video_w * scale))
                target_h = max(1, int(video_h * scale))
                frame_clip = frame_clip.resized((target_w, target_h))
                print(f"[FRAME] ì˜¤ë²„ë ˆì´ í¬ê¸° ì¡°ì •: {target_w}x{target_h} (scale={scale})")

                opacity = float(frame_overlay_cfg.get("opacity", 1.0))
                if 0.0 <= opacity < 1.0:
                    frame_clip = frame_clip.with_opacity(opacity)

                position_cfg = frame_overlay_cfg.get("position", "center") or "center"
                if isinstance(position_cfg, str) and position_cfg == "center":
                    position = ("center", "center")
                else:
                    position = position_cfg
                frame_clip = frame_clip.with_position(position)

                final_video = CompositeVideoClip([final_video, frame_clip])
                print(f"\n[FRAME] í”„ë ˆì„ ì˜¤ë²„ë ˆì´ ì ìš©: {image_path}")
            except Exception as e:
                print(f"\n[FRAME] í”„ë ˆì„ ì˜¤ë²„ë ˆì´ ì ìš© ì‹¤íŒ¨: {e}")
        else:
            print(f"\n[FRAME] í”„ë ˆì„ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")

    # ìë§‰ì„ ë§¨ ë§ˆì§€ë§‰ì— ì¶”ê°€ (ìµœìƒë‹¨ ë ˆì´ì–´)
    if subtitle_clips:
        print(f"\n[SUBTITLE] ìë§‰ì„ ìµœìƒë‹¨ ë ˆì´ì–´ë¡œ ì¶”ê°€ ì¤‘... ({len(subtitle_clips)}ê°œ)")
        final_video = CompositeVideoClip([final_video] + subtitle_clips)
        print(f"[OK] ìë§‰ì´ ìµœìƒë‹¨ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤")

    # ìµœì¢… í¬ê¸° í™•ì¸ ë° 9:16 ê°•ì œ
    final_w, final_h = final_video.size
    print(f"[VIDEO] í˜„ì¬ ë¹„ë””ì˜¤ í¬ê¸°: {final_w}x{final_h}")

    TARGET_WIDTH = 1080
    TARGET_HEIGHT = 1920

    if final_w != TARGET_WIDTH or final_h != TARGET_HEIGHT:
        print(f"[RESIZE] ìµœì¢… í¬ê¸° ì¡°ì •: {final_w}x{final_h} â†’ {TARGET_WIDTH}x{TARGET_HEIGHT} (9:16)")
        final_video = final_video.resized((TARGET_WIDTH, TARGET_HEIGHT))
    else:
        print(f"[OK] ì´ë¯¸ ì˜¬ë°”ë¥¸ í¬ê¸° (9:16): {TARGET_WIDTH}x{TARGET_HEIGHT}")

    print("[VIDEO] ìµœì¢… ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")

    # ë¹„ë””ì˜¤ ì €ì¥ (H.264 ì½”ë±, AAC ì˜¤ë””ì˜¤)
    # ë™ì‹œ ì¸ì½”ë”©ì„ ìœ„í•œ ì¸ìŠ¤í„´ìŠ¤ë³„ temp íŒŒì¼ëª…
    temp_dir = get_config_value(["paths", "temp_dir"], "Temp")
    temp_audio_file = os.path.join(temp_dir, f"temp-audio-{os.getpid()}.m4a")

    video_write_kwargs = {
        "codec": get_config_value(["audio_settings", "codec"], "libx264"),
        "audio_codec": get_config_value(["audio_settings", "audio_codec"], "aac"),
        "temp_audiofile": temp_audio_file,
        "remove_temp": True,
        "threads": 2,  # FFmpeg ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ (ë™ì‹œ ì¸ì½”ë”© ëŒ€ì‘)
        "fps": 30,
    }

    configured_bitrate = get_config_value(["video_settings", "bitrate"])
    if configured_bitrate:
        video_write_kwargs["bitrate"] = configured_bitrate

    configured_preset = get_config_value(["video_settings", "preset"])
    if configured_preset:
        video_write_kwargs["preset"] = configured_preset

    ffmpeg_params = get_config_value(["video_settings", "ffmpeg_params"])
    if ffmpeg_params:
        video_write_kwargs["ffmpeg_params"] = ffmpeg_params

    final_video.write_videofile(
        output_path,
        **video_write_kwargs,
    )

    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    video.close()
    final_video.close()
    for clip in voice_clips:
        clip.close()

    for temp_file in temp_voice_files:
        if os.path.exists(temp_file):
            os.remove(temp_file)

    print(f"\n[OK] ìŒì„± ì˜¤ë²„ë ˆì´ ë° ìë§‰ ì™„ë£Œ: {output_path}")
    return output_path


def auto_upload_processed_video(video_path=None, metadata=None):
    """ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ë¥¼ ìë™ìœ¼ë¡œ ì—…ë¡œë“œ (í™˜ê²½ ì„¤ì •ì— ë”°ë¼ ë©€í‹° ê³„ì • ì§€ì›)."""
    if not get_config_value(["youtube_settings", "auto_upload"], False):
        return

    if video_path is not None and not os.path.exists(video_path):
        print(f"[WARNING] ìë™ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤. ì¶œë ¥ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return

    if video_path is None:
        output_dir = get_config_value(["paths", "output_dir"], "Output")
        candidate_path = os.path.join(output_dir, "final_video.mp4")
        if not os.path.exists(candidate_path):
            print("[WARNING] ìë™ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤. ì¶œë ¥ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        video_path = candidate_path

    accounts_config_path = os.path.join("Config", "accounts.json")
    use_multi_account = False

    if os.path.exists(accounts_config_path):
        try:
            with open(accounts_config_path, "r", encoding="utf-8") as accounts_file:
                accounts_config = json.load(accounts_file)
            use_multi_account = bool(accounts_config.get("accounts"))
        except (OSError, json.JSONDecodeError) as exc:
            print(f"[WARNING] ê³„ì • ì„¤ì •ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ ({exc}). ë‹¨ì¼ ê³„ì • ì—…ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")

    uploader = None

    if use_multi_account:
        try:
            from youtube_upload_multi import upload_from_output as uploader
        except ImportError as exc:
            print(f"[WARNING] ë©€í‹° ê³„ì • ì—…ë¡œë“œ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {exc}. ë‹¨ì¼ ê³„ì • ì—…ë¡œë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            use_multi_account = False

    if not use_multi_account:
        try:
            from youtube_upload import upload_from_output as uploader
        except ImportError as exc:
            print(f"[ERROR] ìë™ ì—…ë¡œë“œë¥¼ ìœ„í•œ ì—…ë¡œë“œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {exc}")
            return

    if uploader is None:
        print("[ERROR] ìë™ ì—…ë¡œë“œë¥¼ ìœ„í•œ ì—…ë¡œë“œ í•¨ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    try:
        video_id = uploader(video_path=video_path, metadata=metadata)
        if video_id:
            print(f"[UPLOAD] ìë™ ì—…ë¡œë“œ ì™„ë£Œ: {video_id}")
    except Exception as exc:
        print(f"[ERROR] ìë™ ì—…ë¡œë“œ ì‹¤íŒ¨: {exc}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # MoviePy ì„ì‹œ ë””ë ‰í† ë¦¬ë¥¼ ì¸ìŠ¤í„´ìŠ¤ë³„ë¡œ ë¶„ë¦¬ (ë™ì‹œ ì¸ì½”ë”© ëŒ€ì‘)
    temp_dir = get_config_value(["paths", "temp_dir"], "Temp")
    moviepy_temp_dir = os.path.join(temp_dir, f"moviepy_{os.getpid()}")
    os.makedirs(moviepy_temp_dir, exist_ok=True)
    os.environ["MOVIEPY_TEMP_DIR"] = moviepy_temp_dir
    print(f"[INIT] MoviePy temp ë””ë ‰í† ë¦¬: {moviepy_temp_dir}")

    # ì…ë ¥/ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    input_dir = get_config_value(["paths", "input_dir"], "Input")
    try:
        input_video, original_source, folder_name = find_first_video_file(input_dir)
    except FileNotFoundError as exc:
        print(f"[ERROR] {exc}")
        return
    print(f"\n[VIDEO] ë¶„ì„ ëŒ€ìƒ ë¹„ë””ì˜¤: {input_video}")
    if folder_name:
        print(f"[FOLDER] í´ë” ì´ë¦„: {folder_name}")

    try:
        script = generate_script_with_gemini(input_video)
    except Exception as exc:
        import traceback
        print(f"\n[ERROR] Gemini ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘ ì—ëŸ¬ ë°œìƒ:")
        print(f"   ì—ëŸ¬ íƒ€ì…: {type(exc).__name__}")
        print(f"   ì—ëŸ¬ ë©”ì‹œì§€: {exc}")
        print(f"\nìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
        traceback.print_exc()

        # PROHIBITED_CONTENT ì—ëŸ¬ì¸ ê²½ìš° íŒŒì¼ì„ ê±´ë„ˆë›°ê¸° í´ë”ë¡œ ì´ë™
        if "PROHIBITED_CONTENT" in str(exc) or "block_reason" in str(exc):
            print(f"\n[WARNING] Geminiì— ì˜í•´ ì°¨ë‹¨ëœ ì½˜í…ì¸ ì…ë‹ˆë‹¤. ì´ ë¹„ë””ì˜¤ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            cleaned = cleanup_extracted_video(input_video)
            if cleaned:
                print(f"[DELETE] ì¶”ì¶œëœ ì„ì‹œ ë¹„ë””ì˜¤ ì‚­ì œ: {input_video}")

            # ì›ë³¸ íŒŒì¼ì„ Used/Blocked í´ë”ë¡œ ì´ë™
            if original_source:
                used_root = os.path.join(input_dir, "Used", "Blocked")
                os.makedirs(used_root, exist_ok=True)
                dest_path = os.path.join(used_root, os.path.basename(original_source))

                # íŒŒì¼ëª… ì¤‘ë³µ ì²˜ë¦¬
                counter = 1
                while os.path.exists(dest_path):
                    base, ext = os.path.splitext(os.path.basename(original_source))
                    dest_path = os.path.join(used_root, f"{base}_{counter}{ext}")
                    counter += 1

                try:
                    shutil.move(original_source, dest_path)
                    print(f"[BLOCKED] ì°¨ë‹¨ëœ íŒŒì¼ ì´ë™: {original_source} -> {dest_path}")
                except Exception as move_err:
                    print(f"[WARNING] íŒŒì¼ ì´ë™ ì‹¤íŒ¨: {move_err}")

            return

        fallback_path = get_config_value(["ai_settings", "fallback_script_path"])
        if fallback_path and os.path.exists(fallback_path):
            print(f"\n[WARNING] ëŒ€ì²´ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©: {fallback_path}")
            with open(fallback_path, "r", encoding="utf-8") as script_file:
                script = script_file.read()
        else:
            print(f"\n[ERROR] ëŒ€ì²´ ìŠ¤í¬ë¦½íŠ¸ë„ ì—†ìŠµë‹ˆë‹¤. ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")

            # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ íŒŒì¼ ì •ë¦¬
            cleaned = cleanup_extracted_video(input_video)
            if cleaned:
                print(f"[DELETE] ì¶”ì¶œëœ ì„ì‹œ ë¹„ë””ì˜¤ ì‚­ì œ: {input_video}")

            return

    print("\n[NOTE] ìŠ¤í¬ë¦½íŠ¸ íŒŒì‹± ì¤‘...")
    segments, metadata = parse_script(script)

    if folder_name:
        credit_line = f"credit: {folder_name}"
        if metadata.get('youtube_description'):
            if credit_line not in metadata['youtube_description']:
                metadata['youtube_description'] = metadata['youtube_description'].rstrip() + "\n" + credit_line
        else:
            metadata['youtube_description'] = credit_line

        if credit_line not in script:
            script = script.rstrip() + "\n" + credit_line + "\n"

    print(f"\nì´ {len(segments)}ê°œì˜ ì„¸ê·¸ë¨¼íŠ¸ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
    for seg in segments:
        print(f"  - {seg['start']}ì´ˆ ~ {seg['end']}ì´ˆ: {seg['text'][:50]}...")

    if metadata.get('key_moment'):
        print(f"\n[KEY] Key moment: {metadata['key_moment']}ì´ˆ")
    # if metadata.get('thumbnail_title'):  # ë¹„í™œì„±í™”ë¨
    #     print(f"[NOTE] Thumbnail title: {metadata['thumbnail_title']}")

    output_dir = get_config_value(["paths", "output_dir"], "Output")
    os.makedirs(output_dir, exist_ok=True)

    base_name = (
        metadata.get("youtube_title")
        # or metadata.get("thumbnail_title")  # ë¹„í™œì„±í™”ë¨
        or os.path.splitext(os.path.basename(input_video))[0]
    )
    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„± (ì„¤ì •ì— ë”°ë¼ íƒ€ì„ìŠ¤íƒ¬í”„ ì ‘ë‘ì–´ ì¶”ê°€)
    output_base = generate_output_basename(base_name, output_dir, extension=".mp4")
    final_output_video = os.path.join(output_dir, f"{output_base}.mp4")

    # ìƒì„±ëœ ìŠ¤í¬ë¦½íŠ¸ ë¡œê·¸ ì €ì¥ ì˜µì…˜
    script_log_path = get_config_value(["ai_settings", "last_script_path"])
    if script_log_path:
        try:
            os.makedirs(os.path.dirname(script_log_path), exist_ok=True)
            with open(script_log_path, "w", encoding="utf-8") as log_file:
                log_file.write(script)
        except Exception:
            pass

    # ë¹„ë””ì˜¤ì™€ ê°™ì€ ì´ë¦„ì˜ ë©”íƒ€ë°ì´í„° í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
    try:
        metadata_filename = f"{output_base}.txt"
        metadata_file_path = os.path.join(output_dir, metadata_filename)
        meta_counter = 1
        while os.path.exists(metadata_file_path):
            metadata_filename = f"{output_base}_{meta_counter}.txt"
            metadata_file_path = os.path.join(output_dir, metadata_filename)
            meta_counter += 1
        with open(metadata_file_path, "w", encoding="utf-8") as meta_file:
            meta_file.write(script)
        print(f"[METADATA] ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_filename}")
    except Exception as e:
        print(f"[WARNING] ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

    # ë¹„ë””ì˜¤ ì¹´ìš´íŠ¸ì— ë”°ë¼ ìƒ‰ìƒ ê²°ì •
    # Output í´ë”ì˜ ë¹„ë””ì˜¤ íŒŒì¼ ê°œìˆ˜ ì„¸ê¸°
    try:
        video_count = len([f for f in os.listdir(output_dir)
                          if f.lower().endswith(('.mp4', '.mov', '.avi', '.mkv'))])

        # ì²« ë²ˆì§¸ ë¹„ë””ì˜¤(count=0): ì¸ë„¤ì¼ íƒ€ì´í‹€ ë¶„í™ìƒ‰, í‚¤ì›Œë“œ ë¹¨ê°„ìƒ‰, ìë§‰ ë¹¨ê°„ìƒ‰
        # ë‘ ë²ˆì§¸ ë¹„ë””ì˜¤(count=1): ì¸ë„¤ì¼ íƒ€ì´í‹€ í•˜ì–€ìƒ‰, í‚¤ì›Œë“œ ë…¸ë€ìƒ‰, ìë§‰ íŒŒë€ìƒ‰
        if video_count % 2 == 0:
            # ì²« ë²ˆì§¸, ì„¸ ë²ˆì§¸, ë‹¤ì„¯ ë²ˆì§¸... ë¹„ë””ì˜¤
            title_color = "pink"
            keyword_color = "red"
            subtitle_color = "red"
        else:
            # ë‘ ë²ˆì§¸, ë„¤ ë²ˆì§¸, ì—¬ì„¯ ë²ˆì§¸... ë¹„ë””ì˜¤
            title_color = "white"
            keyword_color = "yellow"
            subtitle_color = "blue"

        print(f"[COLOR] ë¹„ë””ì˜¤ ì¹´ìš´íŠ¸: {video_count + 1}")
        print(f"  - ì¸ë„¤ì¼ íƒ€ì´í‹€: {title_color}")
        print(f"  - í‚¤ì›Œë“œ: {keyword_color}")
        print(f"  - ìë§‰: {subtitle_color}")
    except Exception as e:
        print(f"[WARNING] ë¹„ë””ì˜¤ ì¹´ìš´íŠ¸ í™•ì¸ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ìƒ‰ìƒ ì‚¬ìš©")
        title_color = None
        keyword_color = None
        subtitle_color = None

    # ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤í–‰ (ìŒì„± + ìë§‰ í•œ ë²ˆì— ì²˜ë¦¬)
    final_output_path = overlay_voice_on_video(
        input_video,
        segments,
        final_output_video,
        metadata,
        folder_name,
        add_subtitles=True,  # ìë§‰ ì¶”ê°€ í™œì„±í™”
        subtitle_color="white",  # í•˜ì–€ìƒ‰ ìë§‰
        title_color="white",  # í•˜ì–€ìƒ‰ íƒ€ì´í‹€
        keyword_color="white"  # í•˜ì–€ìƒ‰ í‚¤ì›Œë“œ
    )

    print(f"\n[OK] ìµœì¢… ì¶œë ¥ íŒŒì¼: {final_output_path}")

    # ìë™ ì—…ë¡œë“œ (í•„ìš” ì‹œ)
    auto_upload_processed_video(final_output_path, metadata)

    # ì²˜ë¦¬ ì™„ë£Œ í›„ ì…ë ¥ íŒŒì¼ ì´ë™/ì •ë¦¬
    cleaned = cleanup_extracted_video(input_video)
    if cleaned:
        print(f"\n[DELETE]  ì¶”ì¶œëœ ì„ì‹œ ë¹„ë””ì˜¤ ì‚­ì œ: {input_video}")

    moved_paths = set()
    if original_source:
        if move_input_file_to_used(original_source):
            moved_paths.add(os.path.abspath(original_source))

    input_abs = os.path.abspath(input_video)
    if input_abs not in moved_paths and not cleaned:
        move_input_file_to_used(input_video)


def process_all_videos():
    """Input í´ë”ì˜ ëª¨ë“  ë¹„ë””ì˜¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬"""
    import hashlib

    input_dir = get_config_value(["paths", "input_dir"], "Input")
    processed_count = 0
    processed_hashes = set()  # ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ í•´ì‹œ ì €ì¥

    print("[VIDEO] ë¹„ë””ì˜¤ ìë™ ì²˜ë¦¬ ì‹œì‘...")
    print("=" * 60)

    while True:
        try:
            # ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
            next_video, next_video_origin, _ = find_first_video_file(input_dir)
        except FileNotFoundError:
            # ë” ì´ìƒ ë¹„ë””ì˜¤ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
            break

        # ë¹„ë””ì˜¤ íŒŒì¼ í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ì²´í¬)
        try:
            with open(next_video, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()

            if file_hash in processed_hashes:
                print(f"\n[SKIP] ì´ë¯¸ ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ì…ë‹ˆë‹¤ (ì¤‘ë³µ): {os.path.basename(next_video)}")
                # ì¤‘ë³µ íŒŒì¼ ì‚­ì œ
                cleanup_extracted_video(next_video)
                # ì›ë³¸ ZIP íŒŒì¼ë„ ì´ë™
                if next_video_origin:
                    move_input_file_to_used(next_video_origin)
                continue

            processed_hashes.add(file_hash)
        except Exception as e:
            print(f"\n[WARNING] í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")

        processed_count += 1
        print(f"\n{'=' * 60}")
        print(f"[VIDEO] ì²˜ë¦¬ ì¤‘: {processed_count}ë²ˆì§¸ ë¹„ë””ì˜¤")
        print(f"{'=' * 60}")

        # ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
        main()

        # 3ê°œë§ˆë‹¤ ìë™ ë³‘í•© (ë¹„í™œì„±í™” - run_mac.shì—ì„œ ì¼ê´„ ë³‘í•©)
        # if processed_count % 3 == 0:
        #     print("\n" + "="*60)
        #     print(f"[MERGE] ë¹„ë””ì˜¤ 3ê°œ ìƒì„± ì™„ë£Œ! ìë™ ë³‘í•© ì‹œì‘...")
        #     print("="*60)
        #
        #     try:
        #         import subprocess
        #         merge_script = "Scripts/merge_with_transition.py"
        #         result = subprocess.run(
        #             [sys.executable, merge_script],
        #             check=True,
        #             capture_output=False,
        #             text=True
        #         )
        #         print("\n[SUCCESS] ìë™ ë³‘í•© ì™„ë£Œ!")
        #     except subprocess.CalledProcessError as e:
        #         print(f"\n[WARNING] ìë™ ë³‘í•© ì‹¤íŒ¨: {e}")
        #     except Exception as e:
        #         print(f"\n[WARNING] ìë™ ë³‘í•© ì¤‘ ì˜¤ë¥˜: {e}")

        # ì²˜ë¦¬ ì™„ë£Œ í›„ ì ì‹œ ëŒ€ê¸°
        print("\n[WAIT] ë‹¤ìŒ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤€ë¹„ ì¤‘...\n")

    print(f"\n{'=' * 60}")
    print(f"[OK] ëª¨ë“  ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   ì´ {processed_count}ê°œì˜ ë¹„ë””ì˜¤ ì²˜ë¦¬ë¨")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    # Gemini APIì™€ì˜ ì¶©ëŒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ GOOGLE_APPLICATION_CREDENTIALSë¥¼ ì „ì—­ì ìœ¼ë¡œ ì œê±°
    # (TTS ì‚¬ìš© ì‹œ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ ì„¤ì •ë¨)
    SAVED_GOOGLE_CREDS = os.environ.pop("GOOGLE_APPLICATION_CREDENTIALS", None)

    # ì—°ì† ì²˜ë¦¬ ëª¨ë“œ
    process_all_videos()

    # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì „ í™˜ê²½ë³€ìˆ˜ ë³µì›
    if SAVED_GOOGLE_CREDS:
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = SAVED_GOOGLE_CREDS
